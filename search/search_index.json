{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"eucalypt eucalypt is a tool, and a little language, for generating and transforming structured data formats like YAML, JSON and TOML. If you use text-based templating to process these formats or you pipe this these formats through several different tools or build steps, eucalypt might be able to help you generate your output more cleanly and with fewer cognitive somersaults. eucalypt is a purely functional language that can be used quickly and easily from the command line. It has the following features: a concise native syntax that allows you to define data, functions, and operators a simple embedding into YAML files to support in-place manipulation of the data (a la templating) facilities for manipulating blocks (think JSON objects, YAML mappings) facilities for manipulating text including string interpolation and regular expressions an ergonomic command line interface and access to environment variables metadata annotations and numerous extension points a prelude of built in functions, acting like a standard library It can currently read YAML, JSON, JSON Lines, TOML, EDN, XML, CSV and plain text and eucalypt's own (\"eu\") syntax and it can export YAML, JSON, TOML, EDN or plain text. Warning eucalypt is still in an early phase of development and subject to change. A lightning tour Eucalypt has a native syntax for writing blocks, lists and expressions. The YAML embedding consists of a few YAML tags used to embed eucalypt expression in YAML so a basic understanding of the native syntax is helpful. A few micro-examples should help give a flavour of eucalypt's native syntax. If you want to follow along, see Getting Started for notes on installation. Example 1 Here is a simple one: target-zones: [\"a\", \"b\", \"c\"] map(\"eu-west-1{}\") You can put this in a file named test.eu and run it with just: eu test.eu This outputs the following YAML: target-zones: - eu-west-1a - eu-west-1b - eu-west-1c As an aside, although we're looking at the native eucalypt syntax here, this example could just as easily be embedded directly in a YAML file using the !eu tag. Pop the following in a test.yaml file and process it with: eu test.yaml . You'll get the same result. target-zones: !eu [\"a\", \"b\", \"c\"] map(\"eu-west-1{}\") First, this example illustrates how we apply transformations like map simply by concatenation. This \"pipelining\" or \"catenation\" is the natural way to apply transformations to values in eucalypt. In fact this is simply a function call with the arguments rearranged a bit. In this example, map is a function of two parameters. Its first argument is provided in parentheses and its second argument is the value of what came before. Note Users of languages like Elixir or OCaml may recognise an implicit |> operator here. Clojure users may see an invisible threading macro. Note that writing elements next to each other like this gives you the reverse of what you might expect in Haskell or OCaml or Lisp: we write x f not f x . There is a lot of freedom in eucalypt to express ideas in different ways and develop colorful and cryptic expressions. In a larger or more ambitious language this could be viewed as rope to hang yourself with. Please be careful. The string template, \"eu-west-1{}\" , actually defines a function of one argument that returns a string. The key ingredients here are: the interpolation syntax \"{...}\" which allows values to be inserted into the string the (hidden) use of numeric anaphora in the interpolation syntax ( {0} , {1} , {2} , ...) which cause the string to define a function, not just sequence of characters the use of the unnumbered anaphor ( {} ) which is numbered automatically for us, so in this case, {} is a convenient synonym for {0} - the first argument Note Anaphora crop up in various contexts in eucalypt and are generally preferable to the full generality of lambdas. If the idea is too complex to be expressed with anaphora, it should generally be explicitly named. So: a: 42 \"The answer is {0}\" renders as a: The answer is 42 eucalypt also has expression anaphora and block anaphora Note Users of Groovy or Kotlin may recognise an equivalent of the it parameter. Seasoned Lisp hackers are familiar with anaphoric macros. Clojure users will recognise the % , %1 , %2 forms from #(...) contexts. Unlike % repeated uses of unnumbered anaphora in eucalypt refer to different parameters. \"{}{}\" is a two-argument function which concatenates strings. Back to: target-zones: [\"a\", \"b\", \"c\"] map(\"eu-west-1{}\") The whole line is a declaration . Declarations come in several types - this one is a property declaration . A block is written as a sequence of declarations enclosed in braces. For example: { w: \"foo\" # a string x: 3 # a whole number y: 22.2 # a floaty number z: true # the truth } (The # character introduces a comment which is ignored.) Unlike YAML, indentation is never significant. Unlike JSON, commas are not needed to separate declarations. Instead, the eucalypt parser determines the declarations mainly based on the location of colons. You can write: { x: 1 increment negate y: 2 } ...and eucalypt knows it's two declarations. If that's a bit too crazy for you, then feel free to insert the commas. Eucalypt will accept them. Any of these are okay: ok1: { a: 1 b: 2 c: 3 } ok2: { a: 1, b: 2, c: 3 } ok3: { a: 1, b: 2, c: 3, } Note Unlike Clojure which makes commas optional by treating them as whitespace, Eucalypt demands that if you are going to put commas in, they have to be in the right place, at the end of declarations. So you can use them if you believe it makes things clearer but you are prevented from using them in ways which would misguide. Our target-zones property declaration is at the top level so need not be surrounded by braces. Nevertheless it is in a block: the top level block, known as a unit , that is defined by the file that contains it. You can imagine the braces to be there if you like. As a final point on this example, it is probably worthwhile documenting declarations. eucalypt offers an easy way to do that using declaration metadata which we squeeze in between a leading backtick and the declaration itself: ` \"AZs to deploy alien widgets in\" target-zones: [\"a\", \"b\", \"c\"] map(\"eu-west-1{}\") In fact, all sorts of things can be wedged in there, but if a string appears on its own, it is interpreted as documentation. Example 2 Let's look at another small example: character(name): { resource-name: name created: io.epoch-time } prentice: character(\"Pirate Prentice\") { laser-colour: \"red\" } slothrop: character(\"Tyrone Slothrop\") { eye-count: 7 } We've introduced a new type of declaration here of the form f(x): . This is a function declaration . Remember we saw a property declaration earlier. Eucalypt also has operator declarations but we'll ignore those for now. The function declaration declares a function called character , which accepts a single parameter ( name ) and returns a block containing two properties. Functions, like everything else in eucalypt, are declared in and live in blocks but they are left out when output is rendered, so you won't see them in the YAML or JSON that eucalypt produces. The braces in the definition of character are there to delimit the resulting block - not to define a function body. A function that returned a number would not need them: inc(x): x + 1 # this defines an increment function The next important ingredient in this example is block catenation . Blocks can be treated as functions of a single parameter. When they are applied as functions, the effect is a block merge . We've already seen that functions can be applied to arguments by concatenation. So writing one block after another produces a merged block. It contains the contents of the second block merged \"on top\" of the first. There is more to be said on block merge, but for now: { a: 1 } { b: 2 } evaluates to { a: 1 b: 2 } . and { a: 1 } { a: 2 } evaluates to { a: 2 } . In our example, the resulting YAML is just: prentice: resource-name: Pirate Prentice created: 1526991765 laser-colour: red slothrop: resource-name: Tyrone Slothrop created: 1526991765 eye-count: 7 As you can see, io.epoch-time evaluates to a unix timestamp. This metadata is generated once at launch time, not each time the expression is evaluated. eucalypt the language is a pure functional language, and there are no side-effects or non-deterministic functions (although its command line driver can perform all sorts of side-effects as input to the evaluation and as output from the evaluation and there are one or two dirty tricks in the debugging functions). For this reason, prentice and slothrop will have the same timestamps. Block merge can be a useful means of generating common content in objects. The common content can appear first as in this case, allowing it to be overridden. Or it couple be applied second allowing it to override the existing detail. Or a mixture of both. Many more sophisticated means of combining block data are available too. Note This merge is similar to the effect of merge keys in YAML, where a special << mapping key causes a similar merge to occur. Not all YAML processors support this and nor does eucalypt at present, but it probably will some day. Be aware that eucalypt has nothing like virtual functions. The functions in scope when an expression is created are the ones that are applied. So if you redefine an f like this, in an overriding block... { f(x): x+1 a: f(2) } { f(x): x-2 } ...the definition of a will not see it. a: 3 So block merge is only very loosely related to object oriented inheritance. Also by default you only get a shallow merge - deep merges are provided in the standard prelude. It is possible that a deep merge will become the default for block catenation in future. Many more complicated ways of processing blocks are possible using functions, block anaphora and standard prelude functions. Quick tour of the command line On macOS you can install the eu command line tools using Homebrew with: brew install curvelogic/homebrew-tap/eucalypt Check the version you are running with: eu version eu is intended to be easy to use for common tasks and does its best to allow you to say what you want succinctly. The intention is to be easy to use in pipelines in combination with other tools like jq . By default, it runs in ergonomic mode which will make a few assumptions in order to allow you to be a little less explicit. It also pulls in user-specific declarations from ~/.eucalypt . For repeatable builds and scripted usage, it is better to turn ergonomic mode off using the -B (--batch) switch. The simplest usage is to specify a eucalypt file to evaluate and leave the default render format (YAML) and output (standard out) alone. > eu test.eu eu with no arguments will generally be taken to specify that input is coming from standard in. So the above is equivalent to: > cat test.eu | eu There is an -x switch to control output format explicitly (setting \"yaml\", \"json\", \"text\", \"csv\" or \"eu\") but for the very common case of requiring JSON output there is a shortcut: > eu test.eu -j You can, of course, redirect standard output to a file but if you specify the output file explicitly (with -o ), eu will infer the output format from the extension: > eu test.eu -o output.json # equivalent to eu test.eu -j > output.json Small snippets of eucalypt can be passed in directly using the -e switch. > eu -e '{ a: 8 * 8 }' The fact that eucalypt makes relatively infrequent use of single quotes makes this straightforward for most shells. By default, eu evaluates the entirety of the loaded source and uses all of it to render the result, leaving out any function values and other non-renderable content. It is possible to select just parts of the eucalypt for rendering: A declaration in the source may be identified as the main target using the :main declaration metadata and we become the part rendered by default. targets may be defined and named using the :target declaration metadata and those targets can then be specified using the -t option to eu The -e option can be used in addition to other source file(s) to identify an expression to be rendered (e.g. eu test.eu -e x.y.z ) So eu 's ability to read JSON and YAML natively combined with the last options give a simple way to pick values out of structured data which can be very handy for \"querying\" services that return YAML or JSON data. > aws s3api list-buckets | eu -e 'Buckets map(lookup(:Name))' There is much more to this story. For instance eu can: accept several inputs to make definitions in earlier inputs available to subsequent inputs eu test1.eu test2.eu test3.eu accept YAML and JSON files as pure data to be merged in: eu data.yaml tools.eu accept YAML or JSON annotated with eucalypt to execute: eu data.yaml override the default extensions: eu yaml@info.txt automatically use Eufile files in the current folder hierarchy See command line for more complete documentation.","title":"Home"},{"location":"#eucalypt","text":"eucalypt is a tool, and a little language, for generating and transforming structured data formats like YAML, JSON and TOML. If you use text-based templating to process these formats or you pipe this these formats through several different tools or build steps, eucalypt might be able to help you generate your output more cleanly and with fewer cognitive somersaults. eucalypt is a purely functional language that can be used quickly and easily from the command line. It has the following features: a concise native syntax that allows you to define data, functions, and operators a simple embedding into YAML files to support in-place manipulation of the data (a la templating) facilities for manipulating blocks (think JSON objects, YAML mappings) facilities for manipulating text including string interpolation and regular expressions an ergonomic command line interface and access to environment variables metadata annotations and numerous extension points a prelude of built in functions, acting like a standard library It can currently read YAML, JSON, JSON Lines, TOML, EDN, XML, CSV and plain text and eucalypt's own (\"eu\") syntax and it can export YAML, JSON, TOML, EDN or plain text. Warning eucalypt is still in an early phase of development and subject to change.","title":"eucalypt"},{"location":"#a-lightning-tour","text":"Eucalypt has a native syntax for writing blocks, lists and expressions. The YAML embedding consists of a few YAML tags used to embed eucalypt expression in YAML so a basic understanding of the native syntax is helpful. A few micro-examples should help give a flavour of eucalypt's native syntax. If you want to follow along, see Getting Started for notes on installation.","title":"A lightning tour"},{"location":"#example-1","text":"Here is a simple one: target-zones: [\"a\", \"b\", \"c\"] map(\"eu-west-1{}\") You can put this in a file named test.eu and run it with just: eu test.eu This outputs the following YAML: target-zones: - eu-west-1a - eu-west-1b - eu-west-1c As an aside, although we're looking at the native eucalypt syntax here, this example could just as easily be embedded directly in a YAML file using the !eu tag. Pop the following in a test.yaml file and process it with: eu test.yaml . You'll get the same result. target-zones: !eu [\"a\", \"b\", \"c\"] map(\"eu-west-1{}\") First, this example illustrates how we apply transformations like map simply by concatenation. This \"pipelining\" or \"catenation\" is the natural way to apply transformations to values in eucalypt. In fact this is simply a function call with the arguments rearranged a bit. In this example, map is a function of two parameters. Its first argument is provided in parentheses and its second argument is the value of what came before. Note Users of languages like Elixir or OCaml may recognise an implicit |> operator here. Clojure users may see an invisible threading macro. Note that writing elements next to each other like this gives you the reverse of what you might expect in Haskell or OCaml or Lisp: we write x f not f x . There is a lot of freedom in eucalypt to express ideas in different ways and develop colorful and cryptic expressions. In a larger or more ambitious language this could be viewed as rope to hang yourself with. Please be careful. The string template, \"eu-west-1{}\" , actually defines a function of one argument that returns a string. The key ingredients here are: the interpolation syntax \"{...}\" which allows values to be inserted into the string the (hidden) use of numeric anaphora in the interpolation syntax ( {0} , {1} , {2} , ...) which cause the string to define a function, not just sequence of characters the use of the unnumbered anaphor ( {} ) which is numbered automatically for us, so in this case, {} is a convenient synonym for {0} - the first argument Note Anaphora crop up in various contexts in eucalypt and are generally preferable to the full generality of lambdas. If the idea is too complex to be expressed with anaphora, it should generally be explicitly named. So: a: 42 \"The answer is {0}\" renders as a: The answer is 42 eucalypt also has expression anaphora and block anaphora Note Users of Groovy or Kotlin may recognise an equivalent of the it parameter. Seasoned Lisp hackers are familiar with anaphoric macros. Clojure users will recognise the % , %1 , %2 forms from #(...) contexts. Unlike % repeated uses of unnumbered anaphora in eucalypt refer to different parameters. \"{}{}\" is a two-argument function which concatenates strings. Back to: target-zones: [\"a\", \"b\", \"c\"] map(\"eu-west-1{}\") The whole line is a declaration . Declarations come in several types - this one is a property declaration . A block is written as a sequence of declarations enclosed in braces. For example: { w: \"foo\" # a string x: 3 # a whole number y: 22.2 # a floaty number z: true # the truth } (The # character introduces a comment which is ignored.) Unlike YAML, indentation is never significant. Unlike JSON, commas are not needed to separate declarations. Instead, the eucalypt parser determines the declarations mainly based on the location of colons. You can write: { x: 1 increment negate y: 2 } ...and eucalypt knows it's two declarations. If that's a bit too crazy for you, then feel free to insert the commas. Eucalypt will accept them. Any of these are okay: ok1: { a: 1 b: 2 c: 3 } ok2: { a: 1, b: 2, c: 3 } ok3: { a: 1, b: 2, c: 3, } Note Unlike Clojure which makes commas optional by treating them as whitespace, Eucalypt demands that if you are going to put commas in, they have to be in the right place, at the end of declarations. So you can use them if you believe it makes things clearer but you are prevented from using them in ways which would misguide. Our target-zones property declaration is at the top level so need not be surrounded by braces. Nevertheless it is in a block: the top level block, known as a unit , that is defined by the file that contains it. You can imagine the braces to be there if you like. As a final point on this example, it is probably worthwhile documenting declarations. eucalypt offers an easy way to do that using declaration metadata which we squeeze in between a leading backtick and the declaration itself: ` \"AZs to deploy alien widgets in\" target-zones: [\"a\", \"b\", \"c\"] map(\"eu-west-1{}\") In fact, all sorts of things can be wedged in there, but if a string appears on its own, it is interpreted as documentation.","title":"Example 1"},{"location":"#example-2","text":"Let's look at another small example: character(name): { resource-name: name created: io.epoch-time } prentice: character(\"Pirate Prentice\") { laser-colour: \"red\" } slothrop: character(\"Tyrone Slothrop\") { eye-count: 7 } We've introduced a new type of declaration here of the form f(x): . This is a function declaration . Remember we saw a property declaration earlier. Eucalypt also has operator declarations but we'll ignore those for now. The function declaration declares a function called character , which accepts a single parameter ( name ) and returns a block containing two properties. Functions, like everything else in eucalypt, are declared in and live in blocks but they are left out when output is rendered, so you won't see them in the YAML or JSON that eucalypt produces. The braces in the definition of character are there to delimit the resulting block - not to define a function body. A function that returned a number would not need them: inc(x): x + 1 # this defines an increment function The next important ingredient in this example is block catenation . Blocks can be treated as functions of a single parameter. When they are applied as functions, the effect is a block merge . We've already seen that functions can be applied to arguments by concatenation. So writing one block after another produces a merged block. It contains the contents of the second block merged \"on top\" of the first. There is more to be said on block merge, but for now: { a: 1 } { b: 2 } evaluates to { a: 1 b: 2 } . and { a: 1 } { a: 2 } evaluates to { a: 2 } . In our example, the resulting YAML is just: prentice: resource-name: Pirate Prentice created: 1526991765 laser-colour: red slothrop: resource-name: Tyrone Slothrop created: 1526991765 eye-count: 7 As you can see, io.epoch-time evaluates to a unix timestamp. This metadata is generated once at launch time, not each time the expression is evaluated. eucalypt the language is a pure functional language, and there are no side-effects or non-deterministic functions (although its command line driver can perform all sorts of side-effects as input to the evaluation and as output from the evaluation and there are one or two dirty tricks in the debugging functions). For this reason, prentice and slothrop will have the same timestamps. Block merge can be a useful means of generating common content in objects. The common content can appear first as in this case, allowing it to be overridden. Or it couple be applied second allowing it to override the existing detail. Or a mixture of both. Many more sophisticated means of combining block data are available too. Note This merge is similar to the effect of merge keys in YAML, where a special << mapping key causes a similar merge to occur. Not all YAML processors support this and nor does eucalypt at present, but it probably will some day. Be aware that eucalypt has nothing like virtual functions. The functions in scope when an expression is created are the ones that are applied. So if you redefine an f like this, in an overriding block... { f(x): x+1 a: f(2) } { f(x): x-2 } ...the definition of a will not see it. a: 3 So block merge is only very loosely related to object oriented inheritance. Also by default you only get a shallow merge - deep merges are provided in the standard prelude. It is possible that a deep merge will become the default for block catenation in future. Many more complicated ways of processing blocks are possible using functions, block anaphora and standard prelude functions.","title":"Example 2"},{"location":"#quick-tour-of-the-command-line","text":"On macOS you can install the eu command line tools using Homebrew with: brew install curvelogic/homebrew-tap/eucalypt Check the version you are running with: eu version eu is intended to be easy to use for common tasks and does its best to allow you to say what you want succinctly. The intention is to be easy to use in pipelines in combination with other tools like jq . By default, it runs in ergonomic mode which will make a few assumptions in order to allow you to be a little less explicit. It also pulls in user-specific declarations from ~/.eucalypt . For repeatable builds and scripted usage, it is better to turn ergonomic mode off using the -B (--batch) switch. The simplest usage is to specify a eucalypt file to evaluate and leave the default render format (YAML) and output (standard out) alone. > eu test.eu eu with no arguments will generally be taken to specify that input is coming from standard in. So the above is equivalent to: > cat test.eu | eu There is an -x switch to control output format explicitly (setting \"yaml\", \"json\", \"text\", \"csv\" or \"eu\") but for the very common case of requiring JSON output there is a shortcut: > eu test.eu -j You can, of course, redirect standard output to a file but if you specify the output file explicitly (with -o ), eu will infer the output format from the extension: > eu test.eu -o output.json # equivalent to eu test.eu -j > output.json Small snippets of eucalypt can be passed in directly using the -e switch. > eu -e '{ a: 8 * 8 }' The fact that eucalypt makes relatively infrequent use of single quotes makes this straightforward for most shells. By default, eu evaluates the entirety of the loaded source and uses all of it to render the result, leaving out any function values and other non-renderable content. It is possible to select just parts of the eucalypt for rendering: A declaration in the source may be identified as the main target using the :main declaration metadata and we become the part rendered by default. targets may be defined and named using the :target declaration metadata and those targets can then be specified using the -t option to eu The -e option can be used in addition to other source file(s) to identify an expression to be rendered (e.g. eu test.eu -e x.y.z ) So eu 's ability to read JSON and YAML natively combined with the last options give a simple way to pick values out of structured data which can be very handy for \"querying\" services that return YAML or JSON data. > aws s3api list-buckets | eu -e 'Buckets map(lookup(:Name))' There is much more to this story. For instance eu can: accept several inputs to make definitions in earlier inputs available to subsequent inputs eu test1.eu test2.eu test3.eu accept YAML and JSON files as pure data to be merged in: eu data.yaml tools.eu accept YAML or JSON annotated with eucalypt to execute: eu data.yaml override the default extensions: eu yaml@info.txt automatically use Eufile files in the current folder hierarchy See command line for more complete documentation.","title":"Quick tour of the command line"},{"location":"anaphora-and-lambdas/","text":"Anaphora and Lambdas Eucalypt doesn't have a lambda syntax in itself and prefers to encourage other approaches in most cases where you would use a lambda. named functions function values from composites, combinators, partials anaphoric expressions, blocks or strings However, through the combination of two Eucalypt features, namely block anaphora and generalised lookup , you can express arbitrary lambdas as we'll see below. The various alternatives are considered one by one. Named functions Very likely, the clearest way to square a list of numbers is to map an explicitly named square function across it. square(x): x * x squares: [1, 2, 3] map(square) //=> [1, 4, 9] The drawbacks of this are: - polluting a namespace with a name that is need only once - arguably, a slightly tedious verbosity The first can be dealt with as follows: squares: { square(x): x * x }.([1, 2, 3] map(square)) //=> [1, 4, 9] This exploits a feature called generalised lookup . Why \"generalised lookup\"? In the simple case below, the dot signifies the \"lookup\" of key a in the block preceding the dot: x: { a: 3 b: 4 }.a //=> 3 We can generalise this by allowing arbitrary expressions in place of the a by evaluating the expression after the dot in the context of the namespace introduced by the block to the left. x: { a: 3 b: 4 }.(a + b) //=> 7 It works for any expression after the dot: x: { a: 3 b: 4 }.[a, b] //=> [3, 4] y: { a: 3 b: 4 }.{ c: a + b } //=> { c: 7 } z: { a: 3 b: 4 }.\"{a} and {b}\" //=> \"3 and 4\" Warning This is very effective for short and simple expressions but quickly gets very complicated and hard to understand if you use it too much. Nested or iterated generalised lookups are usually a bad idea. In the squares example above, generalised lookup is used to restrict the scope in which square is visible right down to the only expression which needs it. However in the case of a simple expression like the squaring example, a neater approach is to use expression anaphora . Expression Anaphora Any expression can become a function by referring to implicit parameters known as expression anaphora. These parameters are called _0 , _1 _2 , and so on. There is also an unnumbered anaphorus, _ , which we'll come back to. Just referring to these parameters is enough to turn an expression into a lambda. So an expression that refers _0 and _1 actually defines a function accepting two parameters: xs: zip-with(f, [1, 2, 3], [1, 2, 3]) //=> [3, 6, 9] # or more succinctly xs: zip-with(_0 + 2 * _1, [1, 2, 3], [1, 2, 3]) //=> [3, 6, 9] Warning Again, anaphora intended for use in simple cases where they are readable and readily understood. The scope of the implicit parameters is not easy to work out in complicated contexts. (It does not extend past catenation or commas in lists or function application tuples.) Anaphoric expressions are not, and not intended to be, a fully general lambda syntax. Unlike explicit lambda constructions, you cannot nest anaphoric expressions. squares: [1, 2, 3] map(_0 * _0) //=> [1, 4, 9] In cases where the position of the anaphora in the expression matches the parameter positions in the function call, you can omit the numbers. So, for instance, _0 + _1 can simply be written _ + _ , and _0 * _1 + x * _2 can be written _ * _ + x * _ . Each _ represents a different implicit parameter, which is why we had to write _0 * _0 in our squares example - it was important that the same parameter was reference twice. Sometime you need explicit parentheses to clarify the scope of expression anaphora: block: { a: 1 b: 2 } x: block (_.a) //=> 1 y: block lookup(:a) //=> 1 # # BUT NOT: block _.a # Sections Even more conciseness is on offer in some cases where the anaphora can be entirely omitted. Eucalypt will automatically insert anaphora when it detects gaps in an expression based on its knowledge of an operator's type. So it will automatically read (1 +) as (1 + _) , for example, defining a function of one parameter. Or (*) as (_ * _) , defining a function of two parameters. The parentheses may not even be necessary to delimit the expression: x: foldl(+, 0, [1, 2, 3]) = 6 Again, use of sections is recommended only for short expressions or where the intention is obvious. This level of tersity can lead to baffling code if abused. Block Anaphora Expression anaphora are scoped by an expression which is roughly defined as something within parentheses or something which can be the right hand side of a declaration. Sometimes however you would like to define a block-valued function. Imagine you wanted a two-parameter function which placed the parameters in a block with keys x and y : f(x, y): {x: x y: y } An attempt to define this using expression anaphora would fail. This defines a block with two identity functions: f: {x: _ y: _ } Instead, you can use block anaphora which are scoped by the block that contains them. The block anaphora are named \u20220 , \u20221 , \u20222 with a special unnumbered anaphor \u2022 , playing the same role as _ does for expression anaphora. \u2022 is the BULLET character (usually Option-8 on a Mac but you may find other convenient ways to type it). The slightly awkward character is chosen firstly because it looks like a hole and therefore makes sense as a placeholder, and secondly to discourage overuse of the feature... The following defines the function we want f: { x: \u2022 y: \u2022 } ...and can, of course, be used x: [[1, 2], [3, 4], [5, 6]] map({ x: \u2022 y: \u2022 } uncurry) Pseudo-lambdas Astute observers may realise that by combining generalised lookup and block anaphora you end up with something that's not a million miles away from a lambda syntax: f: { x: \u2022 y: \u2022 }.(x + y) Indeed this does allow declaration of anonymous functions with named parameters and can occasionally be useful but it still falls short of a fully general lambda construction because it cannot (at least for now) be nested. String Anaphora Analogously, Eucalypt's string interpolation syntax allows the use of anaphora {0} , {1} , {2} and the unnumbered {} to define functions which return strings. x: [1, 2, 3] map(\"#{}\") //=> [\"#1\", \"#2\", \"#3\"] Summary There are lots of ways to define functions but the clearest is just defining them with names using function declarations and for anything even slightly complicated this should be the default. The only things you should be tempted to define on the spot are things that are simple enough that the various species of anaphora can handle them neatly.","title":"Anaphora and Lambdas"},{"location":"anaphora-and-lambdas/#anaphora-and-lambdas","text":"Eucalypt doesn't have a lambda syntax in itself and prefers to encourage other approaches in most cases where you would use a lambda. named functions function values from composites, combinators, partials anaphoric expressions, blocks or strings However, through the combination of two Eucalypt features, namely block anaphora and generalised lookup , you can express arbitrary lambdas as we'll see below. The various alternatives are considered one by one.","title":"Anaphora and Lambdas"},{"location":"anaphora-and-lambdas/#named-functions","text":"Very likely, the clearest way to square a list of numbers is to map an explicitly named square function across it. square(x): x * x squares: [1, 2, 3] map(square) //=> [1, 4, 9] The drawbacks of this are: - polluting a namespace with a name that is need only once - arguably, a slightly tedious verbosity The first can be dealt with as follows: squares: { square(x): x * x }.([1, 2, 3] map(square)) //=> [1, 4, 9] This exploits a feature called generalised lookup . Why \"generalised lookup\"? In the simple case below, the dot signifies the \"lookup\" of key a in the block preceding the dot: x: { a: 3 b: 4 }.a //=> 3 We can generalise this by allowing arbitrary expressions in place of the a by evaluating the expression after the dot in the context of the namespace introduced by the block to the left. x: { a: 3 b: 4 }.(a + b) //=> 7 It works for any expression after the dot: x: { a: 3 b: 4 }.[a, b] //=> [3, 4] y: { a: 3 b: 4 }.{ c: a + b } //=> { c: 7 } z: { a: 3 b: 4 }.\"{a} and {b}\" //=> \"3 and 4\" Warning This is very effective for short and simple expressions but quickly gets very complicated and hard to understand if you use it too much. Nested or iterated generalised lookups are usually a bad idea. In the squares example above, generalised lookup is used to restrict the scope in which square is visible right down to the only expression which needs it. However in the case of a simple expression like the squaring example, a neater approach is to use expression anaphora .","title":"Named functions"},{"location":"anaphora-and-lambdas/#expression-anaphora","text":"Any expression can become a function by referring to implicit parameters known as expression anaphora. These parameters are called _0 , _1 _2 , and so on. There is also an unnumbered anaphorus, _ , which we'll come back to. Just referring to these parameters is enough to turn an expression into a lambda. So an expression that refers _0 and _1 actually defines a function accepting two parameters: xs: zip-with(f, [1, 2, 3], [1, 2, 3]) //=> [3, 6, 9] # or more succinctly xs: zip-with(_0 + 2 * _1, [1, 2, 3], [1, 2, 3]) //=> [3, 6, 9] Warning Again, anaphora intended for use in simple cases where they are readable and readily understood. The scope of the implicit parameters is not easy to work out in complicated contexts. (It does not extend past catenation or commas in lists or function application tuples.) Anaphoric expressions are not, and not intended to be, a fully general lambda syntax. Unlike explicit lambda constructions, you cannot nest anaphoric expressions. squares: [1, 2, 3] map(_0 * _0) //=> [1, 4, 9] In cases where the position of the anaphora in the expression matches the parameter positions in the function call, you can omit the numbers. So, for instance, _0 + _1 can simply be written _ + _ , and _0 * _1 + x * _2 can be written _ * _ + x * _ . Each _ represents a different implicit parameter, which is why we had to write _0 * _0 in our squares example - it was important that the same parameter was reference twice. Sometime you need explicit parentheses to clarify the scope of expression anaphora: block: { a: 1 b: 2 } x: block (_.a) //=> 1 y: block lookup(:a) //=> 1 # # BUT NOT: block _.a #","title":"Expression Anaphora"},{"location":"anaphora-and-lambdas/#sections","text":"Even more conciseness is on offer in some cases where the anaphora can be entirely omitted. Eucalypt will automatically insert anaphora when it detects gaps in an expression based on its knowledge of an operator's type. So it will automatically read (1 +) as (1 + _) , for example, defining a function of one parameter. Or (*) as (_ * _) , defining a function of two parameters. The parentheses may not even be necessary to delimit the expression: x: foldl(+, 0, [1, 2, 3]) = 6 Again, use of sections is recommended only for short expressions or where the intention is obvious. This level of tersity can lead to baffling code if abused.","title":"Sections"},{"location":"anaphora-and-lambdas/#block-anaphora","text":"Expression anaphora are scoped by an expression which is roughly defined as something within parentheses or something which can be the right hand side of a declaration. Sometimes however you would like to define a block-valued function. Imagine you wanted a two-parameter function which placed the parameters in a block with keys x and y : f(x, y): {x: x y: y } An attempt to define this using expression anaphora would fail. This defines a block with two identity functions: f: {x: _ y: _ } Instead, you can use block anaphora which are scoped by the block that contains them. The block anaphora are named \u20220 , \u20221 , \u20222 with a special unnumbered anaphor \u2022 , playing the same role as _ does for expression anaphora. \u2022 is the BULLET character (usually Option-8 on a Mac but you may find other convenient ways to type it). The slightly awkward character is chosen firstly because it looks like a hole and therefore makes sense as a placeholder, and secondly to discourage overuse of the feature... The following defines the function we want f: { x: \u2022 y: \u2022 } ...and can, of course, be used x: [[1, 2], [3, 4], [5, 6]] map({ x: \u2022 y: \u2022 } uncurry)","title":"Block Anaphora"},{"location":"anaphora-and-lambdas/#pseudo-lambdas","text":"Astute observers may realise that by combining generalised lookup and block anaphora you end up with something that's not a million miles away from a lambda syntax: f: { x: \u2022 y: \u2022 }.(x + y) Indeed this does allow declaration of anonymous functions with named parameters and can occasionally be useful but it still falls short of a fully general lambda construction because it cannot (at least for now) be nested.","title":"Pseudo-lambdas"},{"location":"anaphora-and-lambdas/#string-anaphora","text":"Analogously, Eucalypt's string interpolation syntax allows the use of anaphora {0} , {1} , {2} and the unnumbered {} to define functions which return strings. x: [1, 2, 3] map(\"#{}\") //=> [\"#1\", \"#2\", \"#3\"]","title":"String Anaphora"},{"location":"anaphora-and-lambdas/#summary","text":"There are lots of ways to define functions but the clearest is just defining them with names using function declarations and for anything even slightly complicated this should be the default. The only things you should be tempted to define on the spot are things that are simple enough that the various species of anaphora can handle them neatly.","title":"Summary"},{"location":"architecture/","text":"Eucalypt Architecture This document provides a comprehensive overview of Eucalypt's design and implementation architecture. Overview Eucalypt is a functional programming language and tool for generating, templating, rendering, and processing structured data formats like YAML, JSON, and TOML. Written in Rust (~44,000 lines), it features a classic multi-phase compiler design with an STG (Spineless Tagless G-machine) runtime for lazy evaluation. System Architecture High-Level Pipeline Source Code (*.eu files) \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Parsing Phase \u2502 src/syntax/ \u2502 Lexer \u2192 Parser \u2192 AST \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Core Phase \u2502 src/core/ \u2502 Desugar \u2192 Cook \u2192 \u2502 \u2502 Transform \u2192 Verify \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Evaluation Phase \u2502 src/eval/ \u2502 STG Compile \u2192 VM \u2192 \u2502 \u2502 Memory Management \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Export Phase \u2502 src/export/ \u2502 JSON/YAML/TOML/etc \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Module Structure eucalypt/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 bin/eu.rs # CLI entry point \u2502 \u251c\u2500\u2500 lib.rs # Library root \u2502 \u251c\u2500\u2500 common/ # Shared utilities \u2502 \u251c\u2500\u2500 syntax/ # Parsing and AST \u2502 \u2502 \u2514\u2500\u2500 rowan/ # Rowan-based incremental parser \u2502 \u251c\u2500\u2500 core/ # Core expression representation \u2502 \u2502 \u251c\u2500\u2500 desugar/ # AST to core transformation \u2502 \u2502 \u251c\u2500\u2500 cook/ # Operator fixity resolution \u2502 \u2502 \u251c\u2500\u2500 transform/ # Expression transformations \u2502 \u2502 \u251c\u2500\u2500 simplify/ # Optimisation passes \u2502 \u2502 \u251c\u2500\u2500 inline/ # Inlining passes \u2502 \u2502 \u251c\u2500\u2500 verify/ # Validation \u2502 \u2502 \u2514\u2500\u2500 analyse/ # Program analysis \u2502 \u251c\u2500\u2500 eval/ # Evaluation engine \u2502 \u2502 \u251c\u2500\u2500 stg/ # STG syntax and compiler \u2502 \u2502 \u251c\u2500\u2500 machine/ # Virtual machine \u2502 \u2502 \u2514\u2500\u2500 memory/ # Heap and garbage collection \u2502 \u251c\u2500\u2500 driver/ # CLI orchestration \u2502 \u251c\u2500\u2500 export/ # Output format generation \u2502 \u2514\u2500\u2500 import/ # Input format parsing \u251c\u2500\u2500 lib/ # Standard library (eucalypt source) \u2502 \u251c\u2500\u2500 prelude.eu # Core prelude \u2502 \u251c\u2500\u2500 test.eu # Test framework \u2502 \u2514\u2500\u2500 markup.eu # Markup utilities \u2514\u2500\u2500 docs/ # Documentation Parsing Pipeline The parsing pipeline transforms source text into a structured AST using Rowan, an incremental parsing library that preserves full source fidelity including whitespace and comments. Lexer Implementation : src/syntax/rowan/lex.rs , src/syntax/rowan/string_lex.rs The lexer ( Lexer<C> ) tokenises source text into a stream of SyntaxKind tokens: pub struct Lexer<C: Iterator<Item = char>> { chars: Peekable<C>, // Character stream with lookahead location: ByteIndex, // Source position tracking last_token: Option<SyntaxKind>, // Context for disambiguation whitespace_since_last_token: bool, token_buffer: VecDeque<(SyntaxKind, Span)>, } Key features: - Unicode-aware identifier and operator recognition - Context-sensitive tokenisation (distinguishes OPEN_PAREN from OPEN_PAREN_APPLY ) - String pattern lexing with interpolation support ( \"Hello {name}\" ) - Preserves trivia (whitespace, comments) for full-fidelity AST Token categories: - Delimiters: { } ( ) [ ] : , \\`` - Identifiers: foo , 'quoted name' , + , && - Literals: Numbers, strings, symbols ( :keyword ) - Annotations: Whitespace, comments ( #`) Parser Implementation : src/syntax/rowan/parse.rs The parser uses an event-driven recursive descent approach: pub struct Parser<'text> { tokens: Vec<(SyntaxKind, &'text str)>, next_token: usize, sink_stack: Vec<Box<dyn EventSink>>, errors: Vec<ParseError>, } Parse events: enum ParseEvent { StartNode(SyntaxKind), // Begin syntax node Finish, // Complete node Token(SyntaxKind), // Include token } Key parsing methods: - parse_unit() - Top-level file (no braces required) - parse_expression() / parse_soup() - Expression sequences - parse_block_expression() - { ... } blocks with declarations - parse_string_pattern() - Interpolated strings The parser maintains error recovery for LSP support, collecting errors while continuing to parse. AST Implementation : src/syntax/rowan/ast.rs The AST uses a two-layer design: 1. SyntaxNode (Rowan) - Rich, source-preserving tree 2. AST Nodes - Typed wrappers via macros macro_rules! ast_node { ($ast:ident, $kind:ident) => { pub struct $ast(SyntaxNode); impl AstNode for $ast { ... } } } Key AST types: Type Purpose Unit Top-level file structure Block Enclosed { ... } block Declaration Property/function declaration DeclHead Declaration name (before : ) DeclBody Declaration value (after : ) Soup Unordered expression sequence List List expression [a, b, c] Name Identifier reference Literal Literal value StringPattern Interpolated string Core Expression Representation The core representation is an intermediate language that facilitates powerful transformations while maintaining source information for error reporting. Expression Type Implementation : src/core/expr.rs The Expr<T> enum (where T: BoundTerm<String> ) represents all expression forms: pub enum Expr<T> { // Variables Var(Smid, Var<String>), // Free or bound variable // Primitives Literal(Smid, Primitive), // Number, string, symbol, bool, null // Binding forms Let(Smid, LetScope<T>, LetType), // Recursive let binding Lam(Smid, bool, LamScope<T>), // Lambda abstraction // Application App(Smid, T, Vec<T>), // Function application // Data structures List(Smid, Vec<T>), // List literal Block(Smid, BlockMap<T>), // Object/record literal // Operators (pre-cooking) Operator(Smid, Fixity, Precedence, T), Soup(Smid, Vec<T>), // Unresolved operator soup // Anaphora (implicit parameters) BlockAnaphor(Smid, ...), ExprAnaphor(Smid, ...), // Access Lookup(Smid, T, String, Option<T>), // Property access // Metadata Meta(Smid, T, T), // Expression with metadata // Intrinsics Intrinsic(Smid, String), // Built-in function reference // Error nodes ErrUnresolved, ErrRedeclaration, ... } Primitive types: pub enum Primitive { Str(String), Sym(String), Num(Number), Bool(bool), Null, } Standard wrapper : RcExpr provides reference-counted immutable expressions with substitution and transformation methods. Transformation Pipeline The core pipeline transforms expressions through several phases: AST \u2192 Desugar \u2192 Cook \u2192 Simplify \u2192 Inline \u2192 Verify \u2192 STG Desugaring Implementation : src/core/desugar/ Transforms parsed AST into core expressions: - Converts block declarations into recursive let bindings - Extracts targets and documentation metadata - Handles imports and cross-file references - Processes both native AST and embedded data (JSON/YAML) Cooking Implementation : src/core/cook/ Resolves operator precedence and anaphora: 1. Fixity distribution - Propagate operator precedence info 2. Anaphor filling - Infer missing implicit parameters ( (+ 10) \u2192 (_ + 10) ) 3. Shunting yard - Apply precedence climbing to linearise operator soup 4. Anaphor processing - Wrap lambda abstractions around anaphoric expressions Example transformation: (+ 10) \u2192 (\u03bb _ . (_ + 10)) a + b * c \u2192 (+ a (* b c)) // with standard precedence Simplification and Inlining Implementation : src/core/simplify/ , src/core/inline/ Compression - Remove eliminated bindings Pruning - Dead code elimination Inlining - Inline marked expressions Verification Implementation : src/core/verify/ Validates transformed expressions before STG compilation: - Binding verification - Content validation STG Compilation and Evaluation Model Eucalypt uses a Spineless Tagless G-machine (STG) as its evaluation model, providing lazy evaluation with memoisation. STG Syntax Implementation : src/eval/stg/syntax.rs The STG syntax represents executable code: pub enum StgSyn { Atom { evaluand: Ref }, // Value or reference Case { scrutinee, branches, fallback }, // Pattern matching (evaluation point) Cons { tag: Tag, args: Vec<Ref> }, // Data constructor App { callable: Ref, args: Vec<Ref> }, // Function application Bif { intrinsic: u8, args: Vec<Ref> }, // Built-in intrinsic Let { bindings: Vec<LambdaForm>, body }, // Non-recursive let LetRec { bindings: Vec<LambdaForm>, body }, // Recursive let Ann { smid: Smid, body }, // Source annotation Meta { meta: Ref, body: Ref }, // Metadata wrapper DeMeta { scrutinee, handler, or_else }, // Metadata destructure BlackHole, // Uninitialized marker } Reference types: pub enum Reference<T> { L(usize), // Local environment index G(usize), // Global environment index V(T), // Direct value (Native) } Lambda forms control laziness: - Lambda - Function with explicit arity - Thunk - Lazy expression (evaluated and updated in-place) - Value - Already in WHNF (no update needed) STG Compiler Implementation : src/eval/stg/compiler.rs The compiler transforms core expressions to STG syntax: impl Compiler { fn compile_body(&mut self, expr: &RcExpr) -> ProtoSyntax; fn compile_binding(&mut self, expr: &RcExpr) -> ProtoBinding; fn compile_lambda(&mut self, expr: &RcExpr) -> ProtoSyntax; fn compile_application(&mut self, f: &RcExpr, args: &[RcExpr]) -> ProtoSyntax; } Key decisions: - Thunk creation : Expressions not in WHNF and used more than once become thunks - WHNF detection : Constructors, native values, and metadata wrappers are WHNF - Deferred compilation : ProtoSyntax allows deferring binding construction until environment size is known Virtual Machine Implementation : src/eval/machine/vm.rs The STG machine is a state machine executing closures: pub struct MachineState { root_env: SynEnvPtr, // Empty root environment closure: SynClosure, // Current (code, environment) pair globals: SynEnvPtr, // Global bindings stack: Vec<Continuation>, // Continuation stack terminated: bool, annotation: Smid, // Current source location } Execution loop: fn run(&mut self) { while !self.terminated { if self.gc_check_needed() { self.collect_garbage(); } self.step(); } } Instruction dispatch ( handle_instruction ): Code Form Action Atom Resolve reference; push Update continuation if thunk Case Push Branch continuation, evaluate scrutinee Cons Return data constructor App Push ApplyTo continuation, evaluate callable Bif Execute intrinsic directly Let Allocate environment frame, continue in body LetRec Allocate frame with backfilled recursive references Continuations Implementation : src/eval/machine/cont.rs Four continuation types manage control flow: Branch - Pattern matching branches for CASE Update - Deferred thunk update (memoisation) ApplyTo - Pending arguments for function application DeMeta - Metadata destructuring handler Lazy Evaluation Laziness is achieved through thunks and updates: Thunk creation (compile time): Non-WHNF expressions become LambdaForm::Thunk Thunk evaluation (runtime): When a thunk is entered, push Update continuation Memoisation : After evaluation, update the environment slot with the result // When entering a local reference if closure.update() { stack.push(Continuation::Update { environment, index }); } // After evaluation completes Continuation::Update { environment, index } => { self.update(environment, index); // Replace thunk with result } Memory Management and Garbage Collection Eucalypt uses an Immix-inspired memory layout with mark-and-sweep collection. Memory Layout Implementation : src/eval/memory/heap.rs , src/eval/memory/bump.rs Block (32KB) \u251c\u2500\u2500 Line 0 (128B) \u2510 \u251c\u2500\u2500 Line 1 (128B) \u2502 256 lines per block \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 Line 255 \u2518 Size classes: - Small (< 128 bytes) - Single line - Medium (128B - 32KB) - Multiple lines within block - Large (> 32KB) - Dedicated Large Object Block Heap state: pub struct HeapState { head: Option<BumpBlock>, // Active small allocation overflow: Option<BumpBlock>, // Active medium allocation recycled: VecDeque<BumpBlock>, // Blocks with reusable holes rest: VecDeque<BumpBlock>, // Used blocks pending collection lobs: Vec<LargeObjectBlock>, // Large objects } Object Headers Implementation : src/eval/memory/header.rs Every object has a 16-byte header: pub struct AllocHeader { bits: HeaderBits, // Mark bit + forwarded flag alloc_length: u32, // Object size forwarded_to: Option<NonNull<()>>, // For potential evacuation } Garbage Collection Implementation : src/eval/memory/collect.rs Mark phase: 1. Reset line maps across all blocks 2. Breadth-first root scanning from machine state 3. Transitive closure following object references 4. Mark lines containing live objects Sweep phase: 1. Scan line maps in each block 2. Identify holes (2+ consecutive free lines) 3. Move recyclable blocks to recycled list Collection triggering: - When --heap-limit-mib is set and limit exceeded - Check performed every 500 VM execution steps - Emergency collection on allocation failure See gc-implementation.md for detailed analysis. The Prelude and Standard Library Intrinsic Functions Implementation : src/eval/intrinsics.rs , src/eval/stg/ Built-in functions are implemented in Rust and indexed by position: Categories: - Control flow : __IF , __PANIC , __TRUE , __FALSE , __NULL - Lists : __CONS , __HEAD , __TAIL , __NIL , __REVERSE - Blocks : __MERGE , __DEEPMERGE , __ELEMENTS , __BLOCK , __LOOKUP - Arithmetic : __ADD , __SUB , __MUL , __DIV , __MOD , comparisons - Strings : __STR , __SPLIT , __JOIN , __MATCH , __FMT - Metadata : __META , __WITHMETA - Time : __ZDT , __ZDT.PARSE , __ZDT.FORMAT - I/O : __io.ENV , __io.EPOCHTIME - Emission : __RENDER , __EMIT* family Each intrinsic implements the StgIntrinsic trait with direct access to machine state. Prelude Implementation : lib/prelude.eu The prelude (~29KB) is written entirely in eucalypt, wrapping intrinsics with ergonomic functions: List functions: - take , drop , nth ( !! ), fold / foldr , map , filter - append ( ++ ), concat , reverse , zip , group-by , qsort Block functions: - merge-all , keys , values , map-kv , map-keys , map-values - lookup-path , alter-value , update-value Combinators: - identity , const , compose ( \u2218 , ; ), flip , curry , uncurry String functions: - str.split , str.join , str.match , str.fmt , str.letters Loading: - Prelude is embedded in the binary at compile time - Loaded by default unless --no-prelude / -Q flag is used Driver and CLI Architecture Entry Point Implementation : src/bin/eu.rs , src/driver/options.rs The CLI uses clap v4 with derive macros: #[derive(Parser)] struct EucalyptCli { #[command(subcommand)] command: Option<Commands>, files: Vec<String>, // Global options... } enum Commands { Run(RunArgs), Test(TestArgs), Dump(DumpArgs), Fmt(FmtArgs), Explain(ExplainArgs), ListTargets(ListTargetsArgs), Version, } Modes of Operation Run (default): eu file.eu # Implicit run eu -e \"expression\" file.eu # With evaluand eu -x json file.eu # JSON output eu -t target file.eu # Select target Test: eu test tests/ # Run all tests in directory eu test -t specific file.eu # Run specific test target Format: eu fmt file.eu # Print formatted to stdout eu fmt --write file.eu # Modify in place eu fmt --check file.eu # Check formatting (exit 1 if needs format) Dump: eu dump ast file.eu # Dump AST eu dump desugared file.eu # Dump after desugaring eu dump stg file.eu # Dump STG syntax eu dump runtime # Dump intrinsic definitions Output Formats Implementation : src/export/ Emitters for each format implement the Emitter trait: - YAML ( yaml.rs ) - Uses yaml_rust , supports tags from metadata - JSON ( json.rs ) - Uses serde_json - TOML ( toml.rs ) - Structured output - Text ( text.rs ) - Plain text - EDN ( edn.rs ) - Clojure-like format - HTML ( html.rs ) - Markup with serialisation Input Handling Inputs are merged in order: 1. Prologue : Prelude, config files, build metadata, IO block 2. Explicit : Files and options ( -c / --collect-as ) 3. Epilogue : CLI evaluand ( -e ) Names from earlier inputs are available to later inputs. Key Design Decisions and Trade-offs Why STG? The STG machine provides a well-defined reference point for lazy functional language implementation: - Clear semantics for lazy evaluation with memoisation - Established compilation strategies - Potential for future optimisations Trade-off : More complex than direct interpretation but provides a solid foundation. Why Rowan for Parsing? Rowan provides: - Incremental parsing for IDE support - Full source fidelity (preserves whitespace and comments) - Error recovery for partial parsing Trade-off : More complex API than traditional parser generators. Core as Intermediate Language The core representation enables powerful transformations: - User-definable operator precedence resolved by syntax transformation - Block semantics (binding vs structuring) separated cleanly - Source information preserved through Smid for error reporting Trade-off : Additional compilation phase, but enables experimentation. Block Duality Eucalypt blocks serve two roles: 1. Name binding - Like let expressions 2. Data structuring - Like objects/records The core phase separates these into distinct Let and Block expressions. Trade-off : Elegant surface syntax at cost of semantic complexity. Immix-Inspired GC The memory layout provides: - Efficient bump allocation - Cache-friendly organisation - Effective hole reuse Current limitation : No evacuation/compaction (mark-sweep only). See gc-implementation.md for detailed analysis. Embedded Prelude The prelude is: - Written in eucalypt itself - Compiled into the binary - Loaded unless explicitly disabled Benefit : Dogfooding, consistent semantics Trade-off : Slower startup if prelude is large Performance Characteristics Compilation Parsing: O(n) with incremental support Desugaring: O(n) pass over AST Cooking: O(n log n) for operator precedence resolution STG compilation: O(n) with binding analysis Execution Bump allocation: O(1) for most objects Thunk updates: O(1) memoisation GC: O(live objects) for marking, O(total blocks) for sweeping Memory 16-byte header overhead per object Block-level allocation granularity Large objects get dedicated blocks Code Organisation Summary Directory Purpose Key Files src/syntax/rowan/ Incremental parsing lex.rs , parse.rs , ast.rs src/core/ Expression representation expr.rs src/core/desugar/ AST to core desugarer.rs src/core/cook/ Operator resolution shunt.rs , fixity.rs src/eval/stg/ STG syntax and compiler syntax.rs , compiler.rs src/eval/machine/ Virtual machine vm.rs , cont.rs , env.rs src/eval/memory/ Heap and GC heap.rs , collect.rs src/driver/ CLI orchestration options.rs , eval.rs src/export/ Output formats yaml.rs , json.rs lib/ Standard library prelude.eu Further Reading implementation.md - Brief implementation overview gc-implementation.md - Detailed GC documentation command-line.md - CLI usage guide syntax.md - Language syntax reference operators-and-identifiers.md - Operator definitions anaphora-and-lambdas.md - Implicit parameter handling","title":"Architecture"},{"location":"architecture/#eucalypt-architecture","text":"This document provides a comprehensive overview of Eucalypt's design and implementation architecture.","title":"Eucalypt Architecture"},{"location":"architecture/#overview","text":"Eucalypt is a functional programming language and tool for generating, templating, rendering, and processing structured data formats like YAML, JSON, and TOML. Written in Rust (~44,000 lines), it features a classic multi-phase compiler design with an STG (Spineless Tagless G-machine) runtime for lazy evaluation.","title":"Overview"},{"location":"architecture/#system-architecture","text":"","title":"System Architecture"},{"location":"architecture/#high-level-pipeline","text":"Source Code (*.eu files) \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Parsing Phase \u2502 src/syntax/ \u2502 Lexer \u2192 Parser \u2192 AST \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Core Phase \u2502 src/core/ \u2502 Desugar \u2192 Cook \u2192 \u2502 \u2502 Transform \u2192 Verify \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Evaluation Phase \u2502 src/eval/ \u2502 STG Compile \u2192 VM \u2192 \u2502 \u2502 Memory Management \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Export Phase \u2502 src/export/ \u2502 JSON/YAML/TOML/etc \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"High-Level Pipeline"},{"location":"architecture/#module-structure","text":"eucalypt/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 bin/eu.rs # CLI entry point \u2502 \u251c\u2500\u2500 lib.rs # Library root \u2502 \u251c\u2500\u2500 common/ # Shared utilities \u2502 \u251c\u2500\u2500 syntax/ # Parsing and AST \u2502 \u2502 \u2514\u2500\u2500 rowan/ # Rowan-based incremental parser \u2502 \u251c\u2500\u2500 core/ # Core expression representation \u2502 \u2502 \u251c\u2500\u2500 desugar/ # AST to core transformation \u2502 \u2502 \u251c\u2500\u2500 cook/ # Operator fixity resolution \u2502 \u2502 \u251c\u2500\u2500 transform/ # Expression transformations \u2502 \u2502 \u251c\u2500\u2500 simplify/ # Optimisation passes \u2502 \u2502 \u251c\u2500\u2500 inline/ # Inlining passes \u2502 \u2502 \u251c\u2500\u2500 verify/ # Validation \u2502 \u2502 \u2514\u2500\u2500 analyse/ # Program analysis \u2502 \u251c\u2500\u2500 eval/ # Evaluation engine \u2502 \u2502 \u251c\u2500\u2500 stg/ # STG syntax and compiler \u2502 \u2502 \u251c\u2500\u2500 machine/ # Virtual machine \u2502 \u2502 \u2514\u2500\u2500 memory/ # Heap and garbage collection \u2502 \u251c\u2500\u2500 driver/ # CLI orchestration \u2502 \u251c\u2500\u2500 export/ # Output format generation \u2502 \u2514\u2500\u2500 import/ # Input format parsing \u251c\u2500\u2500 lib/ # Standard library (eucalypt source) \u2502 \u251c\u2500\u2500 prelude.eu # Core prelude \u2502 \u251c\u2500\u2500 test.eu # Test framework \u2502 \u2514\u2500\u2500 markup.eu # Markup utilities \u2514\u2500\u2500 docs/ # Documentation","title":"Module Structure"},{"location":"architecture/#parsing-pipeline","text":"The parsing pipeline transforms source text into a structured AST using Rowan, an incremental parsing library that preserves full source fidelity including whitespace and comments.","title":"Parsing Pipeline"},{"location":"architecture/#lexer","text":"Implementation : src/syntax/rowan/lex.rs , src/syntax/rowan/string_lex.rs The lexer ( Lexer<C> ) tokenises source text into a stream of SyntaxKind tokens: pub struct Lexer<C: Iterator<Item = char>> { chars: Peekable<C>, // Character stream with lookahead location: ByteIndex, // Source position tracking last_token: Option<SyntaxKind>, // Context for disambiguation whitespace_since_last_token: bool, token_buffer: VecDeque<(SyntaxKind, Span)>, } Key features: - Unicode-aware identifier and operator recognition - Context-sensitive tokenisation (distinguishes OPEN_PAREN from OPEN_PAREN_APPLY ) - String pattern lexing with interpolation support ( \"Hello {name}\" ) - Preserves trivia (whitespace, comments) for full-fidelity AST Token categories: - Delimiters: { } ( ) [ ] : , \\`` - Identifiers: foo , 'quoted name' , + , && - Literals: Numbers, strings, symbols ( :keyword ) - Annotations: Whitespace, comments ( #`)","title":"Lexer"},{"location":"architecture/#parser","text":"Implementation : src/syntax/rowan/parse.rs The parser uses an event-driven recursive descent approach: pub struct Parser<'text> { tokens: Vec<(SyntaxKind, &'text str)>, next_token: usize, sink_stack: Vec<Box<dyn EventSink>>, errors: Vec<ParseError>, } Parse events: enum ParseEvent { StartNode(SyntaxKind), // Begin syntax node Finish, // Complete node Token(SyntaxKind), // Include token } Key parsing methods: - parse_unit() - Top-level file (no braces required) - parse_expression() / parse_soup() - Expression sequences - parse_block_expression() - { ... } blocks with declarations - parse_string_pattern() - Interpolated strings The parser maintains error recovery for LSP support, collecting errors while continuing to parse.","title":"Parser"},{"location":"architecture/#ast","text":"Implementation : src/syntax/rowan/ast.rs The AST uses a two-layer design: 1. SyntaxNode (Rowan) - Rich, source-preserving tree 2. AST Nodes - Typed wrappers via macros macro_rules! ast_node { ($ast:ident, $kind:ident) => { pub struct $ast(SyntaxNode); impl AstNode for $ast { ... } } } Key AST types: Type Purpose Unit Top-level file structure Block Enclosed { ... } block Declaration Property/function declaration DeclHead Declaration name (before : ) DeclBody Declaration value (after : ) Soup Unordered expression sequence List List expression [a, b, c] Name Identifier reference Literal Literal value StringPattern Interpolated string","title":"AST"},{"location":"architecture/#core-expression-representation","text":"The core representation is an intermediate language that facilitates powerful transformations while maintaining source information for error reporting.","title":"Core Expression Representation"},{"location":"architecture/#expression-type","text":"Implementation : src/core/expr.rs The Expr<T> enum (where T: BoundTerm<String> ) represents all expression forms: pub enum Expr<T> { // Variables Var(Smid, Var<String>), // Free or bound variable // Primitives Literal(Smid, Primitive), // Number, string, symbol, bool, null // Binding forms Let(Smid, LetScope<T>, LetType), // Recursive let binding Lam(Smid, bool, LamScope<T>), // Lambda abstraction // Application App(Smid, T, Vec<T>), // Function application // Data structures List(Smid, Vec<T>), // List literal Block(Smid, BlockMap<T>), // Object/record literal // Operators (pre-cooking) Operator(Smid, Fixity, Precedence, T), Soup(Smid, Vec<T>), // Unresolved operator soup // Anaphora (implicit parameters) BlockAnaphor(Smid, ...), ExprAnaphor(Smid, ...), // Access Lookup(Smid, T, String, Option<T>), // Property access // Metadata Meta(Smid, T, T), // Expression with metadata // Intrinsics Intrinsic(Smid, String), // Built-in function reference // Error nodes ErrUnresolved, ErrRedeclaration, ... } Primitive types: pub enum Primitive { Str(String), Sym(String), Num(Number), Bool(bool), Null, } Standard wrapper : RcExpr provides reference-counted immutable expressions with substitution and transformation methods.","title":"Expression Type"},{"location":"architecture/#transformation-pipeline","text":"The core pipeline transforms expressions through several phases: AST \u2192 Desugar \u2192 Cook \u2192 Simplify \u2192 Inline \u2192 Verify \u2192 STG","title":"Transformation Pipeline"},{"location":"architecture/#desugaring","text":"Implementation : src/core/desugar/ Transforms parsed AST into core expressions: - Converts block declarations into recursive let bindings - Extracts targets and documentation metadata - Handles imports and cross-file references - Processes both native AST and embedded data (JSON/YAML)","title":"Desugaring"},{"location":"architecture/#cooking","text":"Implementation : src/core/cook/ Resolves operator precedence and anaphora: 1. Fixity distribution - Propagate operator precedence info 2. Anaphor filling - Infer missing implicit parameters ( (+ 10) \u2192 (_ + 10) ) 3. Shunting yard - Apply precedence climbing to linearise operator soup 4. Anaphor processing - Wrap lambda abstractions around anaphoric expressions Example transformation: (+ 10) \u2192 (\u03bb _ . (_ + 10)) a + b * c \u2192 (+ a (* b c)) // with standard precedence","title":"Cooking"},{"location":"architecture/#simplification-and-inlining","text":"Implementation : src/core/simplify/ , src/core/inline/ Compression - Remove eliminated bindings Pruning - Dead code elimination Inlining - Inline marked expressions","title":"Simplification and Inlining"},{"location":"architecture/#verification","text":"Implementation : src/core/verify/ Validates transformed expressions before STG compilation: - Binding verification - Content validation","title":"Verification"},{"location":"architecture/#stg-compilation-and-evaluation-model","text":"Eucalypt uses a Spineless Tagless G-machine (STG) as its evaluation model, providing lazy evaluation with memoisation.","title":"STG Compilation and Evaluation Model"},{"location":"architecture/#stg-syntax","text":"Implementation : src/eval/stg/syntax.rs The STG syntax represents executable code: pub enum StgSyn { Atom { evaluand: Ref }, // Value or reference Case { scrutinee, branches, fallback }, // Pattern matching (evaluation point) Cons { tag: Tag, args: Vec<Ref> }, // Data constructor App { callable: Ref, args: Vec<Ref> }, // Function application Bif { intrinsic: u8, args: Vec<Ref> }, // Built-in intrinsic Let { bindings: Vec<LambdaForm>, body }, // Non-recursive let LetRec { bindings: Vec<LambdaForm>, body }, // Recursive let Ann { smid: Smid, body }, // Source annotation Meta { meta: Ref, body: Ref }, // Metadata wrapper DeMeta { scrutinee, handler, or_else }, // Metadata destructure BlackHole, // Uninitialized marker } Reference types: pub enum Reference<T> { L(usize), // Local environment index G(usize), // Global environment index V(T), // Direct value (Native) } Lambda forms control laziness: - Lambda - Function with explicit arity - Thunk - Lazy expression (evaluated and updated in-place) - Value - Already in WHNF (no update needed)","title":"STG Syntax"},{"location":"architecture/#stg-compiler","text":"Implementation : src/eval/stg/compiler.rs The compiler transforms core expressions to STG syntax: impl Compiler { fn compile_body(&mut self, expr: &RcExpr) -> ProtoSyntax; fn compile_binding(&mut self, expr: &RcExpr) -> ProtoBinding; fn compile_lambda(&mut self, expr: &RcExpr) -> ProtoSyntax; fn compile_application(&mut self, f: &RcExpr, args: &[RcExpr]) -> ProtoSyntax; } Key decisions: - Thunk creation : Expressions not in WHNF and used more than once become thunks - WHNF detection : Constructors, native values, and metadata wrappers are WHNF - Deferred compilation : ProtoSyntax allows deferring binding construction until environment size is known","title":"STG Compiler"},{"location":"architecture/#virtual-machine","text":"Implementation : src/eval/machine/vm.rs The STG machine is a state machine executing closures: pub struct MachineState { root_env: SynEnvPtr, // Empty root environment closure: SynClosure, // Current (code, environment) pair globals: SynEnvPtr, // Global bindings stack: Vec<Continuation>, // Continuation stack terminated: bool, annotation: Smid, // Current source location } Execution loop: fn run(&mut self) { while !self.terminated { if self.gc_check_needed() { self.collect_garbage(); } self.step(); } } Instruction dispatch ( handle_instruction ): Code Form Action Atom Resolve reference; push Update continuation if thunk Case Push Branch continuation, evaluate scrutinee Cons Return data constructor App Push ApplyTo continuation, evaluate callable Bif Execute intrinsic directly Let Allocate environment frame, continue in body LetRec Allocate frame with backfilled recursive references","title":"Virtual Machine"},{"location":"architecture/#continuations","text":"Implementation : src/eval/machine/cont.rs Four continuation types manage control flow: Branch - Pattern matching branches for CASE Update - Deferred thunk update (memoisation) ApplyTo - Pending arguments for function application DeMeta - Metadata destructuring handler","title":"Continuations"},{"location":"architecture/#lazy-evaluation","text":"Laziness is achieved through thunks and updates: Thunk creation (compile time): Non-WHNF expressions become LambdaForm::Thunk Thunk evaluation (runtime): When a thunk is entered, push Update continuation Memoisation : After evaluation, update the environment slot with the result // When entering a local reference if closure.update() { stack.push(Continuation::Update { environment, index }); } // After evaluation completes Continuation::Update { environment, index } => { self.update(environment, index); // Replace thunk with result }","title":"Lazy Evaluation"},{"location":"architecture/#memory-management-and-garbage-collection","text":"Eucalypt uses an Immix-inspired memory layout with mark-and-sweep collection.","title":"Memory Management and Garbage Collection"},{"location":"architecture/#memory-layout","text":"Implementation : src/eval/memory/heap.rs , src/eval/memory/bump.rs Block (32KB) \u251c\u2500\u2500 Line 0 (128B) \u2510 \u251c\u2500\u2500 Line 1 (128B) \u2502 256 lines per block \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 Line 255 \u2518 Size classes: - Small (< 128 bytes) - Single line - Medium (128B - 32KB) - Multiple lines within block - Large (> 32KB) - Dedicated Large Object Block Heap state: pub struct HeapState { head: Option<BumpBlock>, // Active small allocation overflow: Option<BumpBlock>, // Active medium allocation recycled: VecDeque<BumpBlock>, // Blocks with reusable holes rest: VecDeque<BumpBlock>, // Used blocks pending collection lobs: Vec<LargeObjectBlock>, // Large objects }","title":"Memory Layout"},{"location":"architecture/#object-headers","text":"Implementation : src/eval/memory/header.rs Every object has a 16-byte header: pub struct AllocHeader { bits: HeaderBits, // Mark bit + forwarded flag alloc_length: u32, // Object size forwarded_to: Option<NonNull<()>>, // For potential evacuation }","title":"Object Headers"},{"location":"architecture/#garbage-collection","text":"Implementation : src/eval/memory/collect.rs Mark phase: 1. Reset line maps across all blocks 2. Breadth-first root scanning from machine state 3. Transitive closure following object references 4. Mark lines containing live objects Sweep phase: 1. Scan line maps in each block 2. Identify holes (2+ consecutive free lines) 3. Move recyclable blocks to recycled list Collection triggering: - When --heap-limit-mib is set and limit exceeded - Check performed every 500 VM execution steps - Emergency collection on allocation failure See gc-implementation.md for detailed analysis.","title":"Garbage Collection"},{"location":"architecture/#the-prelude-and-standard-library","text":"","title":"The Prelude and Standard Library"},{"location":"architecture/#intrinsic-functions","text":"Implementation : src/eval/intrinsics.rs , src/eval/stg/ Built-in functions are implemented in Rust and indexed by position: Categories: - Control flow : __IF , __PANIC , __TRUE , __FALSE , __NULL - Lists : __CONS , __HEAD , __TAIL , __NIL , __REVERSE - Blocks : __MERGE , __DEEPMERGE , __ELEMENTS , __BLOCK , __LOOKUP - Arithmetic : __ADD , __SUB , __MUL , __DIV , __MOD , comparisons - Strings : __STR , __SPLIT , __JOIN , __MATCH , __FMT - Metadata : __META , __WITHMETA - Time : __ZDT , __ZDT.PARSE , __ZDT.FORMAT - I/O : __io.ENV , __io.EPOCHTIME - Emission : __RENDER , __EMIT* family Each intrinsic implements the StgIntrinsic trait with direct access to machine state.","title":"Intrinsic Functions"},{"location":"architecture/#prelude","text":"Implementation : lib/prelude.eu The prelude (~29KB) is written entirely in eucalypt, wrapping intrinsics with ergonomic functions: List functions: - take , drop , nth ( !! ), fold / foldr , map , filter - append ( ++ ), concat , reverse , zip , group-by , qsort Block functions: - merge-all , keys , values , map-kv , map-keys , map-values - lookup-path , alter-value , update-value Combinators: - identity , const , compose ( \u2218 , ; ), flip , curry , uncurry String functions: - str.split , str.join , str.match , str.fmt , str.letters Loading: - Prelude is embedded in the binary at compile time - Loaded by default unless --no-prelude / -Q flag is used","title":"Prelude"},{"location":"architecture/#driver-and-cli-architecture","text":"","title":"Driver and CLI Architecture"},{"location":"architecture/#entry-point","text":"Implementation : src/bin/eu.rs , src/driver/options.rs The CLI uses clap v4 with derive macros: #[derive(Parser)] struct EucalyptCli { #[command(subcommand)] command: Option<Commands>, files: Vec<String>, // Global options... } enum Commands { Run(RunArgs), Test(TestArgs), Dump(DumpArgs), Fmt(FmtArgs), Explain(ExplainArgs), ListTargets(ListTargetsArgs), Version, }","title":"Entry Point"},{"location":"architecture/#modes-of-operation","text":"Run (default): eu file.eu # Implicit run eu -e \"expression\" file.eu # With evaluand eu -x json file.eu # JSON output eu -t target file.eu # Select target Test: eu test tests/ # Run all tests in directory eu test -t specific file.eu # Run specific test target Format: eu fmt file.eu # Print formatted to stdout eu fmt --write file.eu # Modify in place eu fmt --check file.eu # Check formatting (exit 1 if needs format) Dump: eu dump ast file.eu # Dump AST eu dump desugared file.eu # Dump after desugaring eu dump stg file.eu # Dump STG syntax eu dump runtime # Dump intrinsic definitions","title":"Modes of Operation"},{"location":"architecture/#output-formats","text":"Implementation : src/export/ Emitters for each format implement the Emitter trait: - YAML ( yaml.rs ) - Uses yaml_rust , supports tags from metadata - JSON ( json.rs ) - Uses serde_json - TOML ( toml.rs ) - Structured output - Text ( text.rs ) - Plain text - EDN ( edn.rs ) - Clojure-like format - HTML ( html.rs ) - Markup with serialisation","title":"Output Formats"},{"location":"architecture/#input-handling","text":"Inputs are merged in order: 1. Prologue : Prelude, config files, build metadata, IO block 2. Explicit : Files and options ( -c / --collect-as ) 3. Epilogue : CLI evaluand ( -e ) Names from earlier inputs are available to later inputs.","title":"Input Handling"},{"location":"architecture/#key-design-decisions-and-trade-offs","text":"","title":"Key Design Decisions and Trade-offs"},{"location":"architecture/#why-stg","text":"The STG machine provides a well-defined reference point for lazy functional language implementation: - Clear semantics for lazy evaluation with memoisation - Established compilation strategies - Potential for future optimisations Trade-off : More complex than direct interpretation but provides a solid foundation.","title":"Why STG?"},{"location":"architecture/#why-rowan-for-parsing","text":"Rowan provides: - Incremental parsing for IDE support - Full source fidelity (preserves whitespace and comments) - Error recovery for partial parsing Trade-off : More complex API than traditional parser generators.","title":"Why Rowan for Parsing?"},{"location":"architecture/#core-as-intermediate-language","text":"The core representation enables powerful transformations: - User-definable operator precedence resolved by syntax transformation - Block semantics (binding vs structuring) separated cleanly - Source information preserved through Smid for error reporting Trade-off : Additional compilation phase, but enables experimentation.","title":"Core as Intermediate Language"},{"location":"architecture/#block-duality","text":"Eucalypt blocks serve two roles: 1. Name binding - Like let expressions 2. Data structuring - Like objects/records The core phase separates these into distinct Let and Block expressions. Trade-off : Elegant surface syntax at cost of semantic complexity.","title":"Block Duality"},{"location":"architecture/#immix-inspired-gc","text":"The memory layout provides: - Efficient bump allocation - Cache-friendly organisation - Effective hole reuse Current limitation : No evacuation/compaction (mark-sweep only). See gc-implementation.md for detailed analysis.","title":"Immix-Inspired GC"},{"location":"architecture/#embedded-prelude","text":"The prelude is: - Written in eucalypt itself - Compiled into the binary - Loaded unless explicitly disabled Benefit : Dogfooding, consistent semantics Trade-off : Slower startup if prelude is large","title":"Embedded Prelude"},{"location":"architecture/#performance-characteristics","text":"","title":"Performance Characteristics"},{"location":"architecture/#compilation","text":"Parsing: O(n) with incremental support Desugaring: O(n) pass over AST Cooking: O(n log n) for operator precedence resolution STG compilation: O(n) with binding analysis","title":"Compilation"},{"location":"architecture/#execution","text":"Bump allocation: O(1) for most objects Thunk updates: O(1) memoisation GC: O(live objects) for marking, O(total blocks) for sweeping","title":"Execution"},{"location":"architecture/#memory","text":"16-byte header overhead per object Block-level allocation granularity Large objects get dedicated blocks","title":"Memory"},{"location":"architecture/#code-organisation-summary","text":"Directory Purpose Key Files src/syntax/rowan/ Incremental parsing lex.rs , parse.rs , ast.rs src/core/ Expression representation expr.rs src/core/desugar/ AST to core desugarer.rs src/core/cook/ Operator resolution shunt.rs , fixity.rs src/eval/stg/ STG syntax and compiler syntax.rs , compiler.rs src/eval/machine/ Virtual machine vm.rs , cont.rs , env.rs src/eval/memory/ Heap and GC heap.rs , collect.rs src/driver/ CLI orchestration options.rs , eval.rs src/export/ Output formats yaml.rs , json.rs lib/ Standard library prelude.eu","title":"Code Organisation Summary"},{"location":"architecture/#further-reading","text":"implementation.md - Brief implementation overview gc-implementation.md - Detailed GC documentation command-line.md - CLI usage guide syntax.md - Language syntax reference operators-and-identifiers.md - Operator definitions anaphora-and-lambdas.md - Implicit parameter handling","title":"Further Reading"},{"location":"command-line/","text":"eu command line Eucalypt is available as a command line tool, eu , which reads inputs and writes outputs. Everything it does in between is purely functional and there is no mutable state. It is intended to be simple to use in unix pipelines. eu --version # shows the current eu version eu --help # lists command line options Command Structure The eu command uses a subcommand structure for clarity and extensibility: eu [GLOBAL_OPTIONS] [SUBCOMMAND] [SUBCOMMAND_OPTIONS] [FILES...] Subcommands run (default) - Evaluate eucalypt code test - Run tests dump - Dump intermediate representations version - Show version information explain - Explain what would be executed list-targets - List targets defined in the source fmt - Format eucalypt source files lsp - Start the Language Server Protocol server When no subcommand is specified, run is used by default, so these are equivalent: eu file.eu eu run file.eu Inputs Files / stdin eu can read several inputs, specified by command line arguments. Inputs specify text data from: files stdin internal resources (ignored for now) (in future) HTTPS URLS or Git refs ...of which the first two are the common case. In the simplest case, file inputs are specified by file name, stdin is specified by - . So eu a.yaml - b.eu ...will read input from a.yaml , stdin and b.eu . Each will be read into eucalypt 's core representation and merged before output is rendered. Input format Inputs must be one of the formats that eucalypt supports, which at present, are: yaml json jsonl (JSON Lines) toml edn xml csv text Of these yaml, json, toml, edn and xml return blocks; jsonl, csv and text return lists. Inputs that return lists frequently need to be named (see below) to allow them to be used. Usually the format is inferred from file extension but it can be overridden on an input by input basis using a format@ prefix. For instance: eu yaml@a.txt json@- yaml@b.txt ...will read YAML from a.txt , JSON from stdin and YAML from b.txt . Named inputs Finally inputs can be named using a name= prefix. This alters the way that data is merged by making the contents of an input available in a block or list with the specified name, instead of at the top level. Suppose we have two inputs: foo: bar x: 42 then eu a.yaml b.eu would generate: foo: bar x: 42 but eu data=a.yaml b.eu would generate: data: foo: bar x: 42 This can be useful for various reasons, particularly when: the form of the input's content is not known in advance the input's content is a list rather than a block Full input syntax The full input syntax is therefore: [name=][format@][URL/file] This applies at the command line and also when specifying imports in .eu files. stdin defaulting When no inputs are specified and eu is being used in a pipeline, it will accept input from stdin by default, making it easy to pipe JSON or YAML from other tools into eu. For example, this takes JSON from the aws CLI and formats it as YAML to stdout. aws s3-api list-buckets | eu How inputs are merged When several inputs are listed, names from earlier inputs become available to later inputs, but the content that will be rendered is that of the final input. So for instance: a.eu x: 4 y: 8 b.eu z: x + y eu a.eu b.eu will output z: 12 The common use cases are: - a final input containing logic to inspect or process data provided by previous inputs - a final input which uses functions defined in earlier inputs to process data provided in previous inputs If you want to __render_ contents of earlier inputs, you need a named input to provide a name for that content which you can then use. For instance: eu r=a.eu b.eu -e r will render: x: 4 y: 8 --collect-as and --name-inputs Occasionally it is useful to aggregate data from an arbitrary number of sources files, typically specified by shell wildcards. To refer to this data we need to introduce a name for the collection of data. This is what the command line switch --collect-as / -c is for. eu --collect-as inputs *.eu ...will render: inputs: - x: 4 y: 8 - z: 12 It is common to use -e to select an item to render: eu -c inputs *.eu -e 'inputs head' ...renders: x: 4 y: 8 If you are likely to need to refer to inputs by name, you can add --name-inputs / -N to pass inputs as a block instead of a list: eu --collect-as inputs *.eu ...renders: inputs: a.eu: x: 4 y: 8 b.eu: z: 12 This makes it possible to easier to invoke specific functions from named inputs although you will need single-quote name syntax to use the generated names which contain '.'s. Outputs In the current version, eu can only generate one output. Output format Output is rendered as YAML by default. Other formats can be specified using the -x command line option: eu -x json # for JSON eu -x text # for plain text JSON is such a common case that there is a shortcut: -j . Output targets By default, eucalypt renders all the content of the final input to output. There are various ways to override this. First, :target metadata can be specified in the final input to identify different parts for potential export. To list the targets found in the specified inputs, use the list-targets subcommand. eu list-targets file.eu ...and a particular target can be selected for render using -t . eu -t my-target If there is a target called \"main\" it will be used by default unless another target is specified. Evaluands In addition to inputs, an evaluand can be specified at the command line. This is a eucalypt expression which has access to all names defined in the inputs and replaces the input body or targets as the data to export. It can be used to select content or derive values from data in the inputs: $ aws s3api list-buckets | eu -e 'Buckets map(lookup(:CreationDate)) head' 2016-12-25T14:22:30.000Z ...or just to test out short expressions or command line features: $ eu -e '{a: 1 b: 2 * 2}' -j {\"a\": 1, \"b\": 4} Passing Arguments to Programs You can pass command-line arguments to your eucalypt program using the -- separator. Arguments after -- are available via io.args : $ eu -e 'io.args' -- foo bar baz --- - foo - bar - baz This is useful for writing eucalypt scripts that accept parameters: # greet.eu name: io.args head-or(\"World\") greeting: \"Hello, {name}!\" $ eu greet.eu -e greeting -- Alice --- Hello, Alice! Arguments are passed as strings. Use num to convert numeric arguments: # sum.eu total: io.args map(num) foldl((+), 0) $ eu sum.eu -e total -- 1 2 3 4 5 --- 15 When no arguments are passed, io.args is an empty list: $ eu -e 'io.args nil?' --- true Suppressing prelude A standard prelude containing many functions and operators is automatically prepended to the input list. This can be suppressed using -Q if it is not required or if you would like to provide an alternative. Warning Many very basic facilities - like the definition of true and false and if - are provided by the prelude so suppressing it leaves a very bare environment. Debugging eu has a variety of command line switches for dumping out internal representations or tracing execution. The dump subcommand provides access to intermediate representations: eu dump ast file.eu # Parse and dump syntax tree eu dump desugared file.eu # Dump core expression eu dump stg file.eu # Dump compiled STG syntax eu list-targets file.eu # List available targets Use eu --help and eu <subcommand> --help for complete option lists. Formatting Source Files The fmt subcommand formats eucalypt source files for consistent style: eu fmt file.eu # Print formatted output to stdout eu fmt --write file.eu # Format in place eu fmt --check file.eu # Check formatting (exit 1 if not formatted) eu fmt *.eu --write # Format multiple files in place Options -w, --width <WIDTH> - Line width for formatting (default: 80) --write - Modify files in place --check - Check if files are formatted (exit 1 if not) --reformat - Full reformatting mode (instead of conservative) --indent <INDENT> - Indent size in spaces (default: 2) The formatter has two modes: Conservative mode (default) - Preserves original formatting choices where possible, only reformatting where necessary Reformat mode ( --reformat ) - Full reformatting that applies consistent style throughout Backward Compatibility All existing command patterns continue to work unchanged: eu file.eu # Still works (uses run subcommand) eu -e \"expression\" # Still works (uses run subcommand) eu -j file.eu # Still works (JSON output) eu -S -Q file.eu # Still works (statistics, no prelude)","title":"Command Line"},{"location":"command-line/#eu-command-line","text":"Eucalypt is available as a command line tool, eu , which reads inputs and writes outputs. Everything it does in between is purely functional and there is no mutable state. It is intended to be simple to use in unix pipelines. eu --version # shows the current eu version eu --help # lists command line options","title":"eu command line"},{"location":"command-line/#command-structure","text":"The eu command uses a subcommand structure for clarity and extensibility: eu [GLOBAL_OPTIONS] [SUBCOMMAND] [SUBCOMMAND_OPTIONS] [FILES...]","title":"Command Structure"},{"location":"command-line/#subcommands","text":"run (default) - Evaluate eucalypt code test - Run tests dump - Dump intermediate representations version - Show version information explain - Explain what would be executed list-targets - List targets defined in the source fmt - Format eucalypt source files lsp - Start the Language Server Protocol server When no subcommand is specified, run is used by default, so these are equivalent: eu file.eu eu run file.eu","title":"Subcommands"},{"location":"command-line/#inputs","text":"","title":"Inputs"},{"location":"command-line/#files-stdin","text":"eu can read several inputs, specified by command line arguments. Inputs specify text data from: files stdin internal resources (ignored for now) (in future) HTTPS URLS or Git refs ...of which the first two are the common case. In the simplest case, file inputs are specified by file name, stdin is specified by - . So eu a.yaml - b.eu ...will read input from a.yaml , stdin and b.eu . Each will be read into eucalypt 's core representation and merged before output is rendered.","title":"Files / stdin"},{"location":"command-line/#input-format","text":"Inputs must be one of the formats that eucalypt supports, which at present, are: yaml json jsonl (JSON Lines) toml edn xml csv text Of these yaml, json, toml, edn and xml return blocks; jsonl, csv and text return lists. Inputs that return lists frequently need to be named (see below) to allow them to be used. Usually the format is inferred from file extension but it can be overridden on an input by input basis using a format@ prefix. For instance: eu yaml@a.txt json@- yaml@b.txt ...will read YAML from a.txt , JSON from stdin and YAML from b.txt .","title":"Input format"},{"location":"command-line/#named-inputs","text":"Finally inputs can be named using a name= prefix. This alters the way that data is merged by making the contents of an input available in a block or list with the specified name, instead of at the top level. Suppose we have two inputs: foo: bar x: 42 then eu a.yaml b.eu would generate: foo: bar x: 42 but eu data=a.yaml b.eu would generate: data: foo: bar x: 42 This can be useful for various reasons, particularly when: the form of the input's content is not known in advance the input's content is a list rather than a block","title":"Named inputs"},{"location":"command-line/#full-input-syntax","text":"The full input syntax is therefore: [name=][format@][URL/file] This applies at the command line and also when specifying imports in .eu files.","title":"Full input syntax"},{"location":"command-line/#stdin-defaulting","text":"When no inputs are specified and eu is being used in a pipeline, it will accept input from stdin by default, making it easy to pipe JSON or YAML from other tools into eu. For example, this takes JSON from the aws CLI and formats it as YAML to stdout. aws s3-api list-buckets | eu","title":"stdin defaulting"},{"location":"command-line/#how-inputs-are-merged","text":"When several inputs are listed, names from earlier inputs become available to later inputs, but the content that will be rendered is that of the final input. So for instance: a.eu x: 4 y: 8 b.eu z: x + y eu a.eu b.eu will output z: 12 The common use cases are: - a final input containing logic to inspect or process data provided by previous inputs - a final input which uses functions defined in earlier inputs to process data provided in previous inputs If you want to __render_ contents of earlier inputs, you need a named input to provide a name for that content which you can then use. For instance: eu r=a.eu b.eu -e r will render: x: 4 y: 8","title":"How inputs are merged"},{"location":"command-line/#-collect-as-and-name-inputs","text":"Occasionally it is useful to aggregate data from an arbitrary number of sources files, typically specified by shell wildcards. To refer to this data we need to introduce a name for the collection of data. This is what the command line switch --collect-as / -c is for. eu --collect-as inputs *.eu ...will render: inputs: - x: 4 y: 8 - z: 12 It is common to use -e to select an item to render: eu -c inputs *.eu -e 'inputs head' ...renders: x: 4 y: 8 If you are likely to need to refer to inputs by name, you can add --name-inputs / -N to pass inputs as a block instead of a list: eu --collect-as inputs *.eu ...renders: inputs: a.eu: x: 4 y: 8 b.eu: z: 12 This makes it possible to easier to invoke specific functions from named inputs although you will need single-quote name syntax to use the generated names which contain '.'s.","title":"--collect-as and --name-inputs"},{"location":"command-line/#outputs","text":"In the current version, eu can only generate one output.","title":"Outputs"},{"location":"command-line/#output-format","text":"Output is rendered as YAML by default. Other formats can be specified using the -x command line option: eu -x json # for JSON eu -x text # for plain text JSON is such a common case that there is a shortcut: -j .","title":"Output format"},{"location":"command-line/#output-targets","text":"By default, eucalypt renders all the content of the final input to output. There are various ways to override this. First, :target metadata can be specified in the final input to identify different parts for potential export. To list the targets found in the specified inputs, use the list-targets subcommand. eu list-targets file.eu ...and a particular target can be selected for render using -t . eu -t my-target If there is a target called \"main\" it will be used by default unless another target is specified.","title":"Output targets"},{"location":"command-line/#evaluands","text":"In addition to inputs, an evaluand can be specified at the command line. This is a eucalypt expression which has access to all names defined in the inputs and replaces the input body or targets as the data to export. It can be used to select content or derive values from data in the inputs: $ aws s3api list-buckets | eu -e 'Buckets map(lookup(:CreationDate)) head' 2016-12-25T14:22:30.000Z ...or just to test out short expressions or command line features: $ eu -e '{a: 1 b: 2 * 2}' -j {\"a\": 1, \"b\": 4}","title":"Evaluands"},{"location":"command-line/#passing-arguments-to-programs","text":"You can pass command-line arguments to your eucalypt program using the -- separator. Arguments after -- are available via io.args : $ eu -e 'io.args' -- foo bar baz --- - foo - bar - baz This is useful for writing eucalypt scripts that accept parameters: # greet.eu name: io.args head-or(\"World\") greeting: \"Hello, {name}!\" $ eu greet.eu -e greeting -- Alice --- Hello, Alice! Arguments are passed as strings. Use num to convert numeric arguments: # sum.eu total: io.args map(num) foldl((+), 0) $ eu sum.eu -e total -- 1 2 3 4 5 --- 15 When no arguments are passed, io.args is an empty list: $ eu -e 'io.args nil?' --- true","title":"Passing Arguments to Programs"},{"location":"command-line/#suppressing-prelude","text":"A standard prelude containing many functions and operators is automatically prepended to the input list. This can be suppressed using -Q if it is not required or if you would like to provide an alternative. Warning Many very basic facilities - like the definition of true and false and if - are provided by the prelude so suppressing it leaves a very bare environment.","title":"Suppressing prelude"},{"location":"command-line/#debugging","text":"eu has a variety of command line switches for dumping out internal representations or tracing execution. The dump subcommand provides access to intermediate representations: eu dump ast file.eu # Parse and dump syntax tree eu dump desugared file.eu # Dump core expression eu dump stg file.eu # Dump compiled STG syntax eu list-targets file.eu # List available targets Use eu --help and eu <subcommand> --help for complete option lists.","title":"Debugging"},{"location":"command-line/#formatting-source-files","text":"The fmt subcommand formats eucalypt source files for consistent style: eu fmt file.eu # Print formatted output to stdout eu fmt --write file.eu # Format in place eu fmt --check file.eu # Check formatting (exit 1 if not formatted) eu fmt *.eu --write # Format multiple files in place","title":"Formatting Source Files"},{"location":"command-line/#options","text":"-w, --width <WIDTH> - Line width for formatting (default: 80) --write - Modify files in place --check - Check if files are formatted (exit 1 if not) --reformat - Full reformatting mode (instead of conservative) --indent <INDENT> - Indent size in spaces (default: 2) The formatter has two modes: Conservative mode (default) - Preserves original formatting choices where possible, only reformatting where necessary Reformat mode ( --reformat ) - Full reformatting that applies consistent style throughout","title":"Options"},{"location":"command-line/#backward-compatibility","text":"All existing command patterns continue to work unchanged: eu file.eu # Still works (uses run subcommand) eu -e \"expression\" # Still works (uses run subcommand) eu -j file.eu # Still works (JSON output) eu -S -Q file.eu # Still works (statistics, no prelude)","title":"Backward Compatibility"},{"location":"deep-find-performance-baseline/","text":"Deep Find/Query Performance Baseline Date : 2026-02-06 Bead : eu-d43z Platform : macOS Darwin 24.6.0, release build Summary The prelude-only implementation of deep-find and deep-query is usable for small-to-moderate data sizes (up to ~100 top-level blocks) with acceptable latency. For large data (1000+ top-level blocks), VM execution time becomes significant, dominated by GC pressure from intermediate allocations. Recommendation : Rust intrinsics are NOT urgently needed. The current prelude implementation is sufficient for typical eucalypt use cases. If profiling of real workloads shows deep-find/query as a bottleneck, Rust intrinsics can be pursued as an optimisation. Methodology Two JSON fixtures were generated: List fixture : 2000 records in a JSON array (~1.15 MB). Tests deep-find which traverses lists. Flat fixture : 2500 service blocks as top-level keys (~0.95 MB). Tests both deep-find and deep-query on pure block structures. All measurements use eu -S (statistics flag) on a release build. Results Compile pipeline overhead The compile pipeline (parse, translate, cook, eliminate, etc.) is constant regardless of the query operation. For ~1MB fixtures: Phase Time Parse ~120ms Cook ~190ms Eliminate ~110ms STG compile ~90ms Total pipeline ~500-700ms This dominates wall-clock time for small queries on large data. VM execution time Operation Data VM Time Ticks Allocs Simple lookup ( data.total ) 1.15MB list 1ms 215 25 deep-find-first(\"host\", ...) 1.15MB list 240ms 8K 5K deep-find-first(\"host\", ...) 0.95MB flat 2.0s 19M 53K deep-query-first(\"host\", ...) 0.95MB flat 1.3s 19M 53K deep-query-first(\"config.host\", ...) 0.95MB flat 91.5s 393M 3.9M deep-query-first(\"config.host\", ...) 18KB (100 blocks) 4ms - - deep-query-first(\"config.host\", ...) 9KB (50 blocks) 3ms - - Inline benchmark (20 services) The bench/010_deep_find_perf.eu harness test exercises deep-find, deep-query with bare keys, dotted paths, and wildcards on 20 inline service blocks: Ticks : 102,550 Allocs : 12,738 Max stack : 83 Scaling characteristics deep-find scales roughly linearly with total block count deep-query with ** expands to all descendants before matching \u2014 this is O(n * d) where n is block count and d is average depth The descendants expansion creates many intermediate list allocations, triggering GC pressure GC mark time dominates for large data sets (232ms of 240ms for deep-find-first on list fixture) Key findings GC dominates : For deep-find on large data, GC mark time is 95% of VM time. The actual mutator work is minimal. Descendants expansion is expensive : The deep-query ** handler builds a complete list of all descendant blocks before matching the next segment. For 2500 services with 3-4 nesting levels, this creates ~15,000 intermediate block references. Practical size limit : ~100 top-level blocks for complex patterns ( config.host ), ~500 for simple patterns ( host ). Beyond this, consider using targeted lookups instead. List traversal gap : deep-query does not traverse JSON arrays (lists). Only deep-find correctly recurses into lists. This is a known design gap \u2014 deep-query 's descendants function only expands block children. Recommendations No immediate action needed The prelude implementation is sufficient for eucalypt's typical use case: processing configuration files and moderate-sized API responses. These are rarely >100KB. If optimisation is pursued (Phase 3) Rust intrinsic for deep-find : Would eliminate GC pressure by collecting results in Rust Vec before returning to the VM. Lazy descendants in deep-query : Rather than expanding all descendants eagerly, use a lazy recursive traversal that can short-circuit on first match. Fix list traversal in deep-query : The descendants function should recurse into lists, matching deep-find 's behaviour.","title":"Deep Find/Query Performance Baseline"},{"location":"deep-find-performance-baseline/#deep-findquery-performance-baseline","text":"Date : 2026-02-06 Bead : eu-d43z Platform : macOS Darwin 24.6.0, release build","title":"Deep Find/Query Performance Baseline"},{"location":"deep-find-performance-baseline/#summary","text":"The prelude-only implementation of deep-find and deep-query is usable for small-to-moderate data sizes (up to ~100 top-level blocks) with acceptable latency. For large data (1000+ top-level blocks), VM execution time becomes significant, dominated by GC pressure from intermediate allocations. Recommendation : Rust intrinsics are NOT urgently needed. The current prelude implementation is sufficient for typical eucalypt use cases. If profiling of real workloads shows deep-find/query as a bottleneck, Rust intrinsics can be pursued as an optimisation.","title":"Summary"},{"location":"deep-find-performance-baseline/#methodology","text":"Two JSON fixtures were generated: List fixture : 2000 records in a JSON array (~1.15 MB). Tests deep-find which traverses lists. Flat fixture : 2500 service blocks as top-level keys (~0.95 MB). Tests both deep-find and deep-query on pure block structures. All measurements use eu -S (statistics flag) on a release build.","title":"Methodology"},{"location":"deep-find-performance-baseline/#results","text":"","title":"Results"},{"location":"deep-find-performance-baseline/#compile-pipeline-overhead","text":"The compile pipeline (parse, translate, cook, eliminate, etc.) is constant regardless of the query operation. For ~1MB fixtures: Phase Time Parse ~120ms Cook ~190ms Eliminate ~110ms STG compile ~90ms Total pipeline ~500-700ms This dominates wall-clock time for small queries on large data.","title":"Compile pipeline overhead"},{"location":"deep-find-performance-baseline/#vm-execution-time","text":"Operation Data VM Time Ticks Allocs Simple lookup ( data.total ) 1.15MB list 1ms 215 25 deep-find-first(\"host\", ...) 1.15MB list 240ms 8K 5K deep-find-first(\"host\", ...) 0.95MB flat 2.0s 19M 53K deep-query-first(\"host\", ...) 0.95MB flat 1.3s 19M 53K deep-query-first(\"config.host\", ...) 0.95MB flat 91.5s 393M 3.9M deep-query-first(\"config.host\", ...) 18KB (100 blocks) 4ms - - deep-query-first(\"config.host\", ...) 9KB (50 blocks) 3ms - -","title":"VM execution time"},{"location":"deep-find-performance-baseline/#inline-benchmark-20-services","text":"The bench/010_deep_find_perf.eu harness test exercises deep-find, deep-query with bare keys, dotted paths, and wildcards on 20 inline service blocks: Ticks : 102,550 Allocs : 12,738 Max stack : 83","title":"Inline benchmark (20 services)"},{"location":"deep-find-performance-baseline/#scaling-characteristics","text":"deep-find scales roughly linearly with total block count deep-query with ** expands to all descendants before matching \u2014 this is O(n * d) where n is block count and d is average depth The descendants expansion creates many intermediate list allocations, triggering GC pressure GC mark time dominates for large data sets (232ms of 240ms for deep-find-first on list fixture)","title":"Scaling characteristics"},{"location":"deep-find-performance-baseline/#key-findings","text":"GC dominates : For deep-find on large data, GC mark time is 95% of VM time. The actual mutator work is minimal. Descendants expansion is expensive : The deep-query ** handler builds a complete list of all descendant blocks before matching the next segment. For 2500 services with 3-4 nesting levels, this creates ~15,000 intermediate block references. Practical size limit : ~100 top-level blocks for complex patterns ( config.host ), ~500 for simple patterns ( host ). Beyond this, consider using targeted lookups instead. List traversal gap : deep-query does not traverse JSON arrays (lists). Only deep-find correctly recurses into lists. This is a known design gap \u2014 deep-query 's descendants function only expands block children.","title":"Key findings"},{"location":"deep-find-performance-baseline/#recommendations","text":"","title":"Recommendations"},{"location":"deep-find-performance-baseline/#no-immediate-action-needed","text":"The prelude implementation is sufficient for eucalypt's typical use case: processing configuration files and moderate-sized API responses. These are rarely >100KB.","title":"No immediate action needed"},{"location":"deep-find-performance-baseline/#if-optimisation-is-pursued-phase-3","text":"Rust intrinsic for deep-find : Would eliminate GC pressure by collecting results in Rust Vec before returning to the VM. Lazy descendants in deep-query : Rather than expanding all descendants eagerly, use a lazy recursive traversal that can short-circuit on first match. Fix list traversal in deep-query : The descendants function should recurse into lists, matching deep-find 's behaviour.","title":"If optimisation is pursued (Phase 3)"},{"location":"gc-benchmarking/","text":"GC Benchmarking Workflow This document describes the procedure for measuring GC performance when making changes to the garbage collector or memory management code. Prerequisites Build the release binary (benchmarks should always run against optimised builds): cargo build --release Tools gc-bench.sh \u2014 End-to-end benchmarking The scripts/gc-bench.sh script runs three GC stress benchmark programs multiple times, collects statistics via --statistics-file , computes medians, and compares against a saved baseline. Benchmark programs (in harness/test/bench/ ): File Target What it tests 007_short_lived.eu bench-short-lived High allocation churn \u2014 many temporary lists created and immediately discarded 008_long_lived_graph.eu bench-long-lived-graph Long-lived persistent structure surviving alongside temporary garbage 009_fragmentation.eu bench-fragmentation Interleaved retained/discarded allocations producing heap fragmentation Metrics collected per benchmark: machine_ticks \u2014 VM instruction count (deterministic) machine_allocs \u2014 heap allocation count (deterministic) collections_count \u2014 number of GC collections triggered total_mark_time_secs \u2014 wall-clock time in mark phase total_sweep_time_secs \u2014 wall-clock time in sweep phase peak_heap_blocks \u2014 maximum heap blocks in use at any point Criterion benchmarks cargo bench runs Criterion microbenchmarks defined in benches/gc.rs . These measure low-level GC operations: Allocate then collect \u2014 allocation + full collection cycle Collect with survivors \u2014 collection with varying survival rates (0% to 100%) Allocate into recycled \u2014 allocation into previously-collected blocks Step-by-step procedure 1. Establish baseline Before making any GC changes, on a clean working tree: scripts/gc-bench.sh baseline --runs 5 This saves median statistics to gc-bench-baseline.json (git-ignored). 2. Make your change Implement the GC modification, then rebuild: cargo build --release 3. Run tests Verify correctness is not broken: cargo test --lib cargo test --test harness_test 4. Compare against baseline scripts/gc-bench.sh compare --runs 5 This runs the same benchmarks and reports percentage change for each metric. Regressions above the threshold (default 5%) are flagged. Example output: --- bench-short-lived --- machine_ticks baseline=167037166 current=167037166 change=0.0% machine_allocs baseline=11178642 current=11178642 change=0.0% total_mark_time_secs baseline=0.308 current=0.295 change=-4.2% total_sweep_time_secs baseline=0.073 current=0.068 change=-6.8% 5. Investigate with Criterion if needed If gc-bench.sh shows unexpected results, use Criterion for detailed analysis: # Run all GC benchmarks cargo bench -- gc # Run a specific benchmark group cargo bench -- gc_alloc_then_collect # Compare against saved Criterion baseline cargo bench -- gc --save-baseline before # ... make change, rebuild ... cargo bench -- gc --baseline before 6. Include results in commit message When committing GC changes, include a summary of benchmark results in the commit message body. For example: feat(eu-xxx): implement lazy sweeping gc-bench.sh results (5 runs, median): bench-short-lived: mark -12%, sweep -45%, ticks 0% bench-long-lived: mark -8%, sweep -38%, ticks 0% bench-fragmentation: mark -5%, sweep -42%, ticks 0% Options reference gc-bench.sh Usage: scripts/gc-bench.sh {baseline|compare} [OPTIONS] Options: --runs N Number of runs per benchmark (default: 5) --heap-limit MIB Heap limit in MiB (default: no limit) --threshold PCT Regression threshold percentage (default: 5) Environment: EU_BIN Path to eu binary (default: target/release/eu) Interpreting results Deterministic metrics (ticks, allocs) should show 0% change unless the GC change affects the allocation path or triggers collections at different points. Timing metrics (mark, sweep) naturally vary by a few percent between runs. Use --runs 5 or higher for reliable medians. collections_count changes indicate the GC is triggering at different points, which may be expected for threshold or heuristic changes. peak_heap_blocks changes indicate different peak memory usage. Notes Always benchmark release builds \u2014 debug builds are 10-50x slower. The --heap-limit-mib flag forces a smaller heap, which can trigger more frequent collections. However, there is a known GC assertion issue under very small heap limits, so use this flag with caution. The baseline file ( gc-bench-baseline.json ) is git-ignored and local to each worktree.","title":"GC Benchmarking"},{"location":"gc-benchmarking/#gc-benchmarking-workflow","text":"This document describes the procedure for measuring GC performance when making changes to the garbage collector or memory management code.","title":"GC Benchmarking Workflow"},{"location":"gc-benchmarking/#prerequisites","text":"Build the release binary (benchmarks should always run against optimised builds): cargo build --release","title":"Prerequisites"},{"location":"gc-benchmarking/#tools","text":"","title":"Tools"},{"location":"gc-benchmarking/#gc-benchsh-end-to-end-benchmarking","text":"The scripts/gc-bench.sh script runs three GC stress benchmark programs multiple times, collects statistics via --statistics-file , computes medians, and compares against a saved baseline. Benchmark programs (in harness/test/bench/ ): File Target What it tests 007_short_lived.eu bench-short-lived High allocation churn \u2014 many temporary lists created and immediately discarded 008_long_lived_graph.eu bench-long-lived-graph Long-lived persistent structure surviving alongside temporary garbage 009_fragmentation.eu bench-fragmentation Interleaved retained/discarded allocations producing heap fragmentation Metrics collected per benchmark: machine_ticks \u2014 VM instruction count (deterministic) machine_allocs \u2014 heap allocation count (deterministic) collections_count \u2014 number of GC collections triggered total_mark_time_secs \u2014 wall-clock time in mark phase total_sweep_time_secs \u2014 wall-clock time in sweep phase peak_heap_blocks \u2014 maximum heap blocks in use at any point","title":"gc-bench.sh \u2014 End-to-end benchmarking"},{"location":"gc-benchmarking/#criterion-benchmarks","text":"cargo bench runs Criterion microbenchmarks defined in benches/gc.rs . These measure low-level GC operations: Allocate then collect \u2014 allocation + full collection cycle Collect with survivors \u2014 collection with varying survival rates (0% to 100%) Allocate into recycled \u2014 allocation into previously-collected blocks","title":"Criterion benchmarks"},{"location":"gc-benchmarking/#step-by-step-procedure","text":"","title":"Step-by-step procedure"},{"location":"gc-benchmarking/#1-establish-baseline","text":"Before making any GC changes, on a clean working tree: scripts/gc-bench.sh baseline --runs 5 This saves median statistics to gc-bench-baseline.json (git-ignored).","title":"1. Establish baseline"},{"location":"gc-benchmarking/#2-make-your-change","text":"Implement the GC modification, then rebuild: cargo build --release","title":"2. Make your change"},{"location":"gc-benchmarking/#3-run-tests","text":"Verify correctness is not broken: cargo test --lib cargo test --test harness_test","title":"3. Run tests"},{"location":"gc-benchmarking/#4-compare-against-baseline","text":"scripts/gc-bench.sh compare --runs 5 This runs the same benchmarks and reports percentage change for each metric. Regressions above the threshold (default 5%) are flagged. Example output: --- bench-short-lived --- machine_ticks baseline=167037166 current=167037166 change=0.0% machine_allocs baseline=11178642 current=11178642 change=0.0% total_mark_time_secs baseline=0.308 current=0.295 change=-4.2% total_sweep_time_secs baseline=0.073 current=0.068 change=-6.8%","title":"4. Compare against baseline"},{"location":"gc-benchmarking/#5-investigate-with-criterion-if-needed","text":"If gc-bench.sh shows unexpected results, use Criterion for detailed analysis: # Run all GC benchmarks cargo bench -- gc # Run a specific benchmark group cargo bench -- gc_alloc_then_collect # Compare against saved Criterion baseline cargo bench -- gc --save-baseline before # ... make change, rebuild ... cargo bench -- gc --baseline before","title":"5. Investigate with Criterion if needed"},{"location":"gc-benchmarking/#6-include-results-in-commit-message","text":"When committing GC changes, include a summary of benchmark results in the commit message body. For example: feat(eu-xxx): implement lazy sweeping gc-bench.sh results (5 runs, median): bench-short-lived: mark -12%, sweep -45%, ticks 0% bench-long-lived: mark -8%, sweep -38%, ticks 0% bench-fragmentation: mark -5%, sweep -42%, ticks 0%","title":"6. Include results in commit message"},{"location":"gc-benchmarking/#options-reference","text":"","title":"Options reference"},{"location":"gc-benchmarking/#gc-benchsh","text":"Usage: scripts/gc-bench.sh {baseline|compare} [OPTIONS] Options: --runs N Number of runs per benchmark (default: 5) --heap-limit MIB Heap limit in MiB (default: no limit) --threshold PCT Regression threshold percentage (default: 5) Environment: EU_BIN Path to eu binary (default: target/release/eu)","title":"gc-bench.sh"},{"location":"gc-benchmarking/#interpreting-results","text":"Deterministic metrics (ticks, allocs) should show 0% change unless the GC change affects the allocation path or triggers collections at different points. Timing metrics (mark, sweep) naturally vary by a few percent between runs. Use --runs 5 or higher for reliable medians. collections_count changes indicate the GC is triggering at different points, which may be expected for threshold or heuristic changes. peak_heap_blocks changes indicate different peak memory usage.","title":"Interpreting results"},{"location":"gc-benchmarking/#notes","text":"Always benchmark release builds \u2014 debug builds are 10-50x slower. The --heap-limit-mib flag forces a smaller heap, which can trigger more frequent collections. However, there is a known GC assertion issue under very small heap limits, so use this flag with caution. The baseline file ( gc-bench-baseline.json ) is git-ignored and local to each worktree.","title":"Notes"},{"location":"gc-implementation/","text":"Garbage Collection Implementation This document describes the current state of Eucalypt's Immix-style garbage collector implementation. Overview Eucalypt uses a generational, mark-and-sweep garbage collector based on the Immix algorithm. The implementation provides automatic memory management for the STG (Spineless Tagless G-machine) runtime with support for multiple object size classes and block recycling. Architecture Memory Layout The GC uses a block-based allocation strategy with the following hierarchy: Blocks : 32KB (2^15 bytes) memory regions Lines : 128-byte (2^7 bytes) allocation units within blocks Objects : Variable-sized objects with 16-byte headers Block (32KB) \u251c\u2500\u2500 Line 0 (128B) \u2510 \u251c\u2500\u2500 Line 1 (128B) \u2502 256 lines per block \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 Line 255 \u2518 Size Classes Objects are categorized into three size classes: Small (< 128 bytes): Fit within a single line Medium (128B - 32KB): Span multiple lines within a block Large (> 32KB): Allocated in dedicated Large Object Blocks (LOBs) Implementation : src/eval/memory/heap.rs:29-51 Object Headers Every allocated object has a 16-byte header containing: Mark bits (2 bits): Current mark state and forwarding flag Size field (32 bits): For untyped byte allocations Forwarding pointer (64 bits): For potential moving collection Implementation : src/eval/memory/header.rs:61-69 Allocation Strategy Bump Allocation Each block uses downward bump allocation with: - Cursor : Current allocation position - Lower limit : Boundary of current usable region - Line map : Bitmap tracking which lines contain live objects Implementation : src/eval/memory/bump.rs:181-191 Block Management The heap maintains several block categories: Head block : Active small object allocation Overflow block : Active medium object allocation Rest blocks : Previously used, pending collection Recycled blocks : Reclaimed with usable holes LOBs : Large object storage Implementation : src/eval/memory/heap.rs:54-65 Allocation Paths All allocations flow through the GC system via: Heap::alloc<T>() - Typed objects ( src/eval/memory/heap.rs:240 ) Heap::alloc_bytes() - Arrays/vectors ( src/eval/memory/heap.rs:261 ) Both paths: - Calculate total size (header + object) - Find appropriate space based on size class - Write header and object data - Return typed pointer Collection Algorithm Mark Phase Uses tricolor marking with breadth-first traversal: Reset all line maps in blocks Root scanning from machine state (stack, globals, locals) Transitive closure following object references Line marking for objects and backing storage Implementation : src/eval/memory/collect.rs:123-144 Sweep Phase Conservative Immix sweep with hole identification: Scan line maps in each block Identify holes requiring 2+ consecutive free lines Apply conservative marking (exclude boundary lines) Move recyclable blocks to recycled list Implementation : src/eval/memory/bump.rs:295-306 Mark State Management Uses global mark bit flipping to avoid clearing marks: - MARK_STATE : Global atomic boolean indicating current \"marked\" value - flip_mark_state() : Called after each collection - Header marking : Compares object mark bit with current MARK_STATE Implementation : src/eval/memory/mark.rs Collection Triggering Policy-Based Collection Collection occurs only when: 1. User sets --heap-limit-mib command line option 2. Allocated blocks \u2265 limit AND recycled blocks < 25% of total 3. Check performed every 500 execution steps Implementation : src/eval/machine/vm.rs:810-818 No Emergency Collection Critical limitation : Allocation failures cause panics rather than triggering emergency collection: .expect(\"aargh\") // heap.rs:313,317 Collection Frequency Check interval : Every 500 VM execution steps Default behavior : No automatic collection (no heap limit set) Final collection : Always performed at program termination Memory Management Integration STG Machine Integration The GC integrates with the STG machine through: Root scanning : Machine state implements GcScannable Allocation scoping : MutatorHeapView provides safe allocation Collection timing : Integrated with VM execution loop Object Tracing Objects implement GcScannable trait: - scan() method returns referenced objects - Collector scope ensures lifetime safety - Polymorphic scanning handles different object types Implementation : src/eval/memory/collect.rs:47-53 Performance Characteristics Allocation Performance Bump allocation : O(1) for small/medium objects Block recycling : Efficient hole-finding algorithm Size-based routing : Optimal strategy per size class Collection Performance Mark phase : Proportional to live object count Sweep phase : Proportional to total block count Pause times : Single-threaded stop-the-world collection Memory Utilization Conservative marking : May retain some garbage near live objects Block recycling : Reduces allocation overhead Fragmentation : Managed through hole coalescing Testing and Validation Unit Tests Current test coverage includes: Basic allocation : Simple object allocation and retrieval Multi-block allocation : Stress testing block management Collection cycles : Mark-sweep correctness validation Large objects : LOB allocation and collection Implementation : src/eval/memory/collect.rs:164-259 Benchmark Tests Available benchmark programs: 004_generations.eu : Multi-generational allocation patterns Other benchmarks in harness/test/bench/ Missing Test Coverage Critical gaps identified : Long-running tests : No sustained allocation testing Emergency scenarios : No OOM recovery testing Performance regression : No GC timing benchmarks Leak detection : No long-term memory usage validation Limitations Threading Model Single-threaded : No concurrent or parallel collection Stop-the-world : Full program pause during collection Configuration Fixed parameters : Collection thresholds are not user-configurable Manual heap limits : Users can set heap limits via command line options Algorithm Differences from Full Immix No evacuation : Objects are not moved during collection Mark-in-place only : No adaptive choice between marking and evacuation Conservative marking : Line-based marking without precise object boundaries Current Implementation The implementation includes the following components: Memory Management Block-based allocation with line maps for tracking object locations Three-tier size classification (small/medium/large objects) Mark-and-sweep collection algorithm without object evacuation Block recycling with hole detection and reuse Large object allocation with size-optimised boundaries Integration STG machine integration for root scanning Object header management with mark state tracking Array and vector allocation support Emergency collection on memory exhaustion Error Handling Graceful handling of allocation failures Emergency collection attempts before reporting OOM Detailed error context including heap state information Code Organization The GC implementation spans several modules: src/eval/memory/ \u251c\u2500\u2500 mod.rs # Module declarations \u251c\u2500\u2500 heap.rs # Main heap and allocation logic \u251c\u2500\u2500 collect.rs # Mark-and-sweep collector \u251c\u2500\u2500 bump.rs # Block allocation and line maps \u251c\u2500\u2500 header.rs # Object header management \u251c\u2500\u2500 mark.rs # Mark state management \u251c\u2500\u2500 mutator.rs # Mutator heap access \u251c\u2500\u2500 alloc.rs # Allocation traits \u251c\u2500\u2500 array.rs # Array/vector support \u251c\u2500\u2500 block.rs # Low-level block management \u251c\u2500\u2500 lob.rs # Large object blocks \u2514\u2500\u2500 ... Usage and Behavior Allocation Patterns Small objects (< 128 bytes) are allocated within single lines Medium objects (128 bytes - 32KB) span multiple lines within blocks Large objects (> 32KB) receive dedicated allocation blocks Collection Triggering Collection occurs when heap limit is exceeded (if set) Emergency collection attempts when allocation fails Explicit collection can be triggered programmatically Memory Layout Optimisation Objects are aligned to 16-byte boundaries for cache efficiency Large objects use tiered size boundaries (16KB, 64KB, 256KB) to reduce waste Block recycling prioritises blocks with larger available holes Command Line Interface GC-related options: --heap-limit-mib <SIZE> : Set heap limit in MiB (enables automatic GC) --heap-dump-at-gc : Dump heap state during collection (debugging) -S, --statistics : Print execution metrics including GC stats Analysis Against Immix Standards Research Summary Based on analysis of the original Immix paper (Blackburn & McKinley, 2008) and comparison with open-source implementations, several important disparities have been identified in Eucalypt's implementation. Specification Compliance \u2705 Correct Implementations: - Block size : 32KB (matches original paper specification) - Line size : 128 bytes (consistent across all Immix implementations) - Conservative line marking : Properly requires 2+ consecutive free lines for holes - Downward bump allocation : Correct allocation direction within blocks - Line map organization : Proper bitmap implementation for line marking - Memory layout : Correct block/line hierarchy \u26a0\ufe0f Significant Disparities: 1. Missing Opportunistic Defragmentation (Critical) Standard Immix : Core innovation is opportunistic evacuation of fragmented blocks Eucalypt : No evacuation or moving collection implemented Impact : Loses primary performance advantage of Immix over mark-sweep Evidence : Headers have forwarding fields but they're never used 2. Eager vs Lazy Sweeping (Performance Impact) Standard Immix : Lazy sweeping - blocks swept just before allocation Eucalypt : Eager sweeping - all blocks swept immediately after marking Impact : Higher collection pause times, less responsive allocation 3. Conservative Marking Interpretation (Semantic Difference) Standard Immix : Conservative refers to stack/root scanning without precise type info Eucalypt : Conservative refers to line boundary exclusion during hole detection Impact : Different meaning of \"conservative\" - both valid but semantically different 4. Fragmentation Detection (Missing Core Feature) Standard Immix : Decides whether to evacuate based on fragmentation analysis Eucalypt : No fragmentation detection or evacuation decisions Impact : Cannot adapt collection strategy to heap state Algorithm Classification Current Status : Eucalypt implements a \"Mark-Sweep with Line Maps\" collector rather than true Immix. Missing Core Immix Features: - Opportunistic defragmentation - Block evacuation - Fragmentation-based collection decisions - Moving/copying collection phases - Lazy sweeping optimisation Performance Implications Retained Benefits: - Efficient allocation through bump allocation - Good cache locality from line-based organization - Effective hole identification and reuse Lost Benefits: - Defragmentation to combat fragmentation - Adaptive collection based on heap state - Lower pause times from lazy sweeping - Space efficiency improvements from compaction Technical Quality Assessment Code Quality : \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent - Clean, well-structured implementation - Proper safety invariants - Good abstraction boundaries - Comprehensive unit tests Algorithm Completeness : \u2b50\u2b50\u2b50 Partial - Implements ~60% of full Immix algorithm - Missing core performance innovations - Solid foundation for full implementation Stability : Functional with error handling - Graceful handling of memory exhaustion scenarios - Emergency collection fallback mechanisms - Comprehensive test coverage for allocation and collection scenarios Implementation Recommendations Immediate Improvements (Current Algorithm) Lazy sweeping : Sweep blocks just before allocation Better collection policy : More sophisticated triggering Error handling : Graceful OOM recovery Full Immix Implementation (Major Enhancement) Fragmentation detection : Track fragmented vs dense blocks Evacuation phase : Implement object moving during collection Adaptive collection : Choose mark-in-place vs evacuation per cycle Conservative root scanning : Stack scanning without precise types Conclusion Eucalypt's GC implementation is a mark-sweep collector that uses Immix-inspired memory organisation. While it doesn't implement the full Immix algorithm (particularly the evacuation phase), it provides functional memory management for the STG runtime. Current Status: - Implements block-based allocation with line-level tracking - Provides mark-and-sweep collection without object movement - Includes optimisations for large object allocation and block recycling - Handles memory exhaustion through emergency collection mechanisms Relationship to Immix Algorithm: The implementation captures Immix's memory layout benefits (cache-friendly block organisation, efficient hole detection) but omits the core evacuation phase that distinguishes Immix from traditional mark-sweep collectors. This results in a hybrid approach that combines Immix memory organisation with mark-in-place collection. Practical Considerations: For eucalypt's typical workloads (configuration processing, template rendering, data transformation), the current implementation provides adequate memory management. The lack of evacuation limits performance gains for long-running or heavily fragmented applications, but this matches eucalypt's primary use cases.","title":"GC Implementation"},{"location":"gc-implementation/#garbage-collection-implementation","text":"This document describes the current state of Eucalypt's Immix-style garbage collector implementation.","title":"Garbage Collection Implementation"},{"location":"gc-implementation/#overview","text":"Eucalypt uses a generational, mark-and-sweep garbage collector based on the Immix algorithm. The implementation provides automatic memory management for the STG (Spineless Tagless G-machine) runtime with support for multiple object size classes and block recycling.","title":"Overview"},{"location":"gc-implementation/#architecture","text":"","title":"Architecture"},{"location":"gc-implementation/#memory-layout","text":"The GC uses a block-based allocation strategy with the following hierarchy: Blocks : 32KB (2^15 bytes) memory regions Lines : 128-byte (2^7 bytes) allocation units within blocks Objects : Variable-sized objects with 16-byte headers Block (32KB) \u251c\u2500\u2500 Line 0 (128B) \u2510 \u251c\u2500\u2500 Line 1 (128B) \u2502 256 lines per block \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 Line 255 \u2518","title":"Memory Layout"},{"location":"gc-implementation/#size-classes","text":"Objects are categorized into three size classes: Small (< 128 bytes): Fit within a single line Medium (128B - 32KB): Span multiple lines within a block Large (> 32KB): Allocated in dedicated Large Object Blocks (LOBs) Implementation : src/eval/memory/heap.rs:29-51","title":"Size Classes"},{"location":"gc-implementation/#object-headers","text":"Every allocated object has a 16-byte header containing: Mark bits (2 bits): Current mark state and forwarding flag Size field (32 bits): For untyped byte allocations Forwarding pointer (64 bits): For potential moving collection Implementation : src/eval/memory/header.rs:61-69","title":"Object Headers"},{"location":"gc-implementation/#allocation-strategy","text":"","title":"Allocation Strategy"},{"location":"gc-implementation/#bump-allocation","text":"Each block uses downward bump allocation with: - Cursor : Current allocation position - Lower limit : Boundary of current usable region - Line map : Bitmap tracking which lines contain live objects Implementation : src/eval/memory/bump.rs:181-191","title":"Bump Allocation"},{"location":"gc-implementation/#block-management","text":"The heap maintains several block categories: Head block : Active small object allocation Overflow block : Active medium object allocation Rest blocks : Previously used, pending collection Recycled blocks : Reclaimed with usable holes LOBs : Large object storage Implementation : src/eval/memory/heap.rs:54-65","title":"Block Management"},{"location":"gc-implementation/#allocation-paths","text":"All allocations flow through the GC system via: Heap::alloc<T>() - Typed objects ( src/eval/memory/heap.rs:240 ) Heap::alloc_bytes() - Arrays/vectors ( src/eval/memory/heap.rs:261 ) Both paths: - Calculate total size (header + object) - Find appropriate space based on size class - Write header and object data - Return typed pointer","title":"Allocation Paths"},{"location":"gc-implementation/#collection-algorithm","text":"","title":"Collection Algorithm"},{"location":"gc-implementation/#mark-phase","text":"Uses tricolor marking with breadth-first traversal: Reset all line maps in blocks Root scanning from machine state (stack, globals, locals) Transitive closure following object references Line marking for objects and backing storage Implementation : src/eval/memory/collect.rs:123-144","title":"Mark Phase"},{"location":"gc-implementation/#sweep-phase","text":"Conservative Immix sweep with hole identification: Scan line maps in each block Identify holes requiring 2+ consecutive free lines Apply conservative marking (exclude boundary lines) Move recyclable blocks to recycled list Implementation : src/eval/memory/bump.rs:295-306","title":"Sweep Phase"},{"location":"gc-implementation/#mark-state-management","text":"Uses global mark bit flipping to avoid clearing marks: - MARK_STATE : Global atomic boolean indicating current \"marked\" value - flip_mark_state() : Called after each collection - Header marking : Compares object mark bit with current MARK_STATE Implementation : src/eval/memory/mark.rs","title":"Mark State Management"},{"location":"gc-implementation/#collection-triggering","text":"","title":"Collection Triggering"},{"location":"gc-implementation/#policy-based-collection","text":"Collection occurs only when: 1. User sets --heap-limit-mib command line option 2. Allocated blocks \u2265 limit AND recycled blocks < 25% of total 3. Check performed every 500 execution steps Implementation : src/eval/machine/vm.rs:810-818","title":"Policy-Based Collection"},{"location":"gc-implementation/#no-emergency-collection","text":"Critical limitation : Allocation failures cause panics rather than triggering emergency collection: .expect(\"aargh\") // heap.rs:313,317","title":"No Emergency Collection"},{"location":"gc-implementation/#collection-frequency","text":"Check interval : Every 500 VM execution steps Default behavior : No automatic collection (no heap limit set) Final collection : Always performed at program termination","title":"Collection Frequency"},{"location":"gc-implementation/#memory-management-integration","text":"","title":"Memory Management Integration"},{"location":"gc-implementation/#stg-machine-integration","text":"The GC integrates with the STG machine through: Root scanning : Machine state implements GcScannable Allocation scoping : MutatorHeapView provides safe allocation Collection timing : Integrated with VM execution loop","title":"STG Machine Integration"},{"location":"gc-implementation/#object-tracing","text":"Objects implement GcScannable trait: - scan() method returns referenced objects - Collector scope ensures lifetime safety - Polymorphic scanning handles different object types Implementation : src/eval/memory/collect.rs:47-53","title":"Object Tracing"},{"location":"gc-implementation/#performance-characteristics","text":"","title":"Performance Characteristics"},{"location":"gc-implementation/#allocation-performance","text":"Bump allocation : O(1) for small/medium objects Block recycling : Efficient hole-finding algorithm Size-based routing : Optimal strategy per size class","title":"Allocation Performance"},{"location":"gc-implementation/#collection-performance","text":"Mark phase : Proportional to live object count Sweep phase : Proportional to total block count Pause times : Single-threaded stop-the-world collection","title":"Collection Performance"},{"location":"gc-implementation/#memory-utilization","text":"Conservative marking : May retain some garbage near live objects Block recycling : Reduces allocation overhead Fragmentation : Managed through hole coalescing","title":"Memory Utilization"},{"location":"gc-implementation/#testing-and-validation","text":"","title":"Testing and Validation"},{"location":"gc-implementation/#unit-tests","text":"Current test coverage includes: Basic allocation : Simple object allocation and retrieval Multi-block allocation : Stress testing block management Collection cycles : Mark-sweep correctness validation Large objects : LOB allocation and collection Implementation : src/eval/memory/collect.rs:164-259","title":"Unit Tests"},{"location":"gc-implementation/#benchmark-tests","text":"Available benchmark programs: 004_generations.eu : Multi-generational allocation patterns Other benchmarks in harness/test/bench/","title":"Benchmark Tests"},{"location":"gc-implementation/#missing-test-coverage","text":"Critical gaps identified : Long-running tests : No sustained allocation testing Emergency scenarios : No OOM recovery testing Performance regression : No GC timing benchmarks Leak detection : No long-term memory usage validation","title":"Missing Test Coverage"},{"location":"gc-implementation/#limitations","text":"","title":"Limitations"},{"location":"gc-implementation/#threading-model","text":"Single-threaded : No concurrent or parallel collection Stop-the-world : Full program pause during collection","title":"Threading Model"},{"location":"gc-implementation/#configuration","text":"Fixed parameters : Collection thresholds are not user-configurable Manual heap limits : Users can set heap limits via command line options","title":"Configuration"},{"location":"gc-implementation/#algorithm-differences-from-full-immix","text":"No evacuation : Objects are not moved during collection Mark-in-place only : No adaptive choice between marking and evacuation Conservative marking : Line-based marking without precise object boundaries","title":"Algorithm Differences from Full Immix"},{"location":"gc-implementation/#current-implementation","text":"The implementation includes the following components:","title":"Current Implementation"},{"location":"gc-implementation/#memory-management","text":"Block-based allocation with line maps for tracking object locations Three-tier size classification (small/medium/large objects) Mark-and-sweep collection algorithm without object evacuation Block recycling with hole detection and reuse Large object allocation with size-optimised boundaries","title":"Memory Management"},{"location":"gc-implementation/#integration","text":"STG machine integration for root scanning Object header management with mark state tracking Array and vector allocation support Emergency collection on memory exhaustion","title":"Integration"},{"location":"gc-implementation/#error-handling","text":"Graceful handling of allocation failures Emergency collection attempts before reporting OOM Detailed error context including heap state information","title":"Error Handling"},{"location":"gc-implementation/#code-organization","text":"The GC implementation spans several modules: src/eval/memory/ \u251c\u2500\u2500 mod.rs # Module declarations \u251c\u2500\u2500 heap.rs # Main heap and allocation logic \u251c\u2500\u2500 collect.rs # Mark-and-sweep collector \u251c\u2500\u2500 bump.rs # Block allocation and line maps \u251c\u2500\u2500 header.rs # Object header management \u251c\u2500\u2500 mark.rs # Mark state management \u251c\u2500\u2500 mutator.rs # Mutator heap access \u251c\u2500\u2500 alloc.rs # Allocation traits \u251c\u2500\u2500 array.rs # Array/vector support \u251c\u2500\u2500 block.rs # Low-level block management \u251c\u2500\u2500 lob.rs # Large object blocks \u2514\u2500\u2500 ...","title":"Code Organization"},{"location":"gc-implementation/#usage-and-behavior","text":"","title":"Usage and Behavior"},{"location":"gc-implementation/#allocation-patterns","text":"Small objects (< 128 bytes) are allocated within single lines Medium objects (128 bytes - 32KB) span multiple lines within blocks Large objects (> 32KB) receive dedicated allocation blocks","title":"Allocation Patterns"},{"location":"gc-implementation/#collection-triggering_1","text":"Collection occurs when heap limit is exceeded (if set) Emergency collection attempts when allocation fails Explicit collection can be triggered programmatically","title":"Collection Triggering"},{"location":"gc-implementation/#memory-layout-optimisation","text":"Objects are aligned to 16-byte boundaries for cache efficiency Large objects use tiered size boundaries (16KB, 64KB, 256KB) to reduce waste Block recycling prioritises blocks with larger available holes","title":"Memory Layout Optimisation"},{"location":"gc-implementation/#command-line-interface","text":"GC-related options: --heap-limit-mib <SIZE> : Set heap limit in MiB (enables automatic GC) --heap-dump-at-gc : Dump heap state during collection (debugging) -S, --statistics : Print execution metrics including GC stats","title":"Command Line Interface"},{"location":"gc-implementation/#analysis-against-immix-standards","text":"","title":"Analysis Against Immix Standards"},{"location":"gc-implementation/#research-summary","text":"Based on analysis of the original Immix paper (Blackburn & McKinley, 2008) and comparison with open-source implementations, several important disparities have been identified in Eucalypt's implementation.","title":"Research Summary"},{"location":"gc-implementation/#specification-compliance","text":"\u2705 Correct Implementations: - Block size : 32KB (matches original paper specification) - Line size : 128 bytes (consistent across all Immix implementations) - Conservative line marking : Properly requires 2+ consecutive free lines for holes - Downward bump allocation : Correct allocation direction within blocks - Line map organization : Proper bitmap implementation for line marking - Memory layout : Correct block/line hierarchy \u26a0\ufe0f Significant Disparities:","title":"Specification Compliance"},{"location":"gc-implementation/#1-missing-opportunistic-defragmentation-critical","text":"Standard Immix : Core innovation is opportunistic evacuation of fragmented blocks Eucalypt : No evacuation or moving collection implemented Impact : Loses primary performance advantage of Immix over mark-sweep Evidence : Headers have forwarding fields but they're never used","title":"1. Missing Opportunistic Defragmentation (Critical)"},{"location":"gc-implementation/#2-eager-vs-lazy-sweeping-performance-impact","text":"Standard Immix : Lazy sweeping - blocks swept just before allocation Eucalypt : Eager sweeping - all blocks swept immediately after marking Impact : Higher collection pause times, less responsive allocation","title":"2. Eager vs Lazy Sweeping (Performance Impact)"},{"location":"gc-implementation/#3-conservative-marking-interpretation-semantic-difference","text":"Standard Immix : Conservative refers to stack/root scanning without precise type info Eucalypt : Conservative refers to line boundary exclusion during hole detection Impact : Different meaning of \"conservative\" - both valid but semantically different","title":"3. Conservative Marking Interpretation (Semantic Difference)"},{"location":"gc-implementation/#4-fragmentation-detection-missing-core-feature","text":"Standard Immix : Decides whether to evacuate based on fragmentation analysis Eucalypt : No fragmentation detection or evacuation decisions Impact : Cannot adapt collection strategy to heap state","title":"4. Fragmentation Detection (Missing Core Feature)"},{"location":"gc-implementation/#algorithm-classification","text":"Current Status : Eucalypt implements a \"Mark-Sweep with Line Maps\" collector rather than true Immix. Missing Core Immix Features: - Opportunistic defragmentation - Block evacuation - Fragmentation-based collection decisions - Moving/copying collection phases - Lazy sweeping optimisation","title":"Algorithm Classification"},{"location":"gc-implementation/#performance-implications","text":"Retained Benefits: - Efficient allocation through bump allocation - Good cache locality from line-based organization - Effective hole identification and reuse Lost Benefits: - Defragmentation to combat fragmentation - Adaptive collection based on heap state - Lower pause times from lazy sweeping - Space efficiency improvements from compaction","title":"Performance Implications"},{"location":"gc-implementation/#technical-quality-assessment","text":"Code Quality : \u2b50\u2b50\u2b50\u2b50\u2b50 Excellent - Clean, well-structured implementation - Proper safety invariants - Good abstraction boundaries - Comprehensive unit tests Algorithm Completeness : \u2b50\u2b50\u2b50 Partial - Implements ~60% of full Immix algorithm - Missing core performance innovations - Solid foundation for full implementation Stability : Functional with error handling - Graceful handling of memory exhaustion scenarios - Emergency collection fallback mechanisms - Comprehensive test coverage for allocation and collection scenarios","title":"Technical Quality Assessment"},{"location":"gc-implementation/#implementation-recommendations","text":"","title":"Implementation Recommendations"},{"location":"gc-implementation/#immediate-improvements-current-algorithm","text":"Lazy sweeping : Sweep blocks just before allocation Better collection policy : More sophisticated triggering Error handling : Graceful OOM recovery","title":"Immediate Improvements (Current Algorithm)"},{"location":"gc-implementation/#full-immix-implementation-major-enhancement","text":"Fragmentation detection : Track fragmented vs dense blocks Evacuation phase : Implement object moving during collection Adaptive collection : Choose mark-in-place vs evacuation per cycle Conservative root scanning : Stack scanning without precise types","title":"Full Immix Implementation (Major Enhancement)"},{"location":"gc-implementation/#conclusion","text":"Eucalypt's GC implementation is a mark-sweep collector that uses Immix-inspired memory organisation. While it doesn't implement the full Immix algorithm (particularly the evacuation phase), it provides functional memory management for the STG runtime. Current Status: - Implements block-based allocation with line-level tracking - Provides mark-and-sweep collection without object movement - Includes optimisations for large object allocation and block recycling - Handles memory exhaustion through emergency collection mechanisms Relationship to Immix Algorithm: The implementation captures Immix's memory layout benefits (cache-friendly block organisation, efficient hole detection) but omits the core evacuation phase that distinguishes Immix from traditional mark-sweep collectors. This results in a hybrid approach that combines Immix memory organisation with mark-in-place collection. Practical Considerations: For eucalypt's typical workloads (configuration processing, template rendering, data transformation), the current implementation provides adequate memory management. The lack of evacuation limits performance gains for long-running or heavily fragmented applications, but this matches eucalypt's primary use cases.","title":"Conclusion"},{"location":"getting-started/","text":"Installation The current implementation of eucalypt is available in the eucalypt project and can be installed as follows. On macOS via Homebrew If you use homebrew, you can install using brew install curvelogic/homebrew-tap/eucalypt Otherwise binaries for macOS are available on the releases page . On Linux x86_64 binaries built in CI are available on the releases page On Windows Sorry, haven't got there yet. But you could try installing from source. From source You will need a rust installation and cargo . Build and install should be as simple as: cargo install --path . Testing your installation eu --version ...prints the version: $ eu --version eu 0.2.0 ...and... eu --help ...shows command line help: A functional language for structured data Usage: eu [OPTIONS] [FILES]... [COMMAND] Commands: run Evaluate eucalypt code (default) test Run tests dump Dump intermediate representations version Show version information explain Explain what would be executed list-targets List targets defined in the source fmt Format eucalypt source files lsp Start the Language Server Protocol server help Print this message or the help of the given subcommand(s) Arguments: [FILES]... Files to process (used when no subcommand specified) Options: -L, --lib-path <LIB_PATH> Add directory to lib path -Q, --no-prelude Don't load the standard prelude -B, --batch Batch mode (no .eucalypt.d) -d, --debug Turn on debug features -S, --statistics Print metrics to stderr before exiting --statistics-file <STATISTICS_FILE> Write statistics as JSON to a file -h, --help Print help -V, --version Print version Use eu <command> --help for detailed help on each subcommand.","title":"Getting Started"},{"location":"getting-started/#installation","text":"The current implementation of eucalypt is available in the eucalypt project and can be installed as follows.","title":"Installation"},{"location":"getting-started/#on-macos-via-homebrew","text":"If you use homebrew, you can install using brew install curvelogic/homebrew-tap/eucalypt Otherwise binaries for macOS are available on the releases page .","title":"On macOS via Homebrew"},{"location":"getting-started/#on-linux","text":"x86_64 binaries built in CI are available on the releases page","title":"On Linux"},{"location":"getting-started/#on-windows","text":"Sorry, haven't got there yet. But you could try installing from source.","title":"On Windows"},{"location":"getting-started/#from-source","text":"You will need a rust installation and cargo . Build and install should be as simple as: cargo install --path .","title":"From source"},{"location":"getting-started/#testing-your-installation","text":"eu --version ...prints the version: $ eu --version eu 0.2.0 ...and... eu --help ...shows command line help: A functional language for structured data Usage: eu [OPTIONS] [FILES]... [COMMAND] Commands: run Evaluate eucalypt code (default) test Run tests dump Dump intermediate representations version Show version information explain Explain what would be executed list-targets List targets defined in the source fmt Format eucalypt source files lsp Start the Language Server Protocol server help Print this message or the help of the given subcommand(s) Arguments: [FILES]... Files to process (used when no subcommand specified) Options: -L, --lib-path <LIB_PATH> Add directory to lib path -Q, --no-prelude Don't load the standard prelude -B, --batch Batch mode (no .eucalypt.d) -d, --debug Turn on debug features -S, --statistics Print metrics to stderr before exiting --statistics-file <STATISTICS_FILE> Write statistics as JSON to a file -h, --help Print help -V, --version Print version Use eu <command> --help for detailed help on each subcommand.","title":"Testing your installation"},{"location":"implementation/","text":"Eucalypt implementation Since v0.2 , Eucalypt has been written in Rust . This replaces a previous 0.1.x implementation in Haskell. When you run eu , execution proceeds in several phases: parsing inputs to abstract syntax trees transformation into a core syntax and merging and manipulation of these representations compilation into STG syntax execution which is interpretation of the STG syntax by the STG machine The core syntax facilitates experimentation by allow powerful features to be implemented simply by syntax transformation (e.g. user definable operator precedence, block catenation). It is also in transformation to core syntax that the two roles of blocks in Eucalypt (name binding and data structuring) are peeled apart into separate elements (a recursive let and a data structure). The interpreter is modeled on an eval-apply STG (spineless tagless G-machine) implementation just to have a well-defined reference point in view for a lazy functional language abstract machine, not for any other reason (concurrency etc.) Ultimately it might well be quicker and clearer to tree-walk the core representation. After some experimentation with cycle collectors, the current implementation deliberately leaks memory until a custom allocator and GC can be implemented. This suits most current uses just fine. Diagnostics Raw ASTs can be dumped (as JSON) using the -p command line switch. Execution can be traced out using the -d debug switch. Core syntax can be dumped at various stages using a variety of --dump-xxx command line switches. STG syntax wrappers for intrinsic functions can be viewed with --dump-runtime . The final STG syntax which is executed by the STG machine may be dumped using --dump-stg .","title":"Overview"},{"location":"implementation/#eucalypt-implementation","text":"Since v0.2 , Eucalypt has been written in Rust . This replaces a previous 0.1.x implementation in Haskell. When you run eu , execution proceeds in several phases: parsing inputs to abstract syntax trees transformation into a core syntax and merging and manipulation of these representations compilation into STG syntax execution which is interpretation of the STG syntax by the STG machine The core syntax facilitates experimentation by allow powerful features to be implemented simply by syntax transformation (e.g. user definable operator precedence, block catenation). It is also in transformation to core syntax that the two roles of blocks in Eucalypt (name binding and data structuring) are peeled apart into separate elements (a recursive let and a data structure). The interpreter is modeled on an eval-apply STG (spineless tagless G-machine) implementation just to have a well-defined reference point in view for a lazy functional language abstract machine, not for any other reason (concurrency etc.) Ultimately it might well be quicker and clearer to tree-walk the core representation. After some experimentation with cycle collectors, the current implementation deliberately leaks memory until a custom allocator and GC can be implemented. This suits most current uses just fine.","title":"Eucalypt implementation"},{"location":"implementation/#diagnostics","text":"Raw ASTs can be dumped (as JSON) using the -p command line switch. Execution can be traced out using the -d debug switch. Core syntax can be dumped at various stages using a variety of --dump-xxx command line switches. STG syntax wrappers for intrinsic functions can be viewed with --dump-runtime . The final STG syntax which is executed by the STG machine may be dumped using --dump-stg .","title":"Diagnostics"},{"location":"imports/","text":"Imports Eucalypt supports importing content from other units in a variety of ways. Imported names can be scoped to specific declarations, they may be made accessible under a specific namespace, and they may be imported from disk or direct from git repositories. Import scopes Imports are specified in declaration metadata and make the names in the imported unit available within the declaration that is annotated. { import: \"config.eu\" } data: { # names from config are available here x: config-value } As described in syntax , declaration metadata can be applied at a unit level simply by including a metadata block as the very first thing in a eucalypt file: { import: \"config.eu\" } # names from config are available here x: config-value Import syntax Imports are specified using the key import in a declaration metadata block. The value may be a single import specification: { import: \"dep-a.eu\"} or a list of import specifications: { import: [\"dep-a.eu\", \"dep-b.eu\"]} The import specification itself can be either a simple import or a git import . Simple imports Simple imports are specified in exactly the same way as inputs are specified at the command line (see command line ). So you can override the format of the imported file when the file extension is misleading: { import: \"yaml@dep.txt\" } ...and provide a name under which the imported names will be available: { import: \"cfg=config.eu\" } # names in config.eu are available by lookup in cfg: x: cfg.x In cases, where the import format delivers a list rather than a block (\"text\", \"csv\", \"jsonl\", ...) a name is mandatory: { import: \"txns=transactions.csv\" } Simple imports support exactly the same inputs as the command line, with the proviso that the stdin input (\"-\") will not be consumable if it has already been specified in the command line or another unit. Git imports Git imports allow you to import eucalypt direct from a git repository at a specified commit, combining the convenience of not having to explicitly manage a git working copy and a library path with the repeatability of a git SHA. A git import is specified as a block with the keys \"git\", \"commit\" and \"import\", all of which are mandatory: { import: { git: \"https://github.com/gmorpheme/eu.aws\" commit: \"0140232cf882a922bdd67b520ed56f0cddbd0637\" import: \"aws/cloudformation.eu\" } } The git URL may be any format that the git command line expects. commit is required and should be a SHA. It is intended to ensure the import is repeatable and cacheable. import identifies the file within the repository to import. Just as with simple imports, several git imports may be listed: { import: [{ git: ... }, { git: ... }]} ...and simple imports and git imports may be freely mixed. YAML import features When importing YAML files, eucalypt supports several YAML features that help reduce repetition and express data more naturally. Anchors and aliases YAML anchors ( &name ) and aliases ( *name ) allow you to define a value once and reference it multiple times. When eucalypt imports a YAML file with anchors and aliases, the aliased values are resolved to copies of the anchored expression. # config.yaml defaults: &defaults timeout: 30 retries: 3 development: <<: *defaults debug: true production: <<: *defaults debug: false Anchors can be applied to any YAML value: scalars, lists, or mappings. # Anchor on a scalar name: &author \"Alice\" books: - title: \"First Book\" author: *author - title: \"Second Book\" author: *author # Anchor on a list colours: &primary [red, green, blue] palette: primary: *primary secondary: [yellow, cyan, magenta] # Anchor on a mapping (block) base: &base x: 1 y: 2 ref: *base # ref now has { x: 1, y: 2 } Nested anchors are supported\u2014an anchored structure can itself contain anchored values: outer: &outer inner: &inner 42 ref_outer: *outer # { inner: 42 } ref_inner: *inner # 42 If you reference an undefined alias, eucalypt reports an error: # This will fail: *undefined is not defined value: *undefined Merge keys The YAML merge key ( << ) allows you to merge entries from one or more mappings into another. This is useful for creating configuration variations that share a common base. Single merge: base: &base host: localhost port: 8080 server: <<: *base name: main # server = { host: localhost, port: 8080, name: main } Multiple merge: When merging multiple mappings, later ones override earlier ones: defaults: &defaults timeout: 30 retries: 3 overrides: &overrides timeout: 60 config: <<: [*defaults, *overrides] name: myapp # config = { timeout: 60, retries: 3, name: myapp } Explicit keys override merged values: Keys defined explicitly in the mapping (before or after the merge) always take precedence over merged values: base: &base x: 1 y: 2 derived: <<: *base y: 99 # derived = { x: 1, y: 99 } Inline merge: You can also merge an inline mapping directly: config: <<: { timeout: 30, retries: 3 } name: myapp The merge key value must be a mapping (or list of mappings). Attempting to merge a non-mapping value (e.g., <<: 42 ) results in an error. Timestamps Eucalypt automatically converts YAML timestamps to ZDT (zoned date-time) expressions. Plain scalar values matching timestamp patterns are parsed and converted; quoted strings are left as strings. Supported formats: Format Example Notes Date only 2023-01-15 Midnight UTC ISO 8601 UTC 2023-01-15T10:30:00Z ISO 8601 offset 2023-01-15T10:30:00+05:00 Space separator 2023-01-15 10:30:00 Treated as UTC Fractional seconds 2023-01-15T10:30:00.123456Z Examples: # These are converted to ZDT expressions: created: 2023-01-15 updated: 2023-01-15T10:30:00Z scheduled: 2023-06-01 09:00:00 # This remains a string (quoted): date_string: \"2023-01-15T10:30:00Z\" Invalid timestamps fall back to strings: If a value looks like a timestamp but has invalid date components (e.g., month 13 or day 45), it remains a string: invalid: 2023-13-45 # Remains string \"2023-13-45\" To keep timestamps as strings: If you need to preserve a timestamp-like value as a string rather than converting it to a ZDT, quote it: # As ZDT: actual_date: 2023-01-15 # As string: date_label: \"2023-01-15\"","title":"Imports"},{"location":"imports/#imports","text":"Eucalypt supports importing content from other units in a variety of ways. Imported names can be scoped to specific declarations, they may be made accessible under a specific namespace, and they may be imported from disk or direct from git repositories.","title":"Imports"},{"location":"imports/#import-scopes","text":"Imports are specified in declaration metadata and make the names in the imported unit available within the declaration that is annotated. { import: \"config.eu\" } data: { # names from config are available here x: config-value } As described in syntax , declaration metadata can be applied at a unit level simply by including a metadata block as the very first thing in a eucalypt file: { import: \"config.eu\" } # names from config are available here x: config-value","title":"Import scopes"},{"location":"imports/#import-syntax","text":"Imports are specified using the key import in a declaration metadata block. The value may be a single import specification: { import: \"dep-a.eu\"} or a list of import specifications: { import: [\"dep-a.eu\", \"dep-b.eu\"]} The import specification itself can be either a simple import or a git import .","title":"Import syntax"},{"location":"imports/#simple-imports","text":"Simple imports are specified in exactly the same way as inputs are specified at the command line (see command line ). So you can override the format of the imported file when the file extension is misleading: { import: \"yaml@dep.txt\" } ...and provide a name under which the imported names will be available: { import: \"cfg=config.eu\" } # names in config.eu are available by lookup in cfg: x: cfg.x In cases, where the import format delivers a list rather than a block (\"text\", \"csv\", \"jsonl\", ...) a name is mandatory: { import: \"txns=transactions.csv\" } Simple imports support exactly the same inputs as the command line, with the proviso that the stdin input (\"-\") will not be consumable if it has already been specified in the command line or another unit.","title":"Simple imports"},{"location":"imports/#git-imports","text":"Git imports allow you to import eucalypt direct from a git repository at a specified commit, combining the convenience of not having to explicitly manage a git working copy and a library path with the repeatability of a git SHA. A git import is specified as a block with the keys \"git\", \"commit\" and \"import\", all of which are mandatory: { import: { git: \"https://github.com/gmorpheme/eu.aws\" commit: \"0140232cf882a922bdd67b520ed56f0cddbd0637\" import: \"aws/cloudformation.eu\" } } The git URL may be any format that the git command line expects. commit is required and should be a SHA. It is intended to ensure the import is repeatable and cacheable. import identifies the file within the repository to import. Just as with simple imports, several git imports may be listed: { import: [{ git: ... }, { git: ... }]} ...and simple imports and git imports may be freely mixed.","title":"Git imports"},{"location":"imports/#yaml-import-features","text":"When importing YAML files, eucalypt supports several YAML features that help reduce repetition and express data more naturally.","title":"YAML import features"},{"location":"imports/#anchors-and-aliases","text":"YAML anchors ( &name ) and aliases ( *name ) allow you to define a value once and reference it multiple times. When eucalypt imports a YAML file with anchors and aliases, the aliased values are resolved to copies of the anchored expression. # config.yaml defaults: &defaults timeout: 30 retries: 3 development: <<: *defaults debug: true production: <<: *defaults debug: false Anchors can be applied to any YAML value: scalars, lists, or mappings. # Anchor on a scalar name: &author \"Alice\" books: - title: \"First Book\" author: *author - title: \"Second Book\" author: *author # Anchor on a list colours: &primary [red, green, blue] palette: primary: *primary secondary: [yellow, cyan, magenta] # Anchor on a mapping (block) base: &base x: 1 y: 2 ref: *base # ref now has { x: 1, y: 2 } Nested anchors are supported\u2014an anchored structure can itself contain anchored values: outer: &outer inner: &inner 42 ref_outer: *outer # { inner: 42 } ref_inner: *inner # 42 If you reference an undefined alias, eucalypt reports an error: # This will fail: *undefined is not defined value: *undefined","title":"Anchors and aliases"},{"location":"imports/#merge-keys","text":"The YAML merge key ( << ) allows you to merge entries from one or more mappings into another. This is useful for creating configuration variations that share a common base. Single merge: base: &base host: localhost port: 8080 server: <<: *base name: main # server = { host: localhost, port: 8080, name: main } Multiple merge: When merging multiple mappings, later ones override earlier ones: defaults: &defaults timeout: 30 retries: 3 overrides: &overrides timeout: 60 config: <<: [*defaults, *overrides] name: myapp # config = { timeout: 60, retries: 3, name: myapp } Explicit keys override merged values: Keys defined explicitly in the mapping (before or after the merge) always take precedence over merged values: base: &base x: 1 y: 2 derived: <<: *base y: 99 # derived = { x: 1, y: 99 } Inline merge: You can also merge an inline mapping directly: config: <<: { timeout: 30, retries: 3 } name: myapp The merge key value must be a mapping (or list of mappings). Attempting to merge a non-mapping value (e.g., <<: 42 ) results in an error.","title":"Merge keys"},{"location":"imports/#timestamps","text":"Eucalypt automatically converts YAML timestamps to ZDT (zoned date-time) expressions. Plain scalar values matching timestamp patterns are parsed and converted; quoted strings are left as strings. Supported formats: Format Example Notes Date only 2023-01-15 Midnight UTC ISO 8601 UTC 2023-01-15T10:30:00Z ISO 8601 offset 2023-01-15T10:30:00+05:00 Space separator 2023-01-15 10:30:00 Treated as UTC Fractional seconds 2023-01-15T10:30:00.123456Z Examples: # These are converted to ZDT expressions: created: 2023-01-15 updated: 2023-01-15T10:30:00Z scheduled: 2023-06-01 09:00:00 # This remains a string (quoted): date_string: \"2023-01-15T10:30:00Z\" Invalid timestamps fall back to strings: If a value looks like a timestamp but has invalid date components (e.g., month 13 or day 45), it remains a string: invalid: 2023-13-45 # Remains string \"2023-13-45\" To keep timestamps as strings: If you need to preserve a timestamp-like value as a string rather than converting it to a ZDT, quote it: # As ZDT: actual_date: 2023-01-15 # As string: date_label: \"2023-01-15\"","title":"Timestamps"},{"location":"operators-and-identifiers/","text":"Operators and Identifiers Eucalypt distinguishes two different types of identifier, normal identifiers, like x , y , \u03b1 , \u05d0 , ziggety-zaggety , zoom? , and operator identifiers like * , @ , && , \u2227 , \u2218 , \u2299\u2299\u2299 , <> and so on. It is entirely a matter of the component characters which category and identifier falls into. Normal identifiers contain letters (including non-ascii characters), numbers, \"-\", \"?\", \"$\". Operator identifiers contain the usual suspects and anything identified as an operator or symbol in unicode. Neither can contain \":\" or \",\" or brackets which are special in eucalypt. Any sequence of characters at all can be treated as a normal identifier by surrounding them in single quotes. This is the only use of single quotes in eucalypt. This can be useful when you want to use file paths or other external identifiers as block keys for instance: home: { '.bashrc': false '.emacs.d': false 'notes.txt': true } z: home.'notes.txt' Normal identifiers Normal operators are brought into scope by declarations and can be referred to without qualification in their own block or in more nested blocks: x: { z: 99 foo: z //=> 99 bar: { y: z //=> 99 } } They can be accessed from within other blocks using the lookup operator: x: { z: 99 } y: x.z //=> 99 They can be overridden using generalised lookup: z: 99 y: { z: 100 }.\"z is {z}\" //=> \"z is 100\" They can be shadowed: z: 99 y: { z: 100 r: z //=> 100 } But beware trying to access the outer value: name: \"foo\" x: { name: name } //=> infinite recursion Accessing shadowed values is not yet easily possible unless you can refer to an enclosing block and use a lookup. Prefix operators Some operators are defined as prefix (unary) operators rather than infix (binary) operators. These bind tightly to the expression that follows. For example, the \u2191 operator is a tight-binding prefix form of head : xs: [1, 2, 3] first: \u2191xs //=> 1 Because it binds tightly (precedence 95), it works naturally in pipelines without parentheses: xs: [[1, 2], [3, 4]] result: xs map(\u2191) # map head over list of lists Other prefix operators include ! and \u00ac for boolean negation, and \u2238 for numeric negation. Operator identifiers Operator identifiers are more limited than normal identifiers. They are brought into scope by operator declarations and available without qualification in their own block and more nested blocks: ( l -->> r): \"{l} shoots arrow at {r}\" x: { y: 2 -->> 3 //=> \"2 shoots arrow at 3\" } ...and can be shadowed: (l !!! r): l + r y: { (l !!! r): l - r z: 100 !!! 1 //=> 99 } But: they cannot be accessed by lookup, so there is no way of forming a qualified name to access an operator they cannot be overridden by generalised lookup","title":"Operators and Identifiers"},{"location":"operators-and-identifiers/#operators-and-identifiers","text":"Eucalypt distinguishes two different types of identifier, normal identifiers, like x , y , \u03b1 , \u05d0 , ziggety-zaggety , zoom? , and operator identifiers like * , @ , && , \u2227 , \u2218 , \u2299\u2299\u2299 , <> and so on. It is entirely a matter of the component characters which category and identifier falls into. Normal identifiers contain letters (including non-ascii characters), numbers, \"-\", \"?\", \"$\". Operator identifiers contain the usual suspects and anything identified as an operator or symbol in unicode. Neither can contain \":\" or \",\" or brackets which are special in eucalypt. Any sequence of characters at all can be treated as a normal identifier by surrounding them in single quotes. This is the only use of single quotes in eucalypt. This can be useful when you want to use file paths or other external identifiers as block keys for instance: home: { '.bashrc': false '.emacs.d': false 'notes.txt': true } z: home.'notes.txt'","title":"Operators and Identifiers"},{"location":"operators-and-identifiers/#normal-identifiers","text":"Normal operators are brought into scope by declarations and can be referred to without qualification in their own block or in more nested blocks: x: { z: 99 foo: z //=> 99 bar: { y: z //=> 99 } } They can be accessed from within other blocks using the lookup operator: x: { z: 99 } y: x.z //=> 99 They can be overridden using generalised lookup: z: 99 y: { z: 100 }.\"z is {z}\" //=> \"z is 100\" They can be shadowed: z: 99 y: { z: 100 r: z //=> 100 } But beware trying to access the outer value: name: \"foo\" x: { name: name } //=> infinite recursion Accessing shadowed values is not yet easily possible unless you can refer to an enclosing block and use a lookup.","title":"Normal identifiers"},{"location":"operators-and-identifiers/#prefix-operators","text":"Some operators are defined as prefix (unary) operators rather than infix (binary) operators. These bind tightly to the expression that follows. For example, the \u2191 operator is a tight-binding prefix form of head : xs: [1, 2, 3] first: \u2191xs //=> 1 Because it binds tightly (precedence 95), it works naturally in pipelines without parentheses: xs: [[1, 2], [3, 4]] result: xs map(\u2191) # map head over list of lists Other prefix operators include ! and \u00ac for boolean negation, and \u2238 for numeric negation.","title":"Prefix operators"},{"location":"operators-and-identifiers/#operator-identifiers","text":"Operator identifiers are more limited than normal identifiers. They are brought into scope by operator declarations and available without qualification in their own block and more nested blocks: ( l -->> r): \"{l} shoots arrow at {r}\" x: { y: 2 -->> 3 //=> \"2 shoots arrow at 3\" } ...and can be shadowed: (l !!! r): l + r y: { (l !!! r): l - r z: 100 !!! 1 //=> 99 } But: they cannot be accessed by lookup, so there is no way of forming a qualified name to access an operator they cannot be overridden by generalised lookup","title":"Operator identifiers"},{"location":"philosophy-lang/","text":"eucalypt (the language) eucalypt , the language, is unorthodox in many respects - probably more than you might realise on first acquaintance. People tend to have deep-seated and inflexible opinions about programming languages and language design and will quite possibly find something in here that they have a kneejerk reaction against. However, the design is not unprincipled and, while it is experimental in some respects, I believe it's internally consistent. Several aspects of the design and the aesthetic are driven by the primary use case, templating and generating YAML. Maybe by exploring some of the inspiration and philosophy behind the language itself, I can pre-empt some of the knee jerks. Accept crypticality for minimal intrusion eucalypt is first and foremost a tool , rather than a language. It is intended to replace generation and transformation processes on semi-structured data formats. Many or most uses of eucalypt the language should just be simple one-liner tags in YAML files, or maybe eucalypt files that are predominantly data rather than manipulation. The eucalypt language is the depth behind these one-liners that allows eucalypt to accommodate increasingly ambitious use cases without breaking the paradigm and reaching for a general purpose imperative scripting language or the lowest common denominator or text-based templating languages. The pre-eminence of one-liners and small annotations and \"logic mark-up\", means that eucalypt often favours concise and cryptic over wordy and transparent. This is a controversial approach. eucalypt logic should \"get out of the way\" of the data. Templating is attractive precisely because the generating source looks very like the result. Template tags are often short (with \"cryptic\" delimiters - {{}} , <%= %> , [| ] ...) because these are \"marking up\" the data which is the main event. At the same time, the tags are often \"noisy\" or visually disruptive to ensure they cannot be ignored. eucalypt via operator and bracket definitions, picks and chooseS from a similar palette of expressive effects to try and be a sympathetic cohabitee with its accompanying data. There are many cases where it makes sense to resist offering an incomplete understanding in favour of demanding full understanding. For example, it is spurious to say that bind(x, f) gives more understanding of what is going on than x >>= f - unless you understand the monad abstraction and the role of bind in it, you gain nothing useful from the ideas that the word bind connotes when you are trying to understand program text. eucalypt just plain ignores the notion that program text should be readable as English text . This (well motivated) idea has made a resurgence in recent years through the back door of internal DSLs and \"fluent\" Java interfaces. There is much merit in languages supple enough to allow the APIs to approach the natural means of expression of the problem domain. However, problem domains frequently have their own technical jargon and notation which suit their purpose better than natural language so it cuts both ways. Program text should be approachable by its target audience but that does not mean it should make no demands of its target audience. These stances lead directly to several slightly esoteric aspects of eucalypt that may be obnoxious to some: eucalypt tends to be operator-heavy. Operators are concise (if cryptic) and the full range of unicode is available to call upon. Using operators keeps custom logic visually out of the way of the data whilst also signposting it to attract closer attention. eucalypt lets you define your own operators and specify their precedence and associativity (which are applied at a relatively late stage in the evaluation pipeline - operator soup persists through the initial parse). There are no ternary operators. For absolute minimal intrusion, merely the act of placing elements next to each other (\"catenation\"), x f , is meaningful in eucalypt . By default this is pipeline-order function application, but blocks can be applied as functions to make common transformations, like block merge, very succinct. For even more power, eucalypt might soon let you alter the meaning of concatenation via overloaded idiot brackets 1 . ( \u00abx y\u00bb: ... ). This is inspired by the idiom brackets that can be used to express applicative styles in functional programming 2 . These may also provide an acceptable proxy for ternary and other operators too. An equivalent generalisation of eucalypt block syntax to provide a capability similar to Haskell's do notation could conceivably follow. Cohabitation of code and data Just like templates, eucalypt source (or eucalypt -tagged YAML) should be almost entirely data. The idea behind eucalypt is to adopt the basic maps-and-arrays organisation philosophy of these data formats but make the data active - allowing lambdas to live in and amongst it and operate on it and allowing the data to express dispositions towards its environment by addition of metadata that controls import, export, and execution preferences. eucalypt therefore collapses the separation of code and data to some degree. You can run eu against a mixture of YAML, JSON and eucalypt files and all the data and logic appears there together in the same namespace hierarchy. The namespace hierarchy just is the data. However, code and data aren't unified in the sense of Lisp for instance. eucalypt is not homoiconic. The relationship is more like cohabitation; code lives in amongst the data it operates on but is stripped out before export. Nevertheless eucalypt is heavily inspired by Lisp and aims for a similar fluidity through: lazy evaluation (going some way towards matching uses of Lisp macros which control evaluation order - in eucalypt, if is just a function) economical syntax to facilitate (future) manipulation of code as data Simplicity eucalypt values simplicity in the sense of fewer moving parts (and therefore, hopefully, fewer things to go wrong). It values ease of use in the sense of offering a rich and powerful toolkit. You may not think it achieves either. eucalypt values familiarity mostly in the \"shallower\" parts of the language where it only requires a couple of mental leaps for the average programmer in these areas - the (ab)use of catenation being the key one. However, eucalypt isn't ashamed of its dusty corners. Dusty corners are areas where novices and experts alike can get trapped and lose time but they're also rich seams for experimentation, innovation and discovery. If you have to venture too far off-piste to find what you need, we'll find a way to bring it onto the nursery slopes but we won't close off the mountain. Footnotes Inspired by idiom brackets . If I didn't call them that, someone else would. \u21a9 Applicative Programming with Effects, Conor McBride and Ross Paterson. (2008) http://www.staff.city.ac.uk/~ross/papers/Applicative.html \u21a9","title":"Philosophy"},{"location":"philosophy-lang/#eucalypt-the-language","text":"eucalypt , the language, is unorthodox in many respects - probably more than you might realise on first acquaintance. People tend to have deep-seated and inflexible opinions about programming languages and language design and will quite possibly find something in here that they have a kneejerk reaction against. However, the design is not unprincipled and, while it is experimental in some respects, I believe it's internally consistent. Several aspects of the design and the aesthetic are driven by the primary use case, templating and generating YAML. Maybe by exploring some of the inspiration and philosophy behind the language itself, I can pre-empt some of the knee jerks.","title":"eucalypt (the language)"},{"location":"philosophy-lang/#accept-crypticality-for-minimal-intrusion","text":"eucalypt is first and foremost a tool , rather than a language. It is intended to replace generation and transformation processes on semi-structured data formats. Many or most uses of eucalypt the language should just be simple one-liner tags in YAML files, or maybe eucalypt files that are predominantly data rather than manipulation. The eucalypt language is the depth behind these one-liners that allows eucalypt to accommodate increasingly ambitious use cases without breaking the paradigm and reaching for a general purpose imperative scripting language or the lowest common denominator or text-based templating languages. The pre-eminence of one-liners and small annotations and \"logic mark-up\", means that eucalypt often favours concise and cryptic over wordy and transparent. This is a controversial approach. eucalypt logic should \"get out of the way\" of the data. Templating is attractive precisely because the generating source looks very like the result. Template tags are often short (with \"cryptic\" delimiters - {{}} , <%= %> , [| ] ...) because these are \"marking up\" the data which is the main event. At the same time, the tags are often \"noisy\" or visually disruptive to ensure they cannot be ignored. eucalypt via operator and bracket definitions, picks and chooseS from a similar palette of expressive effects to try and be a sympathetic cohabitee with its accompanying data. There are many cases where it makes sense to resist offering an incomplete understanding in favour of demanding full understanding. For example, it is spurious to say that bind(x, f) gives more understanding of what is going on than x >>= f - unless you understand the monad abstraction and the role of bind in it, you gain nothing useful from the ideas that the word bind connotes when you are trying to understand program text. eucalypt just plain ignores the notion that program text should be readable as English text . This (well motivated) idea has made a resurgence in recent years through the back door of internal DSLs and \"fluent\" Java interfaces. There is much merit in languages supple enough to allow the APIs to approach the natural means of expression of the problem domain. However, problem domains frequently have their own technical jargon and notation which suit their purpose better than natural language so it cuts both ways. Program text should be approachable by its target audience but that does not mean it should make no demands of its target audience. These stances lead directly to several slightly esoteric aspects of eucalypt that may be obnoxious to some: eucalypt tends to be operator-heavy. Operators are concise (if cryptic) and the full range of unicode is available to call upon. Using operators keeps custom logic visually out of the way of the data whilst also signposting it to attract closer attention. eucalypt lets you define your own operators and specify their precedence and associativity (which are applied at a relatively late stage in the evaluation pipeline - operator soup persists through the initial parse). There are no ternary operators. For absolute minimal intrusion, merely the act of placing elements next to each other (\"catenation\"), x f , is meaningful in eucalypt . By default this is pipeline-order function application, but blocks can be applied as functions to make common transformations, like block merge, very succinct. For even more power, eucalypt might soon let you alter the meaning of concatenation via overloaded idiot brackets 1 . ( \u00abx y\u00bb: ... ). This is inspired by the idiom brackets that can be used to express applicative styles in functional programming 2 . These may also provide an acceptable proxy for ternary and other operators too. An equivalent generalisation of eucalypt block syntax to provide a capability similar to Haskell's do notation could conceivably follow.","title":"Accept crypticality for minimal intrusion"},{"location":"philosophy-lang/#cohabitation-of-code-and-data","text":"Just like templates, eucalypt source (or eucalypt -tagged YAML) should be almost entirely data. The idea behind eucalypt is to adopt the basic maps-and-arrays organisation philosophy of these data formats but make the data active - allowing lambdas to live in and amongst it and operate on it and allowing the data to express dispositions towards its environment by addition of metadata that controls import, export, and execution preferences. eucalypt therefore collapses the separation of code and data to some degree. You can run eu against a mixture of YAML, JSON and eucalypt files and all the data and logic appears there together in the same namespace hierarchy. The namespace hierarchy just is the data. However, code and data aren't unified in the sense of Lisp for instance. eucalypt is not homoiconic. The relationship is more like cohabitation; code lives in amongst the data it operates on but is stripped out before export. Nevertheless eucalypt is heavily inspired by Lisp and aims for a similar fluidity through: lazy evaluation (going some way towards matching uses of Lisp macros which control evaluation order - in eucalypt, if is just a function) economical syntax to facilitate (future) manipulation of code as data","title":"Cohabitation of code and data"},{"location":"philosophy-lang/#simplicity","text":"eucalypt values simplicity in the sense of fewer moving parts (and therefore, hopefully, fewer things to go wrong). It values ease of use in the sense of offering a rich and powerful toolkit. You may not think it achieves either. eucalypt values familiarity mostly in the \"shallower\" parts of the language where it only requires a couple of mental leaps for the average programmer in these areas - the (ab)use of catenation being the key one. However, eucalypt isn't ashamed of its dusty corners. Dusty corners are areas where novices and experts alike can get trapped and lose time but they're also rich seams for experimentation, innovation and discovery. If you have to venture too far off-piste to find what you need, we'll find a way to bring it onto the nursery slopes but we won't close off the mountain.","title":"Simplicity"},{"location":"philosophy-lang/#footnotes","text":"Inspired by idiom brackets . If I didn't call them that, someone else would. \u21a9 Applicative Programming with Effects, Conor McBride and Ross Paterson. (2008) http://www.staff.city.ac.uk/~ross/papers/Applicative.html \u21a9","title":"Footnotes"},{"location":"prelude/","text":"Prelude Reference The eucalypt prelude is a standard library of functions, operators, and constants that is automatically loaded before your code runs. You can suppress the prelude with -Q if needed, though this leaves a very bare environment (even true , false , and if are defined in the prelude). Metadata and IO eu namespace eu.prelude.version - Version of the standard prelude eu.build - Build metadata for the eucalypt executable io namespace io.env - Block of environment variables at launch time io.epoch-time - Unix timestamp at launch time io.args - List of command-line arguments passed after -- separator Essentials Constants null - Null value (exports as null in JSON, ~ in YAML) true - Boolean true false - Boolean false nil - Empty list [] Control Flow Function Description if(c, t, f) If c is true return t , else f then(t, f, c) Pipeline-friendly if: x? then(t, f) when(p?, f, x) When x satisfies p? , apply f , else pass through cond(l, d) Select first true condition from list of [condition, value] pairs, else default d Error Handling Function Description panic(s) Raise runtime error with message s assert(c, s, v) If c is true return v , else error with message s Lists Basic Operations Function Description cons(h, t) Prepend item h to list t head(xs) First item of list (error if empty) \u2191xs Tight-binding prefix form of head (prec 95) head-or(d, xs) First item or default d if empty tail(xs) List without first item (error if empty) tail-or(d, xs) List without first item or d if empty first(xs) Alias for head second(xs) Second item of list second-or(d, xs) Second item or default d last(l) Last element of list nil?(xs) True if list is empty nth(n, l) Return n th item (0-indexed) l !! n Operator form of nth count(l) Number of items in list List Construction Function Description repeat(i) Infinite list of item i ints-from(n) Infinite list of integers from n upwards range(b, e) List of integers from b to e (exclusive) cycle(l) Infinite list cycling elements of l iterate(f, i) List of i , f(i) , f(f(i)) , ... Transformations Function Description map(f, l) Apply f to each element f <$> l Operator form of map map2(f, l1, l2) Map f over two lists in parallel filter(p?, l) Keep elements satisfying predicate p? remove(p?, l) Remove elements satisfying predicate p? reverse(l) Reverse list take(n, l) First n elements drop(n, l) List after dropping n elements take-while(p?, l) Initial elements while p? is true take-until(p?, l) Initial elements while p? is false drop-while(p?, l) Skip elements while p? is true drop-until(p?, l) Skip elements while p? is false Combining Lists Function Description append(l1, l2) Concatenate two lists l1 ++ l2 Operator form of append prepend(l1, l2) Concatenate with l1 after l2 concat(ls) Concatenate list of lists mapcat(f, l) Map then concatenate results zip(l1, l2) List of pairs from two lists zip-with(f, l1, l2) Apply f to parallel elements zip-apply(fs, vs) Apply functions to corresponding values Splitting Lists Function Description split-at(n, l) Split at index n , return pair split-after(p?, l) Split where p? becomes false split-when(p?, l) Split where p? becomes true window(n, step, l) Sliding windows of size n with offset step partition(n, l) Non-overlapping segments of size n discriminate(pred, xs) Split into [matches, non-matches] Folds and Scans Function Description foldl(op, i, l) Left fold with initial value i foldr(op, i, l) Right fold with final value i scanl(op, i, l) Left scan (intermediate fold values) scanr(op, i, l) Right scan Predicates Function Description all(p?, l) True if all elements satisfy p? all-true?(l) True if all elements are true any(p?, l) True if any element satisfies p? any-true?(l) True if any element is true Sorting Function Description qsort(lt, xs) Sort using less-than function lt group-by(k, xs) Group by key function, returns block Other Function Description over-sliding-pairs(f, l) Apply binary f to overlapping pairs differences(l) Differences between adjacent numbers Blocks Construction Function Description block(kvs) Construct block from list of [key, value] pairs pair(k, v) Create a [key, value] pair sym(s) Create symbol from string s tongue(ks, v) Create nested block from key path to value zip-kv(ks, vs) Create block by zipping keys and values with-keys(ks) Alias for zip-kv map-as-block(f, syms) Map symbols and create block Access Function Description lookup(s, b) Look up symbol s in block (error if missing) lookup-in(b, s) Same as lookup with swapped args lookup-or(s, d, b) Look up with default d if missing lookup-or-in(b, s, d) Same with swapped args lookup-alts(syms, d, b) Try symbols in order until found lookup-across(s, d, bs) Look up in sequence of blocks lookup-path(ks, b) Look up nested key path has(s, b) True if block has key s elements(b) List of [key, value] pairs keys(b) List of keys values(b) List of values key(pr) Key from a pair value(pr) Value from a pair Merging Function Description merge(b1, b2) Shallow merge b2 onto b1 deep-merge(b1, b2) Deep merge (nested blocks) l << r Operator for deep merge merge-all(bs) Merge list of blocks merge-at(ks, v, b) Merge v at key path ks Transformation Function Description map-values(f, b) Apply f to each value map-keys(f, b) Apply f to each key map-kv(f, b) Apply f(k, v) to each pair, return list filter-items(f, b) Filter items by predicate on pairs filter-values(p?, b) Values matching predicate match-filter-values(re, b) Values with keys matching regex Item Predicates Function Description by-key(p?) Predicate on key by-key-name(p?) Predicate on key as string by-key-match(re) Predicate matching key against regex by-value(p?) Predicate on value Mutation Function Description alter-value(k, v, b) Set b.k to v update-value(k, f, b) Apply f to b.k alter(ks, v, b) Set value at nested key path update(ks, f, b) Apply f at nested key path update-value-or(k, f, d, b) Update or add with default set-value(k, v, b) Set value, adding if absent Booleans Functions Function Description not(b) Toggle boolean and(l, r) Logical and or(l, r) Logical or Operators Operator Description !x or \u00acx Not (prefix) l && r or l \u2227 r And l \\|\\| r or l \u2228 r Or Equality and Comparison Operator Description l = r Equality l != r Inequality l < r Less than l > r Greater than l <= r Less than or equal l >= r Greater than or equal Arithmetic Operators Operator Description l + r Addition l - r Subtraction l * r Multiplication l / r Division l % r Modulus \u2238 n Unary minus (negate) Functions Function Description inc(x) Increment by 1 dec(x) Decrement by 1 negate(n) Negate number num(s) Parse number from string floor(n) Round down to integer ceiling(n) Round up to integer max(l, r) Maximum of two numbers min(l, r) Minimum of two numbers max-of(l) Maximum in list min-of(l) Minimum in list Predicates Function Description zero?(n) True if n is 0 pos?(n) True if n is positive neg?(n) True if n is negative Strings The str namespace contains string functions: Function Description str.of(e) Convert to string str.split(s, re) Split string on regex str.split-on(re, s) Split (pipeline-friendly) str.join(l, s) Join list with separator str.join-on(s, l) Join (pipeline-friendly) str.match(s, re) Match regex, return captures str.match-with(re, s) Match (pipeline-friendly) str.matches(s, re) All matches of regex str.matches-of(re, s) All matches (pipeline-friendly) str.matches?(re, s) True if regex matches full string str.extract(re, s) Extract single capture str.extract-or(re, d, s) Extract with default str.suffix(b, a) Suffix b onto a str.prefix(b, a) Prefix b onto a str.letters(s) List of characters str.len(s) String length str.fmt(x, spec) Printf-style formatting str.to-upper(s) Convert to upper case str.to-lower(s) Convert to lower case Character Constants The ch namespace provides special characters: ch.n - Newline ch.t - Tab ch.dq - Double quote Function Combinators Function Description identity(v) Return v unchanged const(k) Function that always returns k -> k Operator form of const compose(f, g, x) Apply f to g(x) f \u2218 g Composition: g then f f ; g Composition: f then g l @ r Application: l(r) apply(f, xs) Apply f to args in list flip(f) Swap argument order complement(p?) Invert predicate curry(f) Convert f([x,y]) to f(x,y) uncurry(f) Convert f(x,y) to f([x,y]) juxt(f, g, x) Return [f(x), g(x)] fnil(f, v, x) Replace null with v before applying f Pairs Function Description pair(k, v) Create pair [k, v] bimap(f, g, pr) Apply f to first, g to second map-first(f, prs) Apply f to first elements map-second(f, prs) Apply f to second elements Metadata Function Description with-meta(m, e) Add metadata block m to expression e e // m Operator form of with-meta meta(e) Retrieve metadata from expression merge-meta(m, e) Merge into existing metadata e //<< m Operator form of merge-meta Assertions Operator Description e //= v Check if e equals v , return boolean e //=> v Assert e equals v , return e or panic e //=? f Assert e satisfies predicate f e //!? f Assert e does not satisfy f e //! Assert e is true e //!! Assert e is false Calendar / Time The cal namespace provides date/time functions: Function Description cal.now Current time as fields block cal.epoch Unix epoch as fields block cal.zdt(y,m,d,H,M,S,Z) Create zoned datetime cal.datetime(b) Create from block with defaults cal.parse(s) Parse ISO8601 string cal.format(t) Format as ISO8601 cal.fields(t) Decompose to {y,m,d,H,M,S,Z} block","title":"Prelude Reference"},{"location":"prelude/#prelude-reference","text":"The eucalypt prelude is a standard library of functions, operators, and constants that is automatically loaded before your code runs. You can suppress the prelude with -Q if needed, though this leaves a very bare environment (even true , false , and if are defined in the prelude).","title":"Prelude Reference"},{"location":"prelude/#metadata-and-io","text":"","title":"Metadata and IO"},{"location":"prelude/#eu-namespace","text":"eu.prelude.version - Version of the standard prelude eu.build - Build metadata for the eucalypt executable","title":"eu namespace"},{"location":"prelude/#io-namespace","text":"io.env - Block of environment variables at launch time io.epoch-time - Unix timestamp at launch time io.args - List of command-line arguments passed after -- separator","title":"io namespace"},{"location":"prelude/#essentials","text":"","title":"Essentials"},{"location":"prelude/#constants","text":"null - Null value (exports as null in JSON, ~ in YAML) true - Boolean true false - Boolean false nil - Empty list []","title":"Constants"},{"location":"prelude/#control-flow","text":"Function Description if(c, t, f) If c is true return t , else f then(t, f, c) Pipeline-friendly if: x? then(t, f) when(p?, f, x) When x satisfies p? , apply f , else pass through cond(l, d) Select first true condition from list of [condition, value] pairs, else default d","title":"Control Flow"},{"location":"prelude/#error-handling","text":"Function Description panic(s) Raise runtime error with message s assert(c, s, v) If c is true return v , else error with message s","title":"Error Handling"},{"location":"prelude/#lists","text":"","title":"Lists"},{"location":"prelude/#basic-operations","text":"Function Description cons(h, t) Prepend item h to list t head(xs) First item of list (error if empty) \u2191xs Tight-binding prefix form of head (prec 95) head-or(d, xs) First item or default d if empty tail(xs) List without first item (error if empty) tail-or(d, xs) List without first item or d if empty first(xs) Alias for head second(xs) Second item of list second-or(d, xs) Second item or default d last(l) Last element of list nil?(xs) True if list is empty nth(n, l) Return n th item (0-indexed) l !! n Operator form of nth count(l) Number of items in list","title":"Basic Operations"},{"location":"prelude/#list-construction","text":"Function Description repeat(i) Infinite list of item i ints-from(n) Infinite list of integers from n upwards range(b, e) List of integers from b to e (exclusive) cycle(l) Infinite list cycling elements of l iterate(f, i) List of i , f(i) , f(f(i)) , ...","title":"List Construction"},{"location":"prelude/#transformations","text":"Function Description map(f, l) Apply f to each element f <$> l Operator form of map map2(f, l1, l2) Map f over two lists in parallel filter(p?, l) Keep elements satisfying predicate p? remove(p?, l) Remove elements satisfying predicate p? reverse(l) Reverse list take(n, l) First n elements drop(n, l) List after dropping n elements take-while(p?, l) Initial elements while p? is true take-until(p?, l) Initial elements while p? is false drop-while(p?, l) Skip elements while p? is true drop-until(p?, l) Skip elements while p? is false","title":"Transformations"},{"location":"prelude/#combining-lists","text":"Function Description append(l1, l2) Concatenate two lists l1 ++ l2 Operator form of append prepend(l1, l2) Concatenate with l1 after l2 concat(ls) Concatenate list of lists mapcat(f, l) Map then concatenate results zip(l1, l2) List of pairs from two lists zip-with(f, l1, l2) Apply f to parallel elements zip-apply(fs, vs) Apply functions to corresponding values","title":"Combining Lists"},{"location":"prelude/#splitting-lists","text":"Function Description split-at(n, l) Split at index n , return pair split-after(p?, l) Split where p? becomes false split-when(p?, l) Split where p? becomes true window(n, step, l) Sliding windows of size n with offset step partition(n, l) Non-overlapping segments of size n discriminate(pred, xs) Split into [matches, non-matches]","title":"Splitting Lists"},{"location":"prelude/#folds-and-scans","text":"Function Description foldl(op, i, l) Left fold with initial value i foldr(op, i, l) Right fold with final value i scanl(op, i, l) Left scan (intermediate fold values) scanr(op, i, l) Right scan","title":"Folds and Scans"},{"location":"prelude/#predicates","text":"Function Description all(p?, l) True if all elements satisfy p? all-true?(l) True if all elements are true any(p?, l) True if any element satisfies p? any-true?(l) True if any element is true","title":"Predicates"},{"location":"prelude/#sorting","text":"Function Description qsort(lt, xs) Sort using less-than function lt group-by(k, xs) Group by key function, returns block","title":"Sorting"},{"location":"prelude/#other","text":"Function Description over-sliding-pairs(f, l) Apply binary f to overlapping pairs differences(l) Differences between adjacent numbers","title":"Other"},{"location":"prelude/#blocks","text":"","title":"Blocks"},{"location":"prelude/#construction","text":"Function Description block(kvs) Construct block from list of [key, value] pairs pair(k, v) Create a [key, value] pair sym(s) Create symbol from string s tongue(ks, v) Create nested block from key path to value zip-kv(ks, vs) Create block by zipping keys and values with-keys(ks) Alias for zip-kv map-as-block(f, syms) Map symbols and create block","title":"Construction"},{"location":"prelude/#access","text":"Function Description lookup(s, b) Look up symbol s in block (error if missing) lookup-in(b, s) Same as lookup with swapped args lookup-or(s, d, b) Look up with default d if missing lookup-or-in(b, s, d) Same with swapped args lookup-alts(syms, d, b) Try symbols in order until found lookup-across(s, d, bs) Look up in sequence of blocks lookup-path(ks, b) Look up nested key path has(s, b) True if block has key s elements(b) List of [key, value] pairs keys(b) List of keys values(b) List of values key(pr) Key from a pair value(pr) Value from a pair","title":"Access"},{"location":"prelude/#merging","text":"Function Description merge(b1, b2) Shallow merge b2 onto b1 deep-merge(b1, b2) Deep merge (nested blocks) l << r Operator for deep merge merge-all(bs) Merge list of blocks merge-at(ks, v, b) Merge v at key path ks","title":"Merging"},{"location":"prelude/#transformation","text":"Function Description map-values(f, b) Apply f to each value map-keys(f, b) Apply f to each key map-kv(f, b) Apply f(k, v) to each pair, return list filter-items(f, b) Filter items by predicate on pairs filter-values(p?, b) Values matching predicate match-filter-values(re, b) Values with keys matching regex","title":"Transformation"},{"location":"prelude/#item-predicates","text":"Function Description by-key(p?) Predicate on key by-key-name(p?) Predicate on key as string by-key-match(re) Predicate matching key against regex by-value(p?) Predicate on value","title":"Item Predicates"},{"location":"prelude/#mutation","text":"Function Description alter-value(k, v, b) Set b.k to v update-value(k, f, b) Apply f to b.k alter(ks, v, b) Set value at nested key path update(ks, f, b) Apply f at nested key path update-value-or(k, f, d, b) Update or add with default set-value(k, v, b) Set value, adding if absent","title":"Mutation"},{"location":"prelude/#booleans","text":"","title":"Booleans"},{"location":"prelude/#functions","text":"Function Description not(b) Toggle boolean and(l, r) Logical and or(l, r) Logical or","title":"Functions"},{"location":"prelude/#operators","text":"Operator Description !x or \u00acx Not (prefix) l && r or l \u2227 r And l \\|\\| r or l \u2228 r Or","title":"Operators"},{"location":"prelude/#equality-and-comparison","text":"Operator Description l = r Equality l != r Inequality l < r Less than l > r Greater than l <= r Less than or equal l >= r Greater than or equal","title":"Equality and Comparison"},{"location":"prelude/#arithmetic","text":"","title":"Arithmetic"},{"location":"prelude/#operators_1","text":"Operator Description l + r Addition l - r Subtraction l * r Multiplication l / r Division l % r Modulus \u2238 n Unary minus (negate)","title":"Operators"},{"location":"prelude/#functions_1","text":"Function Description inc(x) Increment by 1 dec(x) Decrement by 1 negate(n) Negate number num(s) Parse number from string floor(n) Round down to integer ceiling(n) Round up to integer max(l, r) Maximum of two numbers min(l, r) Minimum of two numbers max-of(l) Maximum in list min-of(l) Minimum in list","title":"Functions"},{"location":"prelude/#predicates_1","text":"Function Description zero?(n) True if n is 0 pos?(n) True if n is positive neg?(n) True if n is negative","title":"Predicates"},{"location":"prelude/#strings","text":"The str namespace contains string functions: Function Description str.of(e) Convert to string str.split(s, re) Split string on regex str.split-on(re, s) Split (pipeline-friendly) str.join(l, s) Join list with separator str.join-on(s, l) Join (pipeline-friendly) str.match(s, re) Match regex, return captures str.match-with(re, s) Match (pipeline-friendly) str.matches(s, re) All matches of regex str.matches-of(re, s) All matches (pipeline-friendly) str.matches?(re, s) True if regex matches full string str.extract(re, s) Extract single capture str.extract-or(re, d, s) Extract with default str.suffix(b, a) Suffix b onto a str.prefix(b, a) Prefix b onto a str.letters(s) List of characters str.len(s) String length str.fmt(x, spec) Printf-style formatting str.to-upper(s) Convert to upper case str.to-lower(s) Convert to lower case","title":"Strings"},{"location":"prelude/#character-constants","text":"The ch namespace provides special characters: ch.n - Newline ch.t - Tab ch.dq - Double quote","title":"Character Constants"},{"location":"prelude/#function-combinators","text":"Function Description identity(v) Return v unchanged const(k) Function that always returns k -> k Operator form of const compose(f, g, x) Apply f to g(x) f \u2218 g Composition: g then f f ; g Composition: f then g l @ r Application: l(r) apply(f, xs) Apply f to args in list flip(f) Swap argument order complement(p?) Invert predicate curry(f) Convert f([x,y]) to f(x,y) uncurry(f) Convert f(x,y) to f([x,y]) juxt(f, g, x) Return [f(x), g(x)] fnil(f, v, x) Replace null with v before applying f","title":"Function Combinators"},{"location":"prelude/#pairs","text":"Function Description pair(k, v) Create pair [k, v] bimap(f, g, pr) Apply f to first, g to second map-first(f, prs) Apply f to first elements map-second(f, prs) Apply f to second elements","title":"Pairs"},{"location":"prelude/#metadata","text":"Function Description with-meta(m, e) Add metadata block m to expression e e // m Operator form of with-meta meta(e) Retrieve metadata from expression merge-meta(m, e) Merge into existing metadata e //<< m Operator form of merge-meta","title":"Metadata"},{"location":"prelude/#assertions","text":"Operator Description e //= v Check if e equals v , return boolean e //=> v Assert e equals v , return e or panic e //=? f Assert e satisfies predicate f e //!? f Assert e does not satisfy f e //! Assert e is true e //!! Assert e is false","title":"Assertions"},{"location":"prelude/#calendar-time","text":"The cal namespace provides date/time functions: Function Description cal.now Current time as fields block cal.epoch Unix epoch as fields block cal.zdt(y,m,d,H,M,S,Z) Create zoned datetime cal.datetime(b) Create from block with defaults cal.parse(s) Parse ISO8601 string cal.format(t) Format as ISO8601 cal.fields(t) Decompose to {y,m,d,H,M,S,Z} block","title":"Calendar / Time"},{"location":"syntax-gotchas/","text":"Syntax Gotchas This document records unintuitive consequences of Eucalypt's syntax design decisions that can lead to subtle bugs or confusion. These are documented here until such time as we can implement a linter or improve error messages to catch them more clearly. Operator Precedence Issues Field Access vs Catenation Problem : The lookup operator ( . ) has higher precedence (90) than catenation (precedence 20), which can lead to unexpected parsing. Gotcha : Writing objects head.id is parsed as objects (head.id) rather than (objects head).id . Example : # This doesn't work as expected: objects: range(0, 5) map({ id: _ }) result: objects head.id # Parsed as: objects (head.id) # Correct syntax requires parentheses: result: (objects head).id # Explicitly groups the field access Error Message : When this occurs, you may see confusing errors like: - cannot return function into case table without default - bad index 18446744073709551615 into environment (under memory pressure) Solution : Always use parentheses to group the expression you want to access fields from: - Use (expression).field instead of expression target.field - Be explicit about precedence when combining catenation with field access Reference : Operator precedences are defined in src/core/metadata.rs:165-186 : - lookup ( . ): precedence 90 - cat (catenation): precedence 20 Anaphora and Function Syntax Lambda Syntax Does Not Exist Problem : Eucalypt does not have lambda expressions like other functional languages. Gotcha : Attempting to write lambda-style syntax will cause syntax errors. Invalid Examples : # These syntaxes DO NOT exist in Eucalypt: map(\\x -> x + 1) # Invalid map(|x| x + 1) # Invalid map(fn(x) => x + 1) # Invalid map(\u03bbx.x + 1) # Invalid Correct Approach : Use anaphora ( _ , _0 , _1 , etc.) or define named functions: # Using anaphora: map(_ + 1) # Using named function: add-one(x): x + 1 map(add-one) # Using block with anaphora for complex expressions: map({ result: _ + 1, doubled: _ * 2 }) Reference : See docs/anaphora-and-lambdas.md for detailed explanation of anaphora usage. Single Quote Identifiers Single Quotes Are Not String Delimiters Problem : Single quotes ( ' ) in Eucalypt are used to create identifiers, not strings. Gotcha : Coming from languages where single quotes delimit strings, developers might expect 'text' to be a string literal. Key Rules : - Single quotes create normal identifiers that can contain any characters - The identifier name is the content between the quotes (quotes are stripped) - This is the only use of single quotes in Eucalypt - String literals use double quotes ( \" ) only Examples : # Single quotes create identifiers (variable names): 'my-file.txt': \"content\" # Creates identifier: my-file.txt home: { '.bashrc': false # Creates identifier: .bashrc '.emacs.d': false # Creates identifier: .emacs.d 'notes.txt': true # Creates identifier: notes.txt } # Access using lookup: z: home.'notes.txt' # Looks up identifier: notes.txt # NOT string literals: 'hello' = 'hello' # Compares two variable references (not strings) \"hello\" = \"hello\" # Compares two string literals (correct) Common Mistake : Using single quotes for strings in test data or evidence files: # Incorrect (this references a variable): stdout: 'default-yaml-stdout-text' # Correct (this would be a string, if that's what you wanted): stdout: \"default-yaml-stdout-text\" Implementation Note : In the Rowan parser, single-quoted identifiers are lexed as SINGLE_QUOTE_IDENTIFIER tokens and the content between quotes is extracted by the ContainsName::name_range() method. Reference : See docs/operators-and-identifiers.md for complete identifier syntax rules. Future Improvements These gotchas highlight areas where the language could benefit from: Better Error Messages : More specific error messages when precedence issues occur Linting Rules : Static analysis to catch common precedence mistakes IDE Support : Syntax highlighting and warnings for ambiguous expressions Documentation : Better examples showing correct precedence usage Contributing When you encounter a new syntax gotcha, please add it to this document with: - A clear description of the problem - Example of incorrect usage - Example of correct usage - The error message(s) that result - Reference to relevant code or documentation","title":"Syntax Gotchas"},{"location":"syntax-gotchas/#syntax-gotchas","text":"This document records unintuitive consequences of Eucalypt's syntax design decisions that can lead to subtle bugs or confusion. These are documented here until such time as we can implement a linter or improve error messages to catch them more clearly.","title":"Syntax Gotchas"},{"location":"syntax-gotchas/#operator-precedence-issues","text":"","title":"Operator Precedence Issues"},{"location":"syntax-gotchas/#field-access-vs-catenation","text":"Problem : The lookup operator ( . ) has higher precedence (90) than catenation (precedence 20), which can lead to unexpected parsing. Gotcha : Writing objects head.id is parsed as objects (head.id) rather than (objects head).id . Example : # This doesn't work as expected: objects: range(0, 5) map({ id: _ }) result: objects head.id # Parsed as: objects (head.id) # Correct syntax requires parentheses: result: (objects head).id # Explicitly groups the field access Error Message : When this occurs, you may see confusing errors like: - cannot return function into case table without default - bad index 18446744073709551615 into environment (under memory pressure) Solution : Always use parentheses to group the expression you want to access fields from: - Use (expression).field instead of expression target.field - Be explicit about precedence when combining catenation with field access Reference : Operator precedences are defined in src/core/metadata.rs:165-186 : - lookup ( . ): precedence 90 - cat (catenation): precedence 20","title":"Field Access vs Catenation"},{"location":"syntax-gotchas/#anaphora-and-function-syntax","text":"","title":"Anaphora and Function Syntax"},{"location":"syntax-gotchas/#lambda-syntax-does-not-exist","text":"Problem : Eucalypt does not have lambda expressions like other functional languages. Gotcha : Attempting to write lambda-style syntax will cause syntax errors. Invalid Examples : # These syntaxes DO NOT exist in Eucalypt: map(\\x -> x + 1) # Invalid map(|x| x + 1) # Invalid map(fn(x) => x + 1) # Invalid map(\u03bbx.x + 1) # Invalid Correct Approach : Use anaphora ( _ , _0 , _1 , etc.) or define named functions: # Using anaphora: map(_ + 1) # Using named function: add-one(x): x + 1 map(add-one) # Using block with anaphora for complex expressions: map({ result: _ + 1, doubled: _ * 2 }) Reference : See docs/anaphora-and-lambdas.md for detailed explanation of anaphora usage.","title":"Lambda Syntax Does Not Exist"},{"location":"syntax-gotchas/#single-quote-identifiers","text":"","title":"Single Quote Identifiers"},{"location":"syntax-gotchas/#single-quotes-are-not-string-delimiters","text":"Problem : Single quotes ( ' ) in Eucalypt are used to create identifiers, not strings. Gotcha : Coming from languages where single quotes delimit strings, developers might expect 'text' to be a string literal. Key Rules : - Single quotes create normal identifiers that can contain any characters - The identifier name is the content between the quotes (quotes are stripped) - This is the only use of single quotes in Eucalypt - String literals use double quotes ( \" ) only Examples : # Single quotes create identifiers (variable names): 'my-file.txt': \"content\" # Creates identifier: my-file.txt home: { '.bashrc': false # Creates identifier: .bashrc '.emacs.d': false # Creates identifier: .emacs.d 'notes.txt': true # Creates identifier: notes.txt } # Access using lookup: z: home.'notes.txt' # Looks up identifier: notes.txt # NOT string literals: 'hello' = 'hello' # Compares two variable references (not strings) \"hello\" = \"hello\" # Compares two string literals (correct) Common Mistake : Using single quotes for strings in test data or evidence files: # Incorrect (this references a variable): stdout: 'default-yaml-stdout-text' # Correct (this would be a string, if that's what you wanted): stdout: \"default-yaml-stdout-text\" Implementation Note : In the Rowan parser, single-quoted identifiers are lexed as SINGLE_QUOTE_IDENTIFIER tokens and the content between quotes is extracted by the ContainsName::name_range() method. Reference : See docs/operators-and-identifiers.md for complete identifier syntax rules.","title":"Single Quotes Are Not String Delimiters"},{"location":"syntax-gotchas/#future-improvements","text":"These gotchas highlight areas where the language could benefit from: Better Error Messages : More specific error messages when precedence issues occur Linting Rules : Static analysis to catch common precedence mistakes IDE Support : Syntax highlighting and warnings for ambiguous expressions Documentation : Better examples showing correct precedence usage","title":"Future Improvements"},{"location":"syntax-gotchas/#contributing","text":"When you encounter a new syntax gotcha, please add it to this document with: - A clear description of the problem - Example of incorrect usage - Example of correct usage - The error message(s) that result - Reference to relevant code or documentation","title":"Contributing"},{"location":"syntax/","text":"The role of Eucalypt's syntax Eucalypt has a native syntax which emphasises the mappings-and-lists nature of its underlying data model but adds enhancements for functions and expressions. Eucalypt is written in .eu files. While eu happily processes YAML inputs with embedded expressions, many features are not yet available in the YAML embedding and the embedded expressions are themselves in Eucalypt syntax, so it is necessary to have an overview of how the syntax works to do anything interesting with Eucalypt. A few aspects are unorthodox and experimental. Overview Eucalypt syntax comes about by the an overlapping of two sub-languages. the block DSL is how you write blocks and their declarations the expression DSL is how you write expressions They are entwined in a fairly typical way: block literals (from the block DSL ) can be used in expressions (from the expression DSL ) and expressions (from the expression DSL ) appear in declarations (from the block DSL ). Comments can be interspersed throughout. Eucalypt only has line level comments. foo: bar # Line comments start with '#' and run till the end of the line Note If you feel you need a block comment, you can use an actual block or a string property within a block and mark it with annotation metadata :suppress to ensure it doesn't appear in output. Eucalypt has two types of names: normal names, which are largely alphanumeric (e.g. f , blah , some-thing! , \u0975 ) and are used to name properties and functions operator names, which are largely symbolic (e.g. &&& , \u2227 , -+-| , \u229a ) and are used to name operators See Operators and Identifiers for more. The block DSL A block is surrounded by curly braces: ... { ... } ...and contains declarations... ... { a: 1 b: 2 c: 3 } ...which may themselves have blocks as values... ... { foo: { bar: { baz: \"hello world\" } } } The top-level block in a file (a unit ) does not have braces: a: 1 b: 2 c: 3 So far all these declarations have been property declarations which contains a name and an expression, separated by a colon. Commas are entirely optional for delimiting declarations. Line endings are not significant. The following is a top-level block of three property declarations . a: 1 b: 2 c: 3 There are other types of declarations. By specifying a parameter list, you get a function declaration : # A function declaration f(x, y): x + y two: f(1, 1) ...and using some brackets and suitable names, you can define operators too, either binary: # A binary operator declaration (x ^|^ y): \"{x} v {y}\" ...or prefix or postfix unary operators: # A prefix operator declaration (\u00ac x): not(x) # A postfix operator declaration (x ******): \"maybe {x}\" Eucalypt should handle unicode gracefully and any unicode characters in the symbol or punctuation classes are fine for operators. To control the precedence and associativity of user defined operators, you need metadata annotations. declaration annotations allow us to specify arbitrary metadata against declarations. These can be used for documentation and similar. To attach an annotation to a declaration, squeeze it between a leading backtick and the declaration itself: ` { doc: \"This is a\"} a: 1 ` { doc: \"This is b\"} b: 2 Some metadata activate special handling, such as the associates and precedence keys you can put on operator declarations: ` { doc: \"`(f \u2218 g)` - return composition of `f` and `g`\" associates: :right precedence: 88 } (f \u2218 g): compose(f,g) Look out for other uses like :target , :suppress , :main . Finally, you can specify metadata at a unit level. If the first item in a unit is an expression, rather than a declaration, it is treated as metadata that is applied to the whole unit. { :doc \"This is just an example unit\" } a: 1 b: 2 c: 3 The expression DSL Everything that can appear to the right of the colon in a declaration is an expression and defined by the expression DSL. Primitives First there are primitives. ...numbers... 123 -123 123.333 ...double quoted strings... \"a string\" ... symbols , prefixed by a colon... :key ...which are currently very like strings, but used in circumstances where their internal structure is generally not significant (i.e. keys in a block's internal representation). Finally, booleans ( true and false ) are pre-defined constants. As is ( null ) which is a value which renders as YAML or JSON's version of null but is not used by Eucalypt itself. Block literals Block literals (in braces, as defined in the block DSL ) are expressions and can be the values of declarations or passed as function arguments or operands in any of the contexts below: foo: { a: 1 b: 2 c: 3} List literals List literals are enclosed in square brackets and contain a comma separated sequence of expressions: list: [1, 2, :a, \"boo\"] Names Then there are names , which refer to the surrounding context. They might refer to properties: x: 22 y: x ...or functions : add-one(x): 1 + x three: add-one(2) ...or operators : (x &&& y): [x, x, x, y] z: \"da\" &&& \"dum\" Calling functions Functions can be applied by suffixing a argument list in parens, with no intervening whitespace : f(x, y): x + y result: f(2, 2) # no whitespace In the special case of applying a single argument, \"catenation\" can be used: add-one(x): 1 + x result: 2 add-one ...which allows succinct expressions of pipelines of operations. In addition, functions are curried so can be partially applied: add(x, y): x + y incremement: add(1) result: 2 increment ...and placeholder underscores (or expression anaphora ) can be used to define simple functions without the song and dance of a function declaration: f: if(tuesday?, (_ * 32 / 12), (99 / _)) result: f(3) In fact, in many cases the underscores can be omitted, leading to a construct very similar to Haskell's sections only even brackets aren't necessary. Note Eucalypt uses its knowledge of the fixity and associativity of each operator to find \"gaps\" and fills them with the unwritten underscores. This is great for simple cases but worth avoiding for complicated expressions. increment: + 1 result: 2 increment (126 /) Both styles of function application together with partial application and sectioning can all be applied together: result: [1, 2, 3] map(+1) filter(odd?) //=> [3] ( //=> is an assertion operator which causes a panic if the left and right hand expressions aren't found to be equal at run time, but returns that value if they are. Note There are no explicit lambda expressions in Eucalypt right now. For simple cases, expression or string anaphora should do the job. For more involved cases, you should use a named function declaration. See Anaphora and Lambdas for more.","title":"Syntax"},{"location":"syntax/#the-role-of-eucalypts-syntax","text":"Eucalypt has a native syntax which emphasises the mappings-and-lists nature of its underlying data model but adds enhancements for functions and expressions. Eucalypt is written in .eu files. While eu happily processes YAML inputs with embedded expressions, many features are not yet available in the YAML embedding and the embedded expressions are themselves in Eucalypt syntax, so it is necessary to have an overview of how the syntax works to do anything interesting with Eucalypt. A few aspects are unorthodox and experimental.","title":"The role of Eucalypt's syntax"},{"location":"syntax/#overview","text":"Eucalypt syntax comes about by the an overlapping of two sub-languages. the block DSL is how you write blocks and their declarations the expression DSL is how you write expressions They are entwined in a fairly typical way: block literals (from the block DSL ) can be used in expressions (from the expression DSL ) and expressions (from the expression DSL ) appear in declarations (from the block DSL ). Comments can be interspersed throughout. Eucalypt only has line level comments. foo: bar # Line comments start with '#' and run till the end of the line Note If you feel you need a block comment, you can use an actual block or a string property within a block and mark it with annotation metadata :suppress to ensure it doesn't appear in output. Eucalypt has two types of names: normal names, which are largely alphanumeric (e.g. f , blah , some-thing! , \u0975 ) and are used to name properties and functions operator names, which are largely symbolic (e.g. &&& , \u2227 , -+-| , \u229a ) and are used to name operators See Operators and Identifiers for more.","title":"Overview"},{"location":"syntax/#the-block-dsl","text":"A block is surrounded by curly braces: ... { ... } ...and contains declarations... ... { a: 1 b: 2 c: 3 } ...which may themselves have blocks as values... ... { foo: { bar: { baz: \"hello world\" } } } The top-level block in a file (a unit ) does not have braces: a: 1 b: 2 c: 3 So far all these declarations have been property declarations which contains a name and an expression, separated by a colon. Commas are entirely optional for delimiting declarations. Line endings are not significant. The following is a top-level block of three property declarations . a: 1 b: 2 c: 3 There are other types of declarations. By specifying a parameter list, you get a function declaration : # A function declaration f(x, y): x + y two: f(1, 1) ...and using some brackets and suitable names, you can define operators too, either binary: # A binary operator declaration (x ^|^ y): \"{x} v {y}\" ...or prefix or postfix unary operators: # A prefix operator declaration (\u00ac x): not(x) # A postfix operator declaration (x ******): \"maybe {x}\" Eucalypt should handle unicode gracefully and any unicode characters in the symbol or punctuation classes are fine for operators. To control the precedence and associativity of user defined operators, you need metadata annotations. declaration annotations allow us to specify arbitrary metadata against declarations. These can be used for documentation and similar. To attach an annotation to a declaration, squeeze it between a leading backtick and the declaration itself: ` { doc: \"This is a\"} a: 1 ` { doc: \"This is b\"} b: 2 Some metadata activate special handling, such as the associates and precedence keys you can put on operator declarations: ` { doc: \"`(f \u2218 g)` - return composition of `f` and `g`\" associates: :right precedence: 88 } (f \u2218 g): compose(f,g) Look out for other uses like :target , :suppress , :main . Finally, you can specify metadata at a unit level. If the first item in a unit is an expression, rather than a declaration, it is treated as metadata that is applied to the whole unit. { :doc \"This is just an example unit\" } a: 1 b: 2 c: 3","title":"The block DSL"},{"location":"syntax/#the-expression-dsl","text":"Everything that can appear to the right of the colon in a declaration is an expression and defined by the expression DSL.","title":"The expression DSL"},{"location":"syntax/#primitives","text":"First there are primitives. ...numbers... 123 -123 123.333 ...double quoted strings... \"a string\" ... symbols , prefixed by a colon... :key ...which are currently very like strings, but used in circumstances where their internal structure is generally not significant (i.e. keys in a block's internal representation). Finally, booleans ( true and false ) are pre-defined constants. As is ( null ) which is a value which renders as YAML or JSON's version of null but is not used by Eucalypt itself.","title":"Primitives"},{"location":"syntax/#block-literals","text":"Block literals (in braces, as defined in the block DSL ) are expressions and can be the values of declarations or passed as function arguments or operands in any of the contexts below: foo: { a: 1 b: 2 c: 3}","title":"Block literals"},{"location":"syntax/#list-literals","text":"List literals are enclosed in square brackets and contain a comma separated sequence of expressions: list: [1, 2, :a, \"boo\"]","title":"List literals"},{"location":"syntax/#names","text":"Then there are names , which refer to the surrounding context. They might refer to properties: x: 22 y: x ...or functions : add-one(x): 1 + x three: add-one(2) ...or operators : (x &&& y): [x, x, x, y] z: \"da\" &&& \"dum\"","title":"Names"},{"location":"syntax/#calling-functions","text":"Functions can be applied by suffixing a argument list in parens, with no intervening whitespace : f(x, y): x + y result: f(2, 2) # no whitespace In the special case of applying a single argument, \"catenation\" can be used: add-one(x): 1 + x result: 2 add-one ...which allows succinct expressions of pipelines of operations. In addition, functions are curried so can be partially applied: add(x, y): x + y incremement: add(1) result: 2 increment ...and placeholder underscores (or expression anaphora ) can be used to define simple functions without the song and dance of a function declaration: f: if(tuesday?, (_ * 32 / 12), (99 / _)) result: f(3) In fact, in many cases the underscores can be omitted, leading to a construct very similar to Haskell's sections only even brackets aren't necessary. Note Eucalypt uses its knowledge of the fixity and associativity of each operator to find \"gaps\" and fills them with the unwritten underscores. This is great for simple cases but worth avoiding for complicated expressions. increment: + 1 result: 2 increment (126 /) Both styles of function application together with partial application and sectioning can all be applied together: result: [1, 2, 3] map(+1) filter(odd?) //=> [3] ( //=> is an assertion operator which causes a panic if the left and right hand expressions aren't found to be equal at run time, but returns that value if they are. Note There are no explicit lambda expressions in Eucalypt right now. For simple cases, expression or string anaphora should do the job. For more involved cases, you should use a named function declaration. See Anaphora and Lambdas for more.","title":"Calling functions"},{"location":"tester/","text":"Test Mode Eucalypt has an experimental built-in test runner which can be used to run tests embedded in eucalypt files. Test mode is invoked by the -T command line flag and: analyses the file to build a test plan consisting of a list of test targets and validations to run executes the test plan and generates an evidence file applies validations against the evidence to generate a results file outputs results and generates an HTML report Simple tests By default eucalypt searches for targets beginning with test- and runs each to render a yaml output. The result is parsed read back in a eucalypt checks for the presence of a RESULT ' key. If it finds it and the value is PASS , the test passes. Anything else is considered a fail. my-add(x, y): x + y ` { target: :test-add } test: { RESULT: (2 + 2 = 4) then(:PASS, :FAIL) } Several test targets can be embedded in one file. Each is run as a separate test. Test files If your intention is not to embed tests in a eucalypt file but instead to write a test as a single file, then you can omit the test targets. Eucalypt will use a main target or run the entire file as usual and then validate the result (looking for a RESULT key, by default). Other formats In test mode, eucalypt processes the test subject to generate output and then parses that back to validate the result. This is to provide for validation of the rendered text and the parsing machinery. By default YAML is generated and parsed back for each test target in the file but other formats can be selected in header metadata. { test-targets: [:yaml, :json] } ` { target: :test-add } add: { RESULT: (2 + 2 = 4) then(:PASS, :FAIL) } ` { target: :test-sub } sub: { RESULT: (2 - 2 = 0) then(:PASS, :FAIL) } Running this file using -T will result in four tests being run, two formats for each of the two targets. Using the default validator, for all formats for which eucalypt provides import and export capability, it shouldn't make any difference which format is used. However, custom validators provide the ability to check the precise text that is rendered. Custom validators When a test runs, the execution generates an evidence block which has the following keys: exit the exit code (0 on success) of the eucalypt execution stdout text as a list of strings stderr text as a list of strings result (the stdout parsed back) stats some statistics from the run","title":"Testing"},{"location":"tester/#test-mode","text":"Eucalypt has an experimental built-in test runner which can be used to run tests embedded in eucalypt files. Test mode is invoked by the -T command line flag and: analyses the file to build a test plan consisting of a list of test targets and validations to run executes the test plan and generates an evidence file applies validations against the evidence to generate a results file outputs results and generates an HTML report","title":"Test Mode"},{"location":"tester/#simple-tests","text":"By default eucalypt searches for targets beginning with test- and runs each to render a yaml output. The result is parsed read back in a eucalypt checks for the presence of a RESULT ' key. If it finds it and the value is PASS , the test passes. Anything else is considered a fail. my-add(x, y): x + y ` { target: :test-add } test: { RESULT: (2 + 2 = 4) then(:PASS, :FAIL) } Several test targets can be embedded in one file. Each is run as a separate test.","title":"Simple tests"},{"location":"tester/#test-files","text":"If your intention is not to embed tests in a eucalypt file but instead to write a test as a single file, then you can omit the test targets. Eucalypt will use a main target or run the entire file as usual and then validate the result (looking for a RESULT key, by default).","title":"Test files"},{"location":"tester/#other-formats","text":"In test mode, eucalypt processes the test subject to generate output and then parses that back to validate the result. This is to provide for validation of the rendered text and the parsing machinery. By default YAML is generated and parsed back for each test target in the file but other formats can be selected in header metadata. { test-targets: [:yaml, :json] } ` { target: :test-add } add: { RESULT: (2 + 2 = 4) then(:PASS, :FAIL) } ` { target: :test-sub } sub: { RESULT: (2 - 2 = 0) then(:PASS, :FAIL) } Running this file using -T will result in four tests being run, two formats for each of the two targets. Using the default validator, for all formats for which eucalypt provides import and export capability, it shouldn't make any difference which format is used. However, custom validators provide the ability to check the precise text that is rendered.","title":"Other formats"},{"location":"tester/#custom-validators","text":"When a test runs, the execution generates an evidence block which has the following keys: exit the exit code (0 on success) of the eucalypt execution stdout text as a list of strings stderr text as a list of strings result (the stdout parsed back) stats some statistics from the run","title":"Custom validators"},{"location":"yaml-embedding/","text":"YAML Embedding Eucalypt can be embedded in YAML files via the following tags: eu eu::suppress eu::fn The YAML embedding is not as capable as the native Eucalypt syntax but it is rich enough to be used for many YAML templating use cases, particularly when combined with the ability to specify several inputs on the command line. Evaluating eucalypt expressions As you would expect, YAML mappings correspond to Eucalypt blocks and bind names just as Eucalypt blocks do and YAML sequences correspond to Eucalypt lists. YAML allow a wide variety of forms of expressing these (block styles and flow styles), to the extent that JSON is valid YAML. Eucalypt expressions can be evaluated using the !eu tag and have access to all the names defined in the YAML unit and any others brought into scope by specifying inputs on the command line. values: x: world y: hello result: !eu \"{values.y} {values.x}!\" ...will render as: values: x: world y: hello result: Hello World! Suppressing rendering Items can be hidden using the eu::suppress tag. This is equivalent to :suppress metadata in the eucalypt syntax. values: !eu::suppress x: world y: hello result: !eu \"{values.y} {values.x}!\" ...will render as: result: Hello World! Defining functions Functions can be defined using eu::fn and supplying an argument list: values: !eu::suppress x: world y: hello greet: !eu::fn (h, w) \"{h} {w}!\" result: !eu values.greet(values.y, values.x) ...will render as: result: Hello World! The escape hatch Larger chunks of eucalypt syntax can be embedded using YAML's support for larger chunks of text, combined with !eu . Using this workaround you can access capabilities of eucalypt that are not yet available in the YAML embedding. (Although operators cannot be made available in YAML blocks because of the way that operator names are bound - see Operators and Identifiers . block: !eu | { x: 99 (l ^^^ r): \"{l} <_> {r}\" f(n): n ^^^ x } result: block.f(99)","title":"YAML Embedding"},{"location":"yaml-embedding/#yaml-embedding","text":"Eucalypt can be embedded in YAML files via the following tags: eu eu::suppress eu::fn The YAML embedding is not as capable as the native Eucalypt syntax but it is rich enough to be used for many YAML templating use cases, particularly when combined with the ability to specify several inputs on the command line.","title":"YAML Embedding"},{"location":"yaml-embedding/#evaluating-eucalypt-expressions","text":"As you would expect, YAML mappings correspond to Eucalypt blocks and bind names just as Eucalypt blocks do and YAML sequences correspond to Eucalypt lists. YAML allow a wide variety of forms of expressing these (block styles and flow styles), to the extent that JSON is valid YAML. Eucalypt expressions can be evaluated using the !eu tag and have access to all the names defined in the YAML unit and any others brought into scope by specifying inputs on the command line. values: x: world y: hello result: !eu \"{values.y} {values.x}!\" ...will render as: values: x: world y: hello result: Hello World!","title":"Evaluating eucalypt expressions"},{"location":"yaml-embedding/#suppressing-rendering","text":"Items can be hidden using the eu::suppress tag. This is equivalent to :suppress metadata in the eucalypt syntax. values: !eu::suppress x: world y: hello result: !eu \"{values.y} {values.x}!\" ...will render as: result: Hello World!","title":"Suppressing rendering"},{"location":"yaml-embedding/#defining-functions","text":"Functions can be defined using eu::fn and supplying an argument list: values: !eu::suppress x: world y: hello greet: !eu::fn (h, w) \"{h} {w}!\" result: !eu values.greet(values.y, values.x) ...will render as: result: Hello World!","title":"Defining functions"},{"location":"yaml-embedding/#the-escape-hatch","text":"Larger chunks of eucalypt syntax can be embedded using YAML's support for larger chunks of text, combined with !eu . Using this workaround you can access capabilities of eucalypt that are not yet available in the YAML embedding. (Although operators cannot be made available in YAML blocks because of the way that operator names are bound - see Operators and Identifiers . block: !eu | { x: 99 (l ^^^ r): \"{l} <_> {r}\" f(n): n ^^^ x } result: block.f(99)","title":"The escape hatch"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/","text":"Lazy Streams and Random Number Facilities Design for lazy streaming infrastructure and random number generation in eucalypt. Overview Eucalypt is purely functional, so exposing randomness as a function breaks referential transparency. Instead, we model randomness as an infinite lazy list of floats \u2014 a value, not an effect. Functions that need randomness consume from the stream and return the unconsumed remainder. This design uses two separate architectures for the two features: Random numbers : Pure functional \u2014 two arithmetic BIFs + prelude recursive lazy list. No mutable state, no runtime changes. Leverages existing thunk memoisation. Streaming file imports : Native::Stream with Rc<RefCell<Box<dyn StreamProducer>>> \u2014 mutable state needed for IO-backed producers. Follows existing Native::Index(Rc<BlockIndex>) pattern for off-heap Rc types. Part 1: Random Numbers API Prelude io.random # Infinite stream, seeded from system entropy (or --seed) random-stream(seed) # Explicit seeding for reproducibility random-int(n, stream) -> {value: 0..n-1, rest: stream} random-choice(list, stream) -> {value: element, rest: stream} shuffle(list, stream) -> {value: shuffled-list, rest: stream} sample(n, list, stream) -> {value: n-elements, rest: stream} CLI eu --seed 12345 myfile.eu # Reproducible io.random Architecture Pure Functional PRNG Random numbers use no mutable state . The PRNG state is a plain integer, threaded through the prelude as a value: random-stream(seed): cons(__PRNG_FLOAT(seed), random-stream(__PRNG_NEXT(seed))) This produces an infinite lazy list because: 1. cons(head, tail) creates a list cell where tail is a thunk 2. The recursive random-stream(__PRNG_NEXT(seed)) is not evaluated until tail is forced 3. STG thunk memoisation ensures each position is computed at most once 4. No special runtime support needed \u2014 standard lazy evaluation handles everything Intrinsics (BIFs) Only two new built-in functions, both pure arithmetic: Intrinsic Args Returns Algorithm __PRNG_NEXT seed (int) Next seed state (int) SplitMix64 state transition __PRNG_FLOAT seed (int) Float in [0,1) SplitMix64 output \u2192 double SplitMix64 was chosen because: - Single u64 state (fits in eucalypt's existing integer representation) - Simple arithmetic (no arrays, no SIMD) - Good statistical quality for non-cryptographic use - Widely used as a seed generator (used by Java's SplittableRandom) --seed CLI Flag Add --seed <integer> to src/driver/options.rs (clap argument) Wire into create_io_pseudoblock() in src/driver/io.rs as __io.RANDOM_SEED If --seed not provided, generate from std::time::SystemTime::now() epoch nanos The seed is a regular integer value, available at runtime as io.RANDOM_SEED Prelude Functions ` \"Infinite lazy stream of random floats in [0,1), seeded by the given integer.\" random-stream(seed): cons(__PRNG_FLOAT(seed), random-stream(__PRNG_NEXT(seed))) ` \"Infinite lazy stream of random floats, seeded from system entropy or --seed flag.\" io.random: random-stream(io.RANDOM_SEED) ` \"Generate a random integer in [0, n) from the stream. Returns {value, rest}.\" random-int(n, stream): { value: floor(stream head * n) rest: stream tail } ` \"Choose a random element from a list. Returns {value, rest}.\" random-choice(list, stream): { r: random-int(list length, stream) value: list nth(r.value) rest: r.rest } ` \"Shuffle a list using Fisher-Yates. Returns {value, rest}.\" shuffle(list, stream): __shuffle-impl(list, list length, stream) ` \"Sample n elements from a list without replacement. Returns {value, rest}.\" sample(n, list, stream): { shuffled: shuffle(list, stream) value: shuffled.value take(n) rest: shuffled.rest } The __shuffle-impl helper implements Fisher-Yates iteratively, consuming one random value per swap. Files Changed Modified: - src/eval/machine/intrinsic.rs \u2014 Register __PRNG_NEXT , __PRNG_FLOAT - src/eval/intrinsics.rs \u2014 BIF implementations (SplitMix64 arithmetic) - src/driver/options.rs \u2014 --seed CLI arg - src/driver/io.rs \u2014 __io.RANDOM_SEED in pseudo-block - lib/prelude.eu \u2014 random-stream, io.random, random-int, random-choice, shuffle, sample - harness/test/077_random.eu (or next available number) \u2014 harness tests Dependency Order eu-cxom : PRNG BIFs ( __PRNG_NEXT , __PRNG_FLOAT ) eu-mbps : CLI --seed flag + __io.RANDOM_SEED (parallel with step 1) eu-qghh : Prelude functions + io.random (depends on steps 1 + 2) eu-64eq : Harness tests (depends on step 3) Part 2: Streaming File Imports API eu jsonl-stream@large.jsonl # Lazy line-by-line JSON processing eu csv-stream@data.csv # Lazy row-by-row CSV processing eu text-stream@log.txt # Lazy line-by-line text processing eu text-stream@- # Lazy streaming from stdin Architecture Why Mutable State is Needed Unlike random numbers, file streaming requires genuine mutable IO state: - A BufReader cursor advances as lines are read - A CSV parser maintains internal state between rows - Stdin is inherently sequential This cannot be modelled purely functionally like PRNG, so we use a different approach. StreamProducer Trait Core abstraction in src/eval/stg/stream.rs : /// A producer that yields values lazily from an IO source trait StreamProducer: Send + Sync { /// Produce next value, or None if exhausted fn next(&mut self) -> Option<RcExpr>; } No fork() method \u2014 streams are inherently single-consumer (you can't rewind a file cursor or stdin). If forking is needed later, it can be added. Native::Stream Variant Add to src/eval/memory/syntax.rs : enum Native { Sym(SymbolId), Str(RefPtr<HeapString>), Num(Number), Zdt(DateTime), Index(Rc<BlockIndex>), Set(RefPtr<HeapSet>), Stream(Rc<RefCell<Box<dyn StreamProducer>>>), // NEW } This follows the existing Rc<BlockIndex> pattern. The Rc lives off-heap, so: - GC evacuation (byte-level memcpy) is safe \u2014 the Rc pointer is copied, refcount is correct - No special GC scanning needed (no heap pointers inside) - RefCell provides interior mutability for advancing the producer Stream Handle Table Producers are stored in a handle table on the VM side: // In the VM or machine context struct StreamTable { handles: HashMap<u32, Rc<RefCell<Box<dyn StreamProducer>>>>, next_id: u32, } Import phase creates a producer, registers it in the table, gets a handle ID The handle ID is embedded in the initial thunk's STG code __STREAM_NEXT(handle) intrinsic looks up the handle to advance the producer __STREAM_NEXT Intrinsic // Intrinsic execution fn execute_stream_next(handle: u32, table: &StreamTable) -> StgValue { let producer = table.get(handle); match producer.borrow_mut().next() { Some(value) => Cons(value, thunk_calling_stream_next(handle)), None => Nil, } } The tail thunk is a standard STG thunk containing a call to __STREAM_NEXT(handle) . When forced, it advances the producer again. Thunk update semantics ensure each position is forced at most once, so the producer advances exactly once per list position. Concrete Producers Producer Source Yields Finite? JsonlProducer BufReader<File> Parsed JSON value per line Yes (EOF) CsvProducer csv::Reader<File> Block with column-name keys per row Yes (EOF) TextProducer BufReader<File> or BufReader<Stdin> String per line Yes (EOF/Ctrl-D) Import System Integration New format specifiers in src/import/mod.rs : jsonl-stream \u2192 JsonlProducer \u2192 register in handle table \u2192 thunk(__STREAM_NEXT(handle)) csv-stream \u2192 CsvProducer \u2192 register in handle table \u2192 thunk(__STREAM_NEXT(handle)) text-stream \u2192 TextProducer \u2192 register in handle table \u2192 thunk(__STREAM_NEXT(handle)) Existing eager imports ( jsonl , csv , text ) remain unchanged. Files Changed New: - src/eval/stg/stream.rs \u2014 StreamProducer trait, concrete producers Modified: - src/eval/memory/syntax.rs \u2014 Native::Stream variant - src/eval/machine/intrinsic.rs \u2014 Register __STREAM_NEXT - src/eval/intrinsics.rs \u2014 __STREAM_NEXT implementation - src/eval/machine/vm.rs \u2014 StreamTable in VM context - src/import/mod.rs \u2014 *-stream format dispatch - src/import/jsonl.rs \u2014 JsonlProducer (lazy variant alongside existing eager) - src/import/csv.rs \u2014 CsvProducer - harness/test/078_streams.eu (or next available number) \u2014 harness tests Dependency Order eu-4yb6 : Stream infrastructure (trait, Native::Stream, handle table) eu-ln9r : __STREAM_NEXT intrinsic + thunk construction (depends on step 1) eu-5rsn : JSONL producer + format dispatch (depends on step 2) eu-fh73 : CSV producer (parallel with step 3, depends on step 2) eu-g6d6 : Text/stdin producer (parallel with steps 3-4, depends on step 2) eu-0g85 : Harness tests (depends on steps 3-5) Error Handling Fail fast \u2014 errors terminate the stream immediately rather than returning inline error values. Consumers rarely handle inline errors properly. Scenario Behaviour Invalid seed Treat as seed.abs() as u64 or hash to u64 Malformed JSONL line Error terminates stream File deleted mid-stream Error on next tail force Stdin EOF Stream returns Nil (finite) CSV parse error Error terminates stream Testing Unit Tests (Rust) SplitMix64 yields deterministic sequence for known seed __PRNG_FLOAT output is in [0, 1) JsonlProducer parses lines correctly CsvProducer yields blocks with correct keys TextProducer yields lines as strings Stream thunk memoisation (same head on repeat access) Harness Tests (eucalypt) Random (077_random.eu or similar): - random-stream(42) head is deterministic - random-stream(42) head is in [0,1) - random-int(10, io.random).value is in [0,9] - shuffle([1,2,3], io.random).value length is 3 - sample(2, [1,2,3,4], io.random).value length is 2 - --seed 42 produces same results across runs - Different seeds produce different sequences Streaming (078_streams.eu or similar): - jsonl-stream@file processes lines lazily, yields correct count - csv-stream@file yields row blocks with correct keys - text-stream@file yields correct line count - Streams terminate with empty list at EOF - Malformed JSONL line produces error Team Assignment Feature Agent Worktree Beads Random Numbers Callum eucalypt-prelude eu-778o (epic), eu-cxom, eu-mbps, eu-qghh, eu-64eq Streaming Imports Niamh eucalypt-vm eu-n50 (epic), eu-4yb6, eu-ln9r, eu-5rsn, eu-fh73, eu-g6d6, eu-0g85 Review (both) Seren eucalypt PR review gate","title":"Lazy Streams and Random Number Facilities"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#lazy-streams-and-random-number-facilities","text":"Design for lazy streaming infrastructure and random number generation in eucalypt.","title":"Lazy Streams and Random Number Facilities"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#overview","text":"Eucalypt is purely functional, so exposing randomness as a function breaks referential transparency. Instead, we model randomness as an infinite lazy list of floats \u2014 a value, not an effect. Functions that need randomness consume from the stream and return the unconsumed remainder. This design uses two separate architectures for the two features: Random numbers : Pure functional \u2014 two arithmetic BIFs + prelude recursive lazy list. No mutable state, no runtime changes. Leverages existing thunk memoisation. Streaming file imports : Native::Stream with Rc<RefCell<Box<dyn StreamProducer>>> \u2014 mutable state needed for IO-backed producers. Follows existing Native::Index(Rc<BlockIndex>) pattern for off-heap Rc types.","title":"Overview"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#part-1-random-numbers","text":"","title":"Part 1: Random Numbers"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#api","text":"","title":"API"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#prelude","text":"io.random # Infinite stream, seeded from system entropy (or --seed) random-stream(seed) # Explicit seeding for reproducibility random-int(n, stream) -> {value: 0..n-1, rest: stream} random-choice(list, stream) -> {value: element, rest: stream} shuffle(list, stream) -> {value: shuffled-list, rest: stream} sample(n, list, stream) -> {value: n-elements, rest: stream}","title":"Prelude"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#cli","text":"eu --seed 12345 myfile.eu # Reproducible io.random","title":"CLI"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#architecture","text":"","title":"Architecture"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#pure-functional-prng","text":"Random numbers use no mutable state . The PRNG state is a plain integer, threaded through the prelude as a value: random-stream(seed): cons(__PRNG_FLOAT(seed), random-stream(__PRNG_NEXT(seed))) This produces an infinite lazy list because: 1. cons(head, tail) creates a list cell where tail is a thunk 2. The recursive random-stream(__PRNG_NEXT(seed)) is not evaluated until tail is forced 3. STG thunk memoisation ensures each position is computed at most once 4. No special runtime support needed \u2014 standard lazy evaluation handles everything","title":"Pure Functional PRNG"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#intrinsics-bifs","text":"Only two new built-in functions, both pure arithmetic: Intrinsic Args Returns Algorithm __PRNG_NEXT seed (int) Next seed state (int) SplitMix64 state transition __PRNG_FLOAT seed (int) Float in [0,1) SplitMix64 output \u2192 double SplitMix64 was chosen because: - Single u64 state (fits in eucalypt's existing integer representation) - Simple arithmetic (no arrays, no SIMD) - Good statistical quality for non-cryptographic use - Widely used as a seed generator (used by Java's SplittableRandom)","title":"Intrinsics (BIFs)"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#-seed-cli-flag","text":"Add --seed <integer> to src/driver/options.rs (clap argument) Wire into create_io_pseudoblock() in src/driver/io.rs as __io.RANDOM_SEED If --seed not provided, generate from std::time::SystemTime::now() epoch nanos The seed is a regular integer value, available at runtime as io.RANDOM_SEED","title":"--seed CLI Flag"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#prelude-functions","text":"` \"Infinite lazy stream of random floats in [0,1), seeded by the given integer.\" random-stream(seed): cons(__PRNG_FLOAT(seed), random-stream(__PRNG_NEXT(seed))) ` \"Infinite lazy stream of random floats, seeded from system entropy or --seed flag.\" io.random: random-stream(io.RANDOM_SEED) ` \"Generate a random integer in [0, n) from the stream. Returns {value, rest}.\" random-int(n, stream): { value: floor(stream head * n) rest: stream tail } ` \"Choose a random element from a list. Returns {value, rest}.\" random-choice(list, stream): { r: random-int(list length, stream) value: list nth(r.value) rest: r.rest } ` \"Shuffle a list using Fisher-Yates. Returns {value, rest}.\" shuffle(list, stream): __shuffle-impl(list, list length, stream) ` \"Sample n elements from a list without replacement. Returns {value, rest}.\" sample(n, list, stream): { shuffled: shuffle(list, stream) value: shuffled.value take(n) rest: shuffled.rest } The __shuffle-impl helper implements Fisher-Yates iteratively, consuming one random value per swap.","title":"Prelude Functions"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#files-changed","text":"Modified: - src/eval/machine/intrinsic.rs \u2014 Register __PRNG_NEXT , __PRNG_FLOAT - src/eval/intrinsics.rs \u2014 BIF implementations (SplitMix64 arithmetic) - src/driver/options.rs \u2014 --seed CLI arg - src/driver/io.rs \u2014 __io.RANDOM_SEED in pseudo-block - lib/prelude.eu \u2014 random-stream, io.random, random-int, random-choice, shuffle, sample - harness/test/077_random.eu (or next available number) \u2014 harness tests","title":"Files Changed"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#dependency-order","text":"eu-cxom : PRNG BIFs ( __PRNG_NEXT , __PRNG_FLOAT ) eu-mbps : CLI --seed flag + __io.RANDOM_SEED (parallel with step 1) eu-qghh : Prelude functions + io.random (depends on steps 1 + 2) eu-64eq : Harness tests (depends on step 3)","title":"Dependency Order"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#part-2-streaming-file-imports","text":"","title":"Part 2: Streaming File Imports"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#api_1","text":"eu jsonl-stream@large.jsonl # Lazy line-by-line JSON processing eu csv-stream@data.csv # Lazy row-by-row CSV processing eu text-stream@log.txt # Lazy line-by-line text processing eu text-stream@- # Lazy streaming from stdin","title":"API"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#architecture_1","text":"","title":"Architecture"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#why-mutable-state-is-needed","text":"Unlike random numbers, file streaming requires genuine mutable IO state: - A BufReader cursor advances as lines are read - A CSV parser maintains internal state between rows - Stdin is inherently sequential This cannot be modelled purely functionally like PRNG, so we use a different approach.","title":"Why Mutable State is Needed"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#streamproducer-trait","text":"Core abstraction in src/eval/stg/stream.rs : /// A producer that yields values lazily from an IO source trait StreamProducer: Send + Sync { /// Produce next value, or None if exhausted fn next(&mut self) -> Option<RcExpr>; } No fork() method \u2014 streams are inherently single-consumer (you can't rewind a file cursor or stdin). If forking is needed later, it can be added.","title":"StreamProducer Trait"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#nativestream-variant","text":"Add to src/eval/memory/syntax.rs : enum Native { Sym(SymbolId), Str(RefPtr<HeapString>), Num(Number), Zdt(DateTime), Index(Rc<BlockIndex>), Set(RefPtr<HeapSet>), Stream(Rc<RefCell<Box<dyn StreamProducer>>>), // NEW } This follows the existing Rc<BlockIndex> pattern. The Rc lives off-heap, so: - GC evacuation (byte-level memcpy) is safe \u2014 the Rc pointer is copied, refcount is correct - No special GC scanning needed (no heap pointers inside) - RefCell provides interior mutability for advancing the producer","title":"Native::Stream Variant"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#stream-handle-table","text":"Producers are stored in a handle table on the VM side: // In the VM or machine context struct StreamTable { handles: HashMap<u32, Rc<RefCell<Box<dyn StreamProducer>>>>, next_id: u32, } Import phase creates a producer, registers it in the table, gets a handle ID The handle ID is embedded in the initial thunk's STG code __STREAM_NEXT(handle) intrinsic looks up the handle to advance the producer","title":"Stream Handle Table"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#__stream_next-intrinsic","text":"// Intrinsic execution fn execute_stream_next(handle: u32, table: &StreamTable) -> StgValue { let producer = table.get(handle); match producer.borrow_mut().next() { Some(value) => Cons(value, thunk_calling_stream_next(handle)), None => Nil, } } The tail thunk is a standard STG thunk containing a call to __STREAM_NEXT(handle) . When forced, it advances the producer again. Thunk update semantics ensure each position is forced at most once, so the producer advances exactly once per list position.","title":"__STREAM_NEXT Intrinsic"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#concrete-producers","text":"Producer Source Yields Finite? JsonlProducer BufReader<File> Parsed JSON value per line Yes (EOF) CsvProducer csv::Reader<File> Block with column-name keys per row Yes (EOF) TextProducer BufReader<File> or BufReader<Stdin> String per line Yes (EOF/Ctrl-D)","title":"Concrete Producers"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#import-system-integration","text":"New format specifiers in src/import/mod.rs : jsonl-stream \u2192 JsonlProducer \u2192 register in handle table \u2192 thunk(__STREAM_NEXT(handle)) csv-stream \u2192 CsvProducer \u2192 register in handle table \u2192 thunk(__STREAM_NEXT(handle)) text-stream \u2192 TextProducer \u2192 register in handle table \u2192 thunk(__STREAM_NEXT(handle)) Existing eager imports ( jsonl , csv , text ) remain unchanged.","title":"Import System Integration"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#files-changed_1","text":"New: - src/eval/stg/stream.rs \u2014 StreamProducer trait, concrete producers Modified: - src/eval/memory/syntax.rs \u2014 Native::Stream variant - src/eval/machine/intrinsic.rs \u2014 Register __STREAM_NEXT - src/eval/intrinsics.rs \u2014 __STREAM_NEXT implementation - src/eval/machine/vm.rs \u2014 StreamTable in VM context - src/import/mod.rs \u2014 *-stream format dispatch - src/import/jsonl.rs \u2014 JsonlProducer (lazy variant alongside existing eager) - src/import/csv.rs \u2014 CsvProducer - harness/test/078_streams.eu (or next available number) \u2014 harness tests","title":"Files Changed"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#dependency-order_1","text":"eu-4yb6 : Stream infrastructure (trait, Native::Stream, handle table) eu-ln9r : __STREAM_NEXT intrinsic + thunk construction (depends on step 1) eu-5rsn : JSONL producer + format dispatch (depends on step 2) eu-fh73 : CSV producer (parallel with step 3, depends on step 2) eu-g6d6 : Text/stdin producer (parallel with steps 3-4, depends on step 2) eu-0g85 : Harness tests (depends on steps 3-5)","title":"Dependency Order"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#error-handling","text":"Fail fast \u2014 errors terminate the stream immediately rather than returning inline error values. Consumers rarely handle inline errors properly. Scenario Behaviour Invalid seed Treat as seed.abs() as u64 or hash to u64 Malformed JSONL line Error terminates stream File deleted mid-stream Error on next tail force Stdin EOF Stream returns Nil (finite) CSV parse error Error terminates stream","title":"Error Handling"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#testing","text":"","title":"Testing"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#unit-tests-rust","text":"SplitMix64 yields deterministic sequence for known seed __PRNG_FLOAT output is in [0, 1) JsonlProducer parses lines correctly CsvProducer yields blocks with correct keys TextProducer yields lines as strings Stream thunk memoisation (same head on repeat access)","title":"Unit Tests (Rust)"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#harness-tests-eucalypt","text":"Random (077_random.eu or similar): - random-stream(42) head is deterministic - random-stream(42) head is in [0,1) - random-int(10, io.random).value is in [0,9] - shuffle([1,2,3], io.random).value length is 3 - sample(2, [1,2,3,4], io.random).value length is 2 - --seed 42 produces same results across runs - Different seeds produce different sequences Streaming (078_streams.eu or similar): - jsonl-stream@file processes lines lazily, yields correct count - csv-stream@file yields row blocks with correct keys - text-stream@file yields correct line count - Streams terminate with empty list at EOF - Malformed JSONL line produces error","title":"Harness Tests (eucalypt)"},{"location":"plans/2026-02-01-lazy-streams-and-random-design/#team-assignment","text":"Feature Agent Worktree Beads Random Numbers Callum eucalypt-prelude eu-778o (epic), eu-cxom, eu-mbps, eu-qghh, eu-64eq Streaming Imports Niamh eucalypt-vm eu-n50 (epic), eu-4yb6, eu-ln9r, eu-5rsn, eu-fh73, eu-g6d6, eu-0g85 Review (both) Seren eucalypt PR review gate","title":"Team Assignment"},{"location":"plans/2026-02-03-gc-immix-design/","text":"GC Immix Completion Design Date: 2026-02-03 Overview and Goals Complete the Immix garbage collector implementation with reliable validation at every step. Three phases of work, strictly ordered: Benchmarking infrastructure -- Statistics enhancements, --statistics-file , Criterion GC benchmarks, end-to-end bench script, agent workflow documentation Lazy sweeping -- Defer sweep work to allocation time, reducing stop-the-world pause duration Opportunistic evacuation -- Object moving/compaction for fragmented blocks, completing the Immix algorithm Each phase produces measurable before/after evidence using the infrastructure from phase 1. No phase begins until the previous one is validated. Success criteria An AI agent can follow the documented workflow to make a GC change and produce reliable before/after comparison Lazy sweep measurably reduces total GC pause time in the bench suite Evacuation measurably reduces fragmentation in the stress programs No correctness regressions (all existing tests pass throughout) Phase 1: Statistics and Benchmarking Infrastructure 1a: Statistics struct enhancements File : src/driver/statistics.rs Add the following fields to Statistics : collections_count: u64 -- how many GC cycles ran peak_heap_blocks: usize -- high-water mark of allocated blocks total_mark_time: Duration -- aggregate mark phase time total_sweep_time: Duration -- aggregate sweep phase time These get populated from the existing Clock (which already tracks CollectorMark and CollectorSweep durations) and from a new counter in collect() . The collection count needs threading through -- most likely a counter on Heap that collect() increments, then read out at the end alongside HeapStats . Also fix the duplicate Machine Max Stack line in the Display impl. 1b: --statistics-file <path> CLI flag Files : src/driver/options.rs , src/driver/eval.rs New option in RunArgs / EucalyptOptions . When set, writes a JSON file at program exit containing all Statistics fields plus the timing breakdown. Structure: { \"machine_ticks\": 42, \"machine_allocs\": 5, \"machine_max_stack\": 2, \"collections_count\": 1, \"blocks_allocated\": 5, \"peak_heap_blocks\": 5, \"blocks_used\": 2, \"blocks_recycled\": 1, \"timings\": { \"parse\": 0.021459, \"stg-execute\": 0.000831, \"VM-CollectorMark\": 0.000213, \"VM-CollectorSweep\": 0.000002, \"VM-Total\": 0.000831 } } Uses serde_json (already a dependency) for serialisation. The existing -S text output remains unchanged. 1c: GC stress programs Three new .eu files in harness/test/bench/ : 006_short_lived.eu -- Repeatedly builds and discards large lists using foldl over repeat(...) take(N) with suppressed intermediate results. Forces frequent collection of short-lived objects. 007_long_lived_graph.eu -- Builds a large persistent block structure, then allocates throwaway lists around it. Tests mark phase performance with a large live set. 008_fragmentation.eu -- Interleaves retained and discarded allocations to create holes in blocks. This is the scenario where evacuation should show improvement -- before evacuation it fragments, after evacuation it compacts. These use existing eucalypt language features only (lists, blocks, folds, suppress metadata). Each should run in under a second but allocate enough to trigger multiple collections with a reasonable --heap-limit-mib (e.g. 2-4 MiB). 1d: Criterion GC benchmarks File : benches/gc.rs (new) Benchmarks isolating GC operations, following the existing pattern in benches/alloc.rs : alloc_then_collect -- Allocate N objects, trigger collection, measure mark + sweep separately via Clock collect_with_survivors -- Allocate N objects, keep M% alive as roots, collect. Vary M to measure mark phase scaling with live set size. alloc_into_recycled -- After collection leaves holes, measure allocation throughput into recycled blocks vs fresh blocks. Around 200-300 lines total. Each benchmark constructs a Heap , gets a MutatorHeapView , and operates directly on the GC internals. 1e: Bench script File : scripts/gc-bench.sh Two modes: ./scripts/gc-bench.sh baseline # Run suite, save to gc-bench-baseline.json ./scripts/gc-bench.sh compare # Run suite, compare against baseline The script: Builds release ( cargo build --release ) Runs each bench program 5 times with --statistics-file and --heap-limit-mib 4 Takes the median of each timing metric In baseline mode: writes medians to gc-bench-baseline.json In compare mode: runs again, computes percentage change per metric, flags regressions beyond a threshold (>5% slower) Bench programs included: harness/test/bench/001_naive_fib.eu harness/test/bench/002_thunk_updates.eu harness/test/bench/003_smarter_fib.eu harness/test/bench/004_generations.eu harness/test/bench/005_drop_cons.eu harness/test/bench/006_short_lived.eu (new) harness/test/bench/007_long_lived_graph.eu (new) harness/test/bench/008_fragmentation.eu (new) Example output: 004_generations: VM-CollectorMark 0.0034s -> 0.0031s (-8.8%) OK 004_generations: VM-CollectorSweep 0.0012s -> 0.0003s (-75.0%) OK 006_short_lived: VM-Total 0.0089s -> 0.0091s (+2.2%) OK 008_fragmentation: blocks_recycled 12 -> 18 (+50.0%) OK 1f: Agent workflow documentation File : docs/gc-benchmarking.md Step-by-step procedure for any GC modification: Ensure clean working tree Run ./scripts/gc-bench.sh baseline Make the GC change Run cargo test (correctness gate) Run ./scripts/gc-bench.sh compare Review output -- no metric should regress beyond threshold If investigating: run cargo bench -- gc for Criterion detail Include summary of benchmark results in commit message Phase 2: Lazy Sweeping 2a: What changes Currently in collect() ( src/eval/memory/collect.rs:160-163 ), the sweep phase runs eagerly -- all blocks are scanned for holes and moved to the recycled list immediately after marking. The full sweep cost is paid during the stop-the-world pause. With lazy sweeping, the collector only marks. Sweeping happens incrementally at allocation time -- when the allocator needs a new block, it sweeps the next unswept block to find holes, then either uses it (if recyclable) or moves it to rest (if fully occupied). 2b: Heap state changes File : src/eval/memory/heap.rs The block categories change from: head, overflow, rest, recycled, lobs, recycled_lobs to: head, overflow, unswept, rest, recycled, lobs, recycled_lobs After collection, blocks that were in rest move to unswept instead of being eagerly swept. The existing rest list retains its meaning (blocks that have been swept but are not recyclable, i.e. fully occupied). 2c: Collection changes File : src/eval/memory/collect.rs Remove the sweep call from collect() After marking, move all non-head/overflow blocks to unswept The Clock still records CollectorSweep but the duration during collection drops to near zero (just the list move) 2d: Allocation changes File : src/eval/memory/heap.rs (allocation paths) When the allocator needs a new block (head block exhausted, no recycled blocks available): Pop a block from unswept Sweep it (scan line map, identify holes) If it has usable holes: use it as the new head/recycled block If it is fully live: move to rest , try next unswept block If unswept is empty: allocate a fresh block from the OS The actual per-block sweep logic ( recycle() in bump.rs ) is unchanged -- just called lazily instead of eagerly. 2e: Sweep timing Track total sweep time for benchmarking by switching the Clock to CollectorSweep around each individual block sweep at allocation time. The aggregate still appears in statistics output. 2f: What doesn't change Mark phase -- identical Object headers -- unchanged GcScannable trait -- unchanged Line map format -- unchanged Per-block sweep logic ( recycle() in bump.rs ) -- unchanged Emergency collection -- still triggers a full mark; unswept blocks get lazily swept on demand 2g: Risk and validation Risks: Allocation latency becomes less predictable (some allocations trigger a sweep). This is the standard Immix trade-off -- shorter GC pauses at the cost of occasional slower allocations. Must ensure unswept blocks are not accidentally used as fresh blocks before sweeping. Validation: All existing tests pass ( cargo test ) Benchmark comparison shows reduced VM-CollectorSweep time during collection pause VM-Total should remain similar or improve (sweep work is the same total, just distributed) The stress programs ( 006_short_lived and 004_generations ) should show lower peak pause times Phase 3: Opportunistic Evacuation 3a: What this adds The core Immix innovation: during collection, instead of always marking objects in place, the collector can move objects out of fragmented blocks into fresh blocks, compacting the heap. This is opportunistic -- it only evacuates blocks where fragmentation is bad enough to justify the copy cost. 3b: Deciding when to evacuate Before each collection, the collector analyses heap fragmentation using the existing FragmentationAnalysis struct (already in heap.rs but currently unused): Candidate blocks : Blocks where occupancy is below a threshold (e.g. <50% of lines marked). These are fragmented enough that evacuating their live objects to a fresh block saves space. Available space : Need at least one fresh block to evacuate into. If the heap is completely full, fall back to mark-in-place. Decision : If there are candidate blocks and space to evacuate, mark that collection cycle as an \"evacuating\" cycle. Otherwise, mark-in-place as today. Uses fragmentation data from the previous collection's line maps -- we know which blocks had poor occupancy last time. 3c: Evacuation during marking During an evacuating collection, the mark phase changes behaviour for objects in candidate blocks: When scanning reaches an object in a candidate block, instead of marking it in place: Copy the object to the evacuation target block (bump-allocate into it) Set the forwarding pointer in the old object's header (the field already exists in AllocHeader but is currently unused) Mark the new copy When scanning reaches an object that has already been forwarded: Follow the forwarding pointer to the new location Use the new location for any references being updated 3d: Reference updating As the mark phase traces references, if a reference points to a forwarded object, update the reference in place to the new location. This means the scan must be able to write to the referring object, not just read it. The GcScannable trait needs extending. Currently scan() pushes discovered pointers but does not support rewriting them. Add an update_refs() method (or modify scan() to support reference rewriting) that follows the same traversal but updates forwarded pointers. The CollectorHeapView already has mutable access to the heap, so the mutation is possible within the existing safety model. This approach (update during scan) follows the Immix paper and avoids an extra full-heap traversal pass. 3e: What changes AllocHeader ( src/eval/memory/header.rs ) -- the forwarded_to field gets used. Add is_forwarded() / set_forwarded() methods. CollectorHeapView ( src/eval/memory/collect.rs ) -- add evacuate() method that copies an object and sets forwarding pointer. GcScannable ( src/eval/memory/collect.rs ) -- extend with scan_and_update() or modify scan() to support reference rewriting. Needs careful design to maintain the lifetime safety model. collect() ( src/eval/memory/collect.rs ) -- takes a strategy enum indicating mark-in-place vs evacuating. Evacuation cycles use the modified scan path. Heap ( src/eval/memory/heap.rs ) -- add evacuation target block management. Track which blocks are candidates. Lazy sweep interaction -- evacuated candidate blocks become fully dead after evacuation (all live objects moved out), so they can be returned to the free list without sweeping. 3f: What doesn't change Allocation paths (bump allocation unchanged) Non-evacuating collections (mark-in-place path identical to phase 2) Object layout and size classes Large objects (LOBs are never evacuated -- too expensive to copy) 3g: Risk and validation Risks: Reference updating is the most complex part. A missed pointer update is a use-after-move bug -- hard to debug, potentially silent corruption. The GcScannable trait change touches every heap object type. Needs careful review. Forwarding pointer chasing adds overhead to the mark phase even when it does not find forwarded objects. Validation: All existing tests pass ( cargo test ) New unit tests specifically for evacuation: create fragmented heap, evacuate, verify objects accessible at new locations 008_fragmentation.eu stress test should show improved blocks_recycled and reduced blocks_used after evacuation Non-fragmented workloads should show no regression (evacuation not triggered) Benchmark comparison confirms mark phase is only slightly slower (forwarding check overhead) and fragmentation recovery is measurably better Beads Existing beads to close on completion: eu-2ij -- GC: Implement lazy sweeping optimisation (phase 2) eu-5si -- GC: Implement full reference updating system (phase 3) eu-w68 -- Memory management unsafe code lacks safety documentation (address as part of phase 3, which adds new unsafe code for evacuation) New beads to create for phase 1 (benchmarking infrastructure) as no existing bead covers this work.","title":"GC Immix Completion Design"},{"location":"plans/2026-02-03-gc-immix-design/#gc-immix-completion-design","text":"Date: 2026-02-03","title":"GC Immix Completion Design"},{"location":"plans/2026-02-03-gc-immix-design/#overview-and-goals","text":"Complete the Immix garbage collector implementation with reliable validation at every step. Three phases of work, strictly ordered: Benchmarking infrastructure -- Statistics enhancements, --statistics-file , Criterion GC benchmarks, end-to-end bench script, agent workflow documentation Lazy sweeping -- Defer sweep work to allocation time, reducing stop-the-world pause duration Opportunistic evacuation -- Object moving/compaction for fragmented blocks, completing the Immix algorithm Each phase produces measurable before/after evidence using the infrastructure from phase 1. No phase begins until the previous one is validated.","title":"Overview and Goals"},{"location":"plans/2026-02-03-gc-immix-design/#success-criteria","text":"An AI agent can follow the documented workflow to make a GC change and produce reliable before/after comparison Lazy sweep measurably reduces total GC pause time in the bench suite Evacuation measurably reduces fragmentation in the stress programs No correctness regressions (all existing tests pass throughout)","title":"Success criteria"},{"location":"plans/2026-02-03-gc-immix-design/#phase-1-statistics-and-benchmarking-infrastructure","text":"","title":"Phase 1: Statistics and Benchmarking Infrastructure"},{"location":"plans/2026-02-03-gc-immix-design/#1a-statistics-struct-enhancements","text":"File : src/driver/statistics.rs Add the following fields to Statistics : collections_count: u64 -- how many GC cycles ran peak_heap_blocks: usize -- high-water mark of allocated blocks total_mark_time: Duration -- aggregate mark phase time total_sweep_time: Duration -- aggregate sweep phase time These get populated from the existing Clock (which already tracks CollectorMark and CollectorSweep durations) and from a new counter in collect() . The collection count needs threading through -- most likely a counter on Heap that collect() increments, then read out at the end alongside HeapStats . Also fix the duplicate Machine Max Stack line in the Display impl.","title":"1a: Statistics struct enhancements"},{"location":"plans/2026-02-03-gc-immix-design/#1b-statistics-file-path-cli-flag","text":"Files : src/driver/options.rs , src/driver/eval.rs New option in RunArgs / EucalyptOptions . When set, writes a JSON file at program exit containing all Statistics fields plus the timing breakdown. Structure: { \"machine_ticks\": 42, \"machine_allocs\": 5, \"machine_max_stack\": 2, \"collections_count\": 1, \"blocks_allocated\": 5, \"peak_heap_blocks\": 5, \"blocks_used\": 2, \"blocks_recycled\": 1, \"timings\": { \"parse\": 0.021459, \"stg-execute\": 0.000831, \"VM-CollectorMark\": 0.000213, \"VM-CollectorSweep\": 0.000002, \"VM-Total\": 0.000831 } } Uses serde_json (already a dependency) for serialisation. The existing -S text output remains unchanged.","title":"1b: --statistics-file &lt;path&gt; CLI flag"},{"location":"plans/2026-02-03-gc-immix-design/#1c-gc-stress-programs","text":"Three new .eu files in harness/test/bench/ : 006_short_lived.eu -- Repeatedly builds and discards large lists using foldl over repeat(...) take(N) with suppressed intermediate results. Forces frequent collection of short-lived objects. 007_long_lived_graph.eu -- Builds a large persistent block structure, then allocates throwaway lists around it. Tests mark phase performance with a large live set. 008_fragmentation.eu -- Interleaves retained and discarded allocations to create holes in blocks. This is the scenario where evacuation should show improvement -- before evacuation it fragments, after evacuation it compacts. These use existing eucalypt language features only (lists, blocks, folds, suppress metadata). Each should run in under a second but allocate enough to trigger multiple collections with a reasonable --heap-limit-mib (e.g. 2-4 MiB).","title":"1c: GC stress programs"},{"location":"plans/2026-02-03-gc-immix-design/#1d-criterion-gc-benchmarks","text":"File : benches/gc.rs (new) Benchmarks isolating GC operations, following the existing pattern in benches/alloc.rs : alloc_then_collect -- Allocate N objects, trigger collection, measure mark + sweep separately via Clock collect_with_survivors -- Allocate N objects, keep M% alive as roots, collect. Vary M to measure mark phase scaling with live set size. alloc_into_recycled -- After collection leaves holes, measure allocation throughput into recycled blocks vs fresh blocks. Around 200-300 lines total. Each benchmark constructs a Heap , gets a MutatorHeapView , and operates directly on the GC internals.","title":"1d: Criterion GC benchmarks"},{"location":"plans/2026-02-03-gc-immix-design/#1e-bench-script","text":"File : scripts/gc-bench.sh Two modes: ./scripts/gc-bench.sh baseline # Run suite, save to gc-bench-baseline.json ./scripts/gc-bench.sh compare # Run suite, compare against baseline The script: Builds release ( cargo build --release ) Runs each bench program 5 times with --statistics-file and --heap-limit-mib 4 Takes the median of each timing metric In baseline mode: writes medians to gc-bench-baseline.json In compare mode: runs again, computes percentage change per metric, flags regressions beyond a threshold (>5% slower) Bench programs included: harness/test/bench/001_naive_fib.eu harness/test/bench/002_thunk_updates.eu harness/test/bench/003_smarter_fib.eu harness/test/bench/004_generations.eu harness/test/bench/005_drop_cons.eu harness/test/bench/006_short_lived.eu (new) harness/test/bench/007_long_lived_graph.eu (new) harness/test/bench/008_fragmentation.eu (new) Example output: 004_generations: VM-CollectorMark 0.0034s -> 0.0031s (-8.8%) OK 004_generations: VM-CollectorSweep 0.0012s -> 0.0003s (-75.0%) OK 006_short_lived: VM-Total 0.0089s -> 0.0091s (+2.2%) OK 008_fragmentation: blocks_recycled 12 -> 18 (+50.0%) OK","title":"1e: Bench script"},{"location":"plans/2026-02-03-gc-immix-design/#1f-agent-workflow-documentation","text":"File : docs/gc-benchmarking.md Step-by-step procedure for any GC modification: Ensure clean working tree Run ./scripts/gc-bench.sh baseline Make the GC change Run cargo test (correctness gate) Run ./scripts/gc-bench.sh compare Review output -- no metric should regress beyond threshold If investigating: run cargo bench -- gc for Criterion detail Include summary of benchmark results in commit message","title":"1f: Agent workflow documentation"},{"location":"plans/2026-02-03-gc-immix-design/#phase-2-lazy-sweeping","text":"","title":"Phase 2: Lazy Sweeping"},{"location":"plans/2026-02-03-gc-immix-design/#2a-what-changes","text":"Currently in collect() ( src/eval/memory/collect.rs:160-163 ), the sweep phase runs eagerly -- all blocks are scanned for holes and moved to the recycled list immediately after marking. The full sweep cost is paid during the stop-the-world pause. With lazy sweeping, the collector only marks. Sweeping happens incrementally at allocation time -- when the allocator needs a new block, it sweeps the next unswept block to find holes, then either uses it (if recyclable) or moves it to rest (if fully occupied).","title":"2a: What changes"},{"location":"plans/2026-02-03-gc-immix-design/#2b-heap-state-changes","text":"File : src/eval/memory/heap.rs The block categories change from: head, overflow, rest, recycled, lobs, recycled_lobs to: head, overflow, unswept, rest, recycled, lobs, recycled_lobs After collection, blocks that were in rest move to unswept instead of being eagerly swept. The existing rest list retains its meaning (blocks that have been swept but are not recyclable, i.e. fully occupied).","title":"2b: Heap state changes"},{"location":"plans/2026-02-03-gc-immix-design/#2c-collection-changes","text":"File : src/eval/memory/collect.rs Remove the sweep call from collect() After marking, move all non-head/overflow blocks to unswept The Clock still records CollectorSweep but the duration during collection drops to near zero (just the list move)","title":"2c: Collection changes"},{"location":"plans/2026-02-03-gc-immix-design/#2d-allocation-changes","text":"File : src/eval/memory/heap.rs (allocation paths) When the allocator needs a new block (head block exhausted, no recycled blocks available): Pop a block from unswept Sweep it (scan line map, identify holes) If it has usable holes: use it as the new head/recycled block If it is fully live: move to rest , try next unswept block If unswept is empty: allocate a fresh block from the OS The actual per-block sweep logic ( recycle() in bump.rs ) is unchanged -- just called lazily instead of eagerly.","title":"2d: Allocation changes"},{"location":"plans/2026-02-03-gc-immix-design/#2e-sweep-timing","text":"Track total sweep time for benchmarking by switching the Clock to CollectorSweep around each individual block sweep at allocation time. The aggregate still appears in statistics output.","title":"2e: Sweep timing"},{"location":"plans/2026-02-03-gc-immix-design/#2f-what-doesnt-change","text":"Mark phase -- identical Object headers -- unchanged GcScannable trait -- unchanged Line map format -- unchanged Per-block sweep logic ( recycle() in bump.rs ) -- unchanged Emergency collection -- still triggers a full mark; unswept blocks get lazily swept on demand","title":"2f: What doesn't change"},{"location":"plans/2026-02-03-gc-immix-design/#2g-risk-and-validation","text":"Risks: Allocation latency becomes less predictable (some allocations trigger a sweep). This is the standard Immix trade-off -- shorter GC pauses at the cost of occasional slower allocations. Must ensure unswept blocks are not accidentally used as fresh blocks before sweeping. Validation: All existing tests pass ( cargo test ) Benchmark comparison shows reduced VM-CollectorSweep time during collection pause VM-Total should remain similar or improve (sweep work is the same total, just distributed) The stress programs ( 006_short_lived and 004_generations ) should show lower peak pause times","title":"2g: Risk and validation"},{"location":"plans/2026-02-03-gc-immix-design/#phase-3-opportunistic-evacuation","text":"","title":"Phase 3: Opportunistic Evacuation"},{"location":"plans/2026-02-03-gc-immix-design/#3a-what-this-adds","text":"The core Immix innovation: during collection, instead of always marking objects in place, the collector can move objects out of fragmented blocks into fresh blocks, compacting the heap. This is opportunistic -- it only evacuates blocks where fragmentation is bad enough to justify the copy cost.","title":"3a: What this adds"},{"location":"plans/2026-02-03-gc-immix-design/#3b-deciding-when-to-evacuate","text":"Before each collection, the collector analyses heap fragmentation using the existing FragmentationAnalysis struct (already in heap.rs but currently unused): Candidate blocks : Blocks where occupancy is below a threshold (e.g. <50% of lines marked). These are fragmented enough that evacuating their live objects to a fresh block saves space. Available space : Need at least one fresh block to evacuate into. If the heap is completely full, fall back to mark-in-place. Decision : If there are candidate blocks and space to evacuate, mark that collection cycle as an \"evacuating\" cycle. Otherwise, mark-in-place as today. Uses fragmentation data from the previous collection's line maps -- we know which blocks had poor occupancy last time.","title":"3b: Deciding when to evacuate"},{"location":"plans/2026-02-03-gc-immix-design/#3c-evacuation-during-marking","text":"During an evacuating collection, the mark phase changes behaviour for objects in candidate blocks: When scanning reaches an object in a candidate block, instead of marking it in place: Copy the object to the evacuation target block (bump-allocate into it) Set the forwarding pointer in the old object's header (the field already exists in AllocHeader but is currently unused) Mark the new copy When scanning reaches an object that has already been forwarded: Follow the forwarding pointer to the new location Use the new location for any references being updated","title":"3c: Evacuation during marking"},{"location":"plans/2026-02-03-gc-immix-design/#3d-reference-updating","text":"As the mark phase traces references, if a reference points to a forwarded object, update the reference in place to the new location. This means the scan must be able to write to the referring object, not just read it. The GcScannable trait needs extending. Currently scan() pushes discovered pointers but does not support rewriting them. Add an update_refs() method (or modify scan() to support reference rewriting) that follows the same traversal but updates forwarded pointers. The CollectorHeapView already has mutable access to the heap, so the mutation is possible within the existing safety model. This approach (update during scan) follows the Immix paper and avoids an extra full-heap traversal pass.","title":"3d: Reference updating"},{"location":"plans/2026-02-03-gc-immix-design/#3e-what-changes","text":"AllocHeader ( src/eval/memory/header.rs ) -- the forwarded_to field gets used. Add is_forwarded() / set_forwarded() methods. CollectorHeapView ( src/eval/memory/collect.rs ) -- add evacuate() method that copies an object and sets forwarding pointer. GcScannable ( src/eval/memory/collect.rs ) -- extend with scan_and_update() or modify scan() to support reference rewriting. Needs careful design to maintain the lifetime safety model. collect() ( src/eval/memory/collect.rs ) -- takes a strategy enum indicating mark-in-place vs evacuating. Evacuation cycles use the modified scan path. Heap ( src/eval/memory/heap.rs ) -- add evacuation target block management. Track which blocks are candidates. Lazy sweep interaction -- evacuated candidate blocks become fully dead after evacuation (all live objects moved out), so they can be returned to the free list without sweeping.","title":"3e: What changes"},{"location":"plans/2026-02-03-gc-immix-design/#3f-what-doesnt-change","text":"Allocation paths (bump allocation unchanged) Non-evacuating collections (mark-in-place path identical to phase 2) Object layout and size classes Large objects (LOBs are never evacuated -- too expensive to copy)","title":"3f: What doesn't change"},{"location":"plans/2026-02-03-gc-immix-design/#3g-risk-and-validation","text":"Risks: Reference updating is the most complex part. A missed pointer update is a use-after-move bug -- hard to debug, potentially silent corruption. The GcScannable trait change touches every heap object type. Needs careful review. Forwarding pointer chasing adds overhead to the mark phase even when it does not find forwarded objects. Validation: All existing tests pass ( cargo test ) New unit tests specifically for evacuation: create fragmented heap, evacuate, verify objects accessible at new locations 008_fragmentation.eu stress test should show improved blocks_recycled and reduced blocks_used after evacuation Non-fragmented workloads should show no regression (evacuation not triggered) Benchmark comparison confirms mark phase is only slightly slower (forwarding check overhead) and fragmentation recovery is measurably better","title":"3g: Risk and validation"},{"location":"plans/2026-02-03-gc-immix-design/#beads","text":"Existing beads to close on completion: eu-2ij -- GC: Implement lazy sweeping optimisation (phase 2) eu-5si -- GC: Implement full reference updating system (phase 3) eu-w68 -- Memory management unsafe code lacks safety documentation (address as part of phase 3, which adds new unsafe code for evacuation) New beads to create for phase 1 (benchmarking infrastructure) as no existing bead covers this work.","title":"Beads"},{"location":"plans/2026-02-03-lsp-server-design/","text":"LSP Server Design Date: 2026-02-03 Overview A Language Server Protocol implementation for eucalypt, bundled as the eu lsp subcommand. Communicates via stdio. Built on the lsp-server crate (same as Gleam, Nickel, rust-analyzer). Three phases, each building on the previous: Syntax-level features \u2014 diagnostics, document symbols, folding, selection ranges Semantic features \u2014 go-to-definition, hover, completion, find references Polish \u2014 formatting, semantic tokens, rename, code actions, inlay hints Foundations Eucalypt already has excellent LSP foundations: Rowan-based lossless parser with error recovery \u2014 preserves all source information (whitespace, comments, tokens) Comprehensive AST with 30+ node types covering the entire language, each with TextRange for source mapping 12+ parse error types with positional ranges \u2014 diagnostics are nearly free Tree-sitter grammar and Emacs major mode in editors/ \u2014 syntax highlighting already works Import graph using petgraph \u2014 transitive import resolution with cycle detection Formatter ( eu fmt ) using Rowan \u2014 two-mode (conservative and full reformat), with internal fragment formatting support Design Principles Reuse the compiler \u2014 the LSP shares the parser, import resolver, and formatter with the main eu binary. No separate analysis engine. File-declared context \u2014 the file being edited declares its own LSP context via metadata. No guessing. Start minimal, iterate \u2014 diagnostics first, then navigation, then polish. Context Model When the LSP analyses a file, it needs to know what other files provide names in scope. Three sources, in order: 1. Prelude (always loaded) The standard prelude ( lib/prelude.eu ) is always loaded as context. Its declarations ( map , filter , foldl , str.split , io.args , etc.) are available for completion, hover, and go-to-definition. Cached once on server start \u2014 it never changes during a session. When the eu binary updates (with new prelude functions), restarting the language server picks up the changes automatically. 2. Imports (followed automatically) Import metadata blocks are already parsed by the existing ImportGraph system: ` { import: \"helpers.eu\" } ` { import: [\"utils.eu\", \"data=config.yaml\"] } The LSP follows these transitively, using the same analyse_rowan_ast and Input::from_str logic as the compiler. Imported files are parsed and their declarations added to the symbol table. 3. lsp-context (explicit co-inputs) Files that would normally be provided as CLI co-inputs at runtime can be declared in the file's top-level metadata: ` { lsp-context: [\"base.yaml\", \"overrides.json\", \"helpers.eu\"] } This is equivalent to the command line: eu base.yaml overrides.json helpers.eu thisfile.eu Ordering matters \u2014 earlier entries provide bindings that later ones can reference, following the same merge_in / rebody semantics as CLI co-inputs. Unnamed entries merge their top-level keys flat into scope. Named entries (e.g. \"data=data.json\" ) wrap keys under the given name. The lsp-context value is a list of strings, each parsed with Input::from_str() \u2014 the same format as CLI arguments. Relative paths are resolved relative to the directory containing the .eu file being edited. For a single entry, a bare string is also accepted: ` { lsp-context: \"data.json\" } For data files (JSON, YAML, TOML, CSV), the LSP parses them to extract top-level keys for completion and go-to-definition. No deep analysis of values. Phase 1: Syntax-Level Features Architecture Crate : lsp-server (synchronous, crossbeam-channel based) Binary : eu lsp subcommand, added to the existing clap command structure in src/driver/options.rs Communication : stdio (stdin/stdout for LSP messages, stderr for logging) Threading : Single-threaded event loop. Re-parse on each didChange notification. For typical eucalypt files (small), full re-parse is sub-millisecond with Rowan. Features 1a. Diagnostics On each textDocument/didOpen and textDocument/didChange : Re-parse the file with the Rowan parser Collect Parse::errors() \u2014 each ParseError has a TextRange Convert to LSP Diagnostic with severity, range, and message Push via textDocument/publishDiagnostics The 12+ error types in src/syntax/rowan/error.rs map directly to diagnostic messages: UnexpectedToken , UnclosedSingleQuote , UnterminatedBlock , EmptyDeclarationBody , etc. Also parse imported files and report their errors, so the user sees downstream breakage. 1b. Document Symbols Walk the Rowan Unit node, extract Declaration nodes: Property declarations \u2192 SymbolKind::Property Function declarations (with parameters) \u2192 SymbolKind::Function Operator declarations \u2192 SymbolKind::Operator Return as DocumentSymbol[] with name, kind, range, and selection range. Provides the outline/breadcrumb view in both VS Code and Emacs. Nested blocks produce nested symbol hierarchies. 1c. Folding Ranges Derive from syntax tree nodes: Block ( { ... } ) \u2192 folding range List ( [ ... ] ) \u2192 folding range Multi-line string patterns \u2192 folding range Multi-line metadata annotations \u2192 folding range Only emit folding ranges for nodes spanning more than one line. 1d. Selection Range Smart expand/contract selection using Rowan's parent chain. Each SyntaxNode maps to a selection range, with its parent as the next expansion level. This is a core Rowan strength \u2014 rust-analyzer does exactly this. Example expansion sequence for cursor on x in f(x + y) : x \u2192 x + y \u2192 (x + y) \u2192 f(x + y) \u2192 full declaration Phase 1 Context Parse the open file only. Follow imports for error reporting. Load prelude as a resource but do not build a symbol table from it yet. lsp-context parsing is deferred to phase 2. Editor Deliverables (Phase 1) Emacs Eglot registration for the existing eucalypt-ts-mode : (add-to-list 'eglot-server-programs '(eucalypt-ts-mode \"eu\" \"lsp\")) The tree-sitter mode already provides syntax highlighting and indentation. Eglot adds diagnostics (via Flymake), document symbols, and folding. VS Code Minimal extension: package.json declaring the eucalypt language, .eu extension, and language server command ( eu lsp ) TextMate grammar ( syntaxes/eucalypt.tmLanguage.json ) derived from the existing tree-sitter queries/highlights.scm Language configuration (comment toggling with # , bracket pairs, auto-closing, 2-space indentation) Extension entry point using vscode-languageclient Phase 2: Semantic Features Phase 2 requires building a symbol table from parsed files. Symbol Table Construction On file open/change, after parsing: Parse the open file with Rowan Load the prelude (cached \u2014 it never changes) Follow import metadata to parse imported files (existing ImportGraph logic) Parse lsp-context metadata \u2014 same Input::from_str() parsing, treated as CLI co-inputs with merge_in / rebody semantics Walk all parsed trees to build a symbol table: name \u2192 (file, range, kind, documentation, parameters) Declaration kinds: Property : name: expression Function : f(x, y): expression \u2014 with parameter names and arity Operator : (x + y): expression \u2014 with fixity and precedence from metadata Documentation comes from ` { doc: \"...\" } metadata blocks that already exist in the language. For data files referenced via lsp-context , extract top-level keys for the symbol table. JSON objects produce property symbols, YAML mappings produce property symbols, etc. Features 2a. Go to Definition Resolve the identifier under the cursor against the symbol table. Search order: Local scope (same block / let binding) File-level declarations Imported file declarations lsp-context file declarations Prelude declarations Return the declaration's file URI and range. For dotted access like data.user , resolve data first, then look up user within its declarations. 2b. Hover Show information about the symbol under the cursor: Declaration kind (property / function / operator) Parameter names for functions Documentation from ` { doc: \"...\" } metadata Source file (for imported / prelude symbols) For prelude built-ins: the intrinsic name No type inference \u2014 purely syntactic information from declarations and metadata. 2c. Completion At the cursor position, determine what is in scope and offer completions. Sources: Local declarations in the same block File-level declarations Imported symbols lsp-context symbols Prelude symbols (highest value \u2014 map , filter , foldl , str.split , io.args , etc.) Each completion item includes: Label (the name) Kind (property / function / operator) Detail (parameter list for functions) Documentation (from metadata) For dotted access ( str. ), offer completions from the str block's declarations. 2d. Find References Given a declaration, search all open/imported files for uses of that name. Walk the Rowan trees looking for Name nodes matching the declaration. Scope-aware \u2014 a local x in an inner block does not match a top-level x . Phase 3: Formatting, Semantic Tokens, and Polish 3a. Formatting Wire up the existing eu fmt formatter via LSP: textDocument/formatting : Call format_source() with reformat mode on the full document. Return the formatted text as a single TextEdit replacing the entire document. textDocument/rangeFormatting : Find the smallest complete syntactic unit (declaration or block) containing the selection. Format that unit using the internal format_soup() / format_unit() API. Return TextEdit s for the affected range only. The formatter already works on the Rowan tree, so this is plumbing rather than new formatting logic. Fragment formatting may need the internal API to be extended slightly to accept a Declaration or Element node rather than only a full Unit . 3b. Semantic Tokens Augment tree-sitter / TextMate highlighting with context-aware token classification. Using the symbol table from phase 2: Prelude / built-in functions \u2192 function + defaultLibrary modifier User-defined functions \u2192 function Parameters \u2192 parameter Top-level properties \u2192 property Operator identifiers \u2192 operator Symbols ( :foo ) \u2192 enumMember (or custom symbol type) Metadata annotations \u2192 decorator String interpolation expressions \u2192 distinguished from string content Comments \u2192 comment Implement textDocument/semanticTokens/full first. Add textDocument/semanticTokens/range later for performance with large files. 3c. Rename Given the symbol table and find-references from phase 2, rename a symbol across the file and its imports: Find all references to the symbol Generate TextEdit s for each occurrence Return as WorkspaceEdit Scope-aware \u2014 renaming a local binding does not affect same-named bindings in other scopes. 3d. Code Actions Quick fixes for common errors: Unresolved name matching a prelude symbol : Suggest the correct qualified name (e.g. split \u2192 str.split ) {name: name} self-reference : If the eu-dlr static check is implemented, offer a fix suggestion Missing import : If a name resolves to a file in the workspace but is not imported, suggest adding an import 3e. Inlay Hints Show operator precedence and associativity inline for operator declarations Show parameter names at call sites for functions with multiple arguments Potentially show inferred structure for complex soup expressions (how operators bind) Crate and Dependency Summary New dependencies for the LSP: lsp-server \u2014 LSP protocol handling, message dispatch lsp-types \u2014 Rust type definitions for LSP protocol messages serde_json \u2014 already a dependency, used for LSP message serialisation No async runtime needed \u2014 lsp-server is synchronous. File Structure src/ bin/eu.rs # Add LspMode dispatch driver/ options.rs # Add LspArgs / lsp subcommand lsp.rs (new) # LSP server main loop lsp/ diagnostics.rs (new) # Parse errors \u2192 LSP diagnostics symbols.rs (new) # Document symbols from Rowan tree folding.rs (new) # Folding ranges selection.rs (new) # Selection ranges context.rs (new) # lsp-context metadata parsing symbol_table.rs (new) # Phase 2: name resolution completion.rs (new) # Phase 2: completion provider hover.rs (new) # Phase 2: hover provider navigation.rs (new) # Phase 2: go-to-def, references formatting.rs (new) # Phase 3: formatter integration semantic.rs (new) # Phase 3: semantic tokens actions.rs (new) # Phase 3: code actions editors/ vscode/ (new) # VS Code extension package.json language-configuration.json syntaxes/eucalypt.tmLanguage.json src/extension.ts emacs/ eucalypt-ts-mode.el # Existing \u2014 add eglot registration Beads Existing: eu-307 \u2014 LSP server. Update description with design reference. New beads to create per phase: Phase 1: - LSP server scaffold (eu lsp subcommand, lsp-server integration, stdio loop) - LSP diagnostics from Rowan parser - LSP document symbols - LSP folding and selection ranges - VS Code extension (TextMate grammar + language client) Phase 2: - lsp-context metadata parsing - Symbol table construction (prelude + imports + lsp-context) - Go-to-definition - Hover - Completion - Find references Phase 3: - LSP formatting integration (full document + range) - Semantic tokens - Rename - Code actions - Inlay hints","title":"LSP Server Design"},{"location":"plans/2026-02-03-lsp-server-design/#lsp-server-design","text":"Date: 2026-02-03","title":"LSP Server Design"},{"location":"plans/2026-02-03-lsp-server-design/#overview","text":"A Language Server Protocol implementation for eucalypt, bundled as the eu lsp subcommand. Communicates via stdio. Built on the lsp-server crate (same as Gleam, Nickel, rust-analyzer). Three phases, each building on the previous: Syntax-level features \u2014 diagnostics, document symbols, folding, selection ranges Semantic features \u2014 go-to-definition, hover, completion, find references Polish \u2014 formatting, semantic tokens, rename, code actions, inlay hints","title":"Overview"},{"location":"plans/2026-02-03-lsp-server-design/#foundations","text":"Eucalypt already has excellent LSP foundations: Rowan-based lossless parser with error recovery \u2014 preserves all source information (whitespace, comments, tokens) Comprehensive AST with 30+ node types covering the entire language, each with TextRange for source mapping 12+ parse error types with positional ranges \u2014 diagnostics are nearly free Tree-sitter grammar and Emacs major mode in editors/ \u2014 syntax highlighting already works Import graph using petgraph \u2014 transitive import resolution with cycle detection Formatter ( eu fmt ) using Rowan \u2014 two-mode (conservative and full reformat), with internal fragment formatting support","title":"Foundations"},{"location":"plans/2026-02-03-lsp-server-design/#design-principles","text":"Reuse the compiler \u2014 the LSP shares the parser, import resolver, and formatter with the main eu binary. No separate analysis engine. File-declared context \u2014 the file being edited declares its own LSP context via metadata. No guessing. Start minimal, iterate \u2014 diagnostics first, then navigation, then polish.","title":"Design Principles"},{"location":"plans/2026-02-03-lsp-server-design/#context-model","text":"When the LSP analyses a file, it needs to know what other files provide names in scope. Three sources, in order:","title":"Context Model"},{"location":"plans/2026-02-03-lsp-server-design/#1-prelude-always-loaded","text":"The standard prelude ( lib/prelude.eu ) is always loaded as context. Its declarations ( map , filter , foldl , str.split , io.args , etc.) are available for completion, hover, and go-to-definition. Cached once on server start \u2014 it never changes during a session. When the eu binary updates (with new prelude functions), restarting the language server picks up the changes automatically.","title":"1. Prelude (always loaded)"},{"location":"plans/2026-02-03-lsp-server-design/#2-imports-followed-automatically","text":"Import metadata blocks are already parsed by the existing ImportGraph system: ` { import: \"helpers.eu\" } ` { import: [\"utils.eu\", \"data=config.yaml\"] } The LSP follows these transitively, using the same analyse_rowan_ast and Input::from_str logic as the compiler. Imported files are parsed and their declarations added to the symbol table.","title":"2. Imports (followed automatically)"},{"location":"plans/2026-02-03-lsp-server-design/#3-lsp-context-explicit-co-inputs","text":"Files that would normally be provided as CLI co-inputs at runtime can be declared in the file's top-level metadata: ` { lsp-context: [\"base.yaml\", \"overrides.json\", \"helpers.eu\"] } This is equivalent to the command line: eu base.yaml overrides.json helpers.eu thisfile.eu Ordering matters \u2014 earlier entries provide bindings that later ones can reference, following the same merge_in / rebody semantics as CLI co-inputs. Unnamed entries merge their top-level keys flat into scope. Named entries (e.g. \"data=data.json\" ) wrap keys under the given name. The lsp-context value is a list of strings, each parsed with Input::from_str() \u2014 the same format as CLI arguments. Relative paths are resolved relative to the directory containing the .eu file being edited. For a single entry, a bare string is also accepted: ` { lsp-context: \"data.json\" } For data files (JSON, YAML, TOML, CSV), the LSP parses them to extract top-level keys for completion and go-to-definition. No deep analysis of values.","title":"3. lsp-context (explicit co-inputs)"},{"location":"plans/2026-02-03-lsp-server-design/#phase-1-syntax-level-features","text":"","title":"Phase 1: Syntax-Level Features"},{"location":"plans/2026-02-03-lsp-server-design/#architecture","text":"Crate : lsp-server (synchronous, crossbeam-channel based) Binary : eu lsp subcommand, added to the existing clap command structure in src/driver/options.rs Communication : stdio (stdin/stdout for LSP messages, stderr for logging) Threading : Single-threaded event loop. Re-parse on each didChange notification. For typical eucalypt files (small), full re-parse is sub-millisecond with Rowan.","title":"Architecture"},{"location":"plans/2026-02-03-lsp-server-design/#features","text":"","title":"Features"},{"location":"plans/2026-02-03-lsp-server-design/#1a-diagnostics","text":"On each textDocument/didOpen and textDocument/didChange : Re-parse the file with the Rowan parser Collect Parse::errors() \u2014 each ParseError has a TextRange Convert to LSP Diagnostic with severity, range, and message Push via textDocument/publishDiagnostics The 12+ error types in src/syntax/rowan/error.rs map directly to diagnostic messages: UnexpectedToken , UnclosedSingleQuote , UnterminatedBlock , EmptyDeclarationBody , etc. Also parse imported files and report their errors, so the user sees downstream breakage.","title":"1a. Diagnostics"},{"location":"plans/2026-02-03-lsp-server-design/#1b-document-symbols","text":"Walk the Rowan Unit node, extract Declaration nodes: Property declarations \u2192 SymbolKind::Property Function declarations (with parameters) \u2192 SymbolKind::Function Operator declarations \u2192 SymbolKind::Operator Return as DocumentSymbol[] with name, kind, range, and selection range. Provides the outline/breadcrumb view in both VS Code and Emacs. Nested blocks produce nested symbol hierarchies.","title":"1b. Document Symbols"},{"location":"plans/2026-02-03-lsp-server-design/#1c-folding-ranges","text":"Derive from syntax tree nodes: Block ( { ... } ) \u2192 folding range List ( [ ... ] ) \u2192 folding range Multi-line string patterns \u2192 folding range Multi-line metadata annotations \u2192 folding range Only emit folding ranges for nodes spanning more than one line.","title":"1c. Folding Ranges"},{"location":"plans/2026-02-03-lsp-server-design/#1d-selection-range","text":"Smart expand/contract selection using Rowan's parent chain. Each SyntaxNode maps to a selection range, with its parent as the next expansion level. This is a core Rowan strength \u2014 rust-analyzer does exactly this. Example expansion sequence for cursor on x in f(x + y) : x \u2192 x + y \u2192 (x + y) \u2192 f(x + y) \u2192 full declaration","title":"1d. Selection Range"},{"location":"plans/2026-02-03-lsp-server-design/#phase-1-context","text":"Parse the open file only. Follow imports for error reporting. Load prelude as a resource but do not build a symbol table from it yet. lsp-context parsing is deferred to phase 2.","title":"Phase 1 Context"},{"location":"plans/2026-02-03-lsp-server-design/#editor-deliverables-phase-1","text":"","title":"Editor Deliverables (Phase 1)"},{"location":"plans/2026-02-03-lsp-server-design/#emacs","text":"Eglot registration for the existing eucalypt-ts-mode : (add-to-list 'eglot-server-programs '(eucalypt-ts-mode \"eu\" \"lsp\")) The tree-sitter mode already provides syntax highlighting and indentation. Eglot adds diagnostics (via Flymake), document symbols, and folding.","title":"Emacs"},{"location":"plans/2026-02-03-lsp-server-design/#vs-code","text":"Minimal extension: package.json declaring the eucalypt language, .eu extension, and language server command ( eu lsp ) TextMate grammar ( syntaxes/eucalypt.tmLanguage.json ) derived from the existing tree-sitter queries/highlights.scm Language configuration (comment toggling with # , bracket pairs, auto-closing, 2-space indentation) Extension entry point using vscode-languageclient","title":"VS Code"},{"location":"plans/2026-02-03-lsp-server-design/#phase-2-semantic-features","text":"Phase 2 requires building a symbol table from parsed files.","title":"Phase 2: Semantic Features"},{"location":"plans/2026-02-03-lsp-server-design/#symbol-table-construction","text":"On file open/change, after parsing: Parse the open file with Rowan Load the prelude (cached \u2014 it never changes) Follow import metadata to parse imported files (existing ImportGraph logic) Parse lsp-context metadata \u2014 same Input::from_str() parsing, treated as CLI co-inputs with merge_in / rebody semantics Walk all parsed trees to build a symbol table: name \u2192 (file, range, kind, documentation, parameters) Declaration kinds: Property : name: expression Function : f(x, y): expression \u2014 with parameter names and arity Operator : (x + y): expression \u2014 with fixity and precedence from metadata Documentation comes from ` { doc: \"...\" } metadata blocks that already exist in the language. For data files referenced via lsp-context , extract top-level keys for the symbol table. JSON objects produce property symbols, YAML mappings produce property symbols, etc.","title":"Symbol Table Construction"},{"location":"plans/2026-02-03-lsp-server-design/#features_1","text":"","title":"Features"},{"location":"plans/2026-02-03-lsp-server-design/#2a-go-to-definition","text":"Resolve the identifier under the cursor against the symbol table. Search order: Local scope (same block / let binding) File-level declarations Imported file declarations lsp-context file declarations Prelude declarations Return the declaration's file URI and range. For dotted access like data.user , resolve data first, then look up user within its declarations.","title":"2a. Go to Definition"},{"location":"plans/2026-02-03-lsp-server-design/#2b-hover","text":"Show information about the symbol under the cursor: Declaration kind (property / function / operator) Parameter names for functions Documentation from ` { doc: \"...\" } metadata Source file (for imported / prelude symbols) For prelude built-ins: the intrinsic name No type inference \u2014 purely syntactic information from declarations and metadata.","title":"2b. Hover"},{"location":"plans/2026-02-03-lsp-server-design/#2c-completion","text":"At the cursor position, determine what is in scope and offer completions. Sources: Local declarations in the same block File-level declarations Imported symbols lsp-context symbols Prelude symbols (highest value \u2014 map , filter , foldl , str.split , io.args , etc.) Each completion item includes: Label (the name) Kind (property / function / operator) Detail (parameter list for functions) Documentation (from metadata) For dotted access ( str. ), offer completions from the str block's declarations.","title":"2c. Completion"},{"location":"plans/2026-02-03-lsp-server-design/#2d-find-references","text":"Given a declaration, search all open/imported files for uses of that name. Walk the Rowan trees looking for Name nodes matching the declaration. Scope-aware \u2014 a local x in an inner block does not match a top-level x .","title":"2d. Find References"},{"location":"plans/2026-02-03-lsp-server-design/#phase-3-formatting-semantic-tokens-and-polish","text":"","title":"Phase 3: Formatting, Semantic Tokens, and Polish"},{"location":"plans/2026-02-03-lsp-server-design/#3a-formatting","text":"Wire up the existing eu fmt formatter via LSP: textDocument/formatting : Call format_source() with reformat mode on the full document. Return the formatted text as a single TextEdit replacing the entire document. textDocument/rangeFormatting : Find the smallest complete syntactic unit (declaration or block) containing the selection. Format that unit using the internal format_soup() / format_unit() API. Return TextEdit s for the affected range only. The formatter already works on the Rowan tree, so this is plumbing rather than new formatting logic. Fragment formatting may need the internal API to be extended slightly to accept a Declaration or Element node rather than only a full Unit .","title":"3a. Formatting"},{"location":"plans/2026-02-03-lsp-server-design/#3b-semantic-tokens","text":"Augment tree-sitter / TextMate highlighting with context-aware token classification. Using the symbol table from phase 2: Prelude / built-in functions \u2192 function + defaultLibrary modifier User-defined functions \u2192 function Parameters \u2192 parameter Top-level properties \u2192 property Operator identifiers \u2192 operator Symbols ( :foo ) \u2192 enumMember (or custom symbol type) Metadata annotations \u2192 decorator String interpolation expressions \u2192 distinguished from string content Comments \u2192 comment Implement textDocument/semanticTokens/full first. Add textDocument/semanticTokens/range later for performance with large files.","title":"3b. Semantic Tokens"},{"location":"plans/2026-02-03-lsp-server-design/#3c-rename","text":"Given the symbol table and find-references from phase 2, rename a symbol across the file and its imports: Find all references to the symbol Generate TextEdit s for each occurrence Return as WorkspaceEdit Scope-aware \u2014 renaming a local binding does not affect same-named bindings in other scopes.","title":"3c. Rename"},{"location":"plans/2026-02-03-lsp-server-design/#3d-code-actions","text":"Quick fixes for common errors: Unresolved name matching a prelude symbol : Suggest the correct qualified name (e.g. split \u2192 str.split ) {name: name} self-reference : If the eu-dlr static check is implemented, offer a fix suggestion Missing import : If a name resolves to a file in the workspace but is not imported, suggest adding an import","title":"3d. Code Actions"},{"location":"plans/2026-02-03-lsp-server-design/#3e-inlay-hints","text":"Show operator precedence and associativity inline for operator declarations Show parameter names at call sites for functions with multiple arguments Potentially show inferred structure for complex soup expressions (how operators bind)","title":"3e. Inlay Hints"},{"location":"plans/2026-02-03-lsp-server-design/#crate-and-dependency-summary","text":"New dependencies for the LSP: lsp-server \u2014 LSP protocol handling, message dispatch lsp-types \u2014 Rust type definitions for LSP protocol messages serde_json \u2014 already a dependency, used for LSP message serialisation No async runtime needed \u2014 lsp-server is synchronous.","title":"Crate and Dependency Summary"},{"location":"plans/2026-02-03-lsp-server-design/#file-structure","text":"src/ bin/eu.rs # Add LspMode dispatch driver/ options.rs # Add LspArgs / lsp subcommand lsp.rs (new) # LSP server main loop lsp/ diagnostics.rs (new) # Parse errors \u2192 LSP diagnostics symbols.rs (new) # Document symbols from Rowan tree folding.rs (new) # Folding ranges selection.rs (new) # Selection ranges context.rs (new) # lsp-context metadata parsing symbol_table.rs (new) # Phase 2: name resolution completion.rs (new) # Phase 2: completion provider hover.rs (new) # Phase 2: hover provider navigation.rs (new) # Phase 2: go-to-def, references formatting.rs (new) # Phase 3: formatter integration semantic.rs (new) # Phase 3: semantic tokens actions.rs (new) # Phase 3: code actions editors/ vscode/ (new) # VS Code extension package.json language-configuration.json syntaxes/eucalypt.tmLanguage.json src/extension.ts emacs/ eucalypt-ts-mode.el # Existing \u2014 add eglot registration","title":"File Structure"},{"location":"plans/2026-02-03-lsp-server-design/#beads","text":"Existing: eu-307 \u2014 LSP server. Update description with design reference. New beads to create per phase: Phase 1: - LSP server scaffold (eu lsp subcommand, lsp-server integration, stdio loop) - LSP diagnostics from Rowan parser - LSP document symbols - LSP folding and selection ranges - VS Code extension (TextMate grammar + language client) Phase 2: - lsp-context metadata parsing - Symbol table construction (prelude + imports + lsp-context) - Go-to-definition - Hover - Completion - Find references Phase 3: - LSP formatting integration (full document + range) - Semantic tokens - Rename - Code actions - Inlay hints","title":"Beads"},{"location":"plans/2026-02-03-quick-wins-design/","text":"Quick Wins Design Date: 2026-02-03 Overview Five small, well-defined features that follow established patterns in the codebase. Each is independent and can be implemented in any order. Catch {name: name} recursion (eu-dlr) \u2014 static + runtime cycle detection min/max for ZDTs (eu-u1m) \u2014 polymorphic comparison operators Version assertions (eu-nbc) \u2014 eu.requires(\">=0.2.0\") Base64 encoding (eu-dyx) \u2014 encode/decode intrinsics Hash built-ins (eu-dd5) \u2014 SHA-256 intrinsic 1. Catch {name: name} Recursion (eu-dlr) Problem The pattern {name: name} is a common mistake for developers coming from other languages. Eucalypt currently enters an infinite loop instead of providing a helpful error message. The thunk enters itself, pushes an Update continuation, and re-enters endlessly. Approach: Two layers of protection Layer 1: Static check in cook phase After soup processing / varification, walk each Let binding's body. If the body is a bare Var::Free(fv) where fv.pretty_name matches the binding's own name (and there is no intervening scope that shadows it), emit a compile error: \"binding 'name' refers to itself\". Also detect simple mutual cycles like {a: b, b: a} within the same let block by building a reference graph among the block's bindings and checking for cycles. Location : New function in src/core/cook/mod.rs or a small new module src/core/verify/self_ref.rs . Runs after varification, before inline/simplify. Performance : Gate with before/after timing using the existing Timings infrastructure. The cook phase already has a timing entry. If the check is measurable, scope it to only check LetType::DefaultBlockLet bindings (blocks, where the mistake actually occurs) rather than all let expressions. Layer 2: Runtime BlackHole When entering a thunk (the code path in vm.rs that pushes Continuation::Update ), overwrite the binding in the environment with a BlackHole sentinel before evaluation. If evaluation re-enters the same binding, it hits the BlackHole and raises ExecutionError::BlackHole . When the Update continuation fires successfully, the BlackHole is replaced with the computed value (which Update already does). The ExecutionError::BlackHole variant already exists but is currently unused. This catches any cycle that the static check misses (indirect cycles through multiple levels of indirection). Important : This does not affect benign recursion. Functions are compiled as lambda forms (already in WHNF), not thunks. They never get an Update continuation, so the BlackHole overwrite never happens. Files changed src/core/cook/mod.rs or src/core/verify/self_ref.rs \u2014 static cycle detection src/eval/machine/vm.rs \u2014 BlackHole overwrite at thunk entry src/eval/memory/syntax.rs \u2014 BlackHole sentinel value src/eval/machine/error.rs \u2014 improve BlackHole error message with binding context Testing New harness error test for {name: name} \u2014 compile error, not hang New harness error test for {a: b, b: a} \u2014 compile error New harness error test for indirect runtime cycle \u2014 BlackHole error Existing recursive programs (fibonacci etc.) continue to work Before/after timing of cook phase to verify no performance regression 2. min/max for ZDTs (eu-u1m) Problem The prelude min and max functions use < and > operators, which only work on numbers. ZDT values ( DateTime<FixedOffset> ) implement PartialOrd and Ord via chrono but the comparison intrinsics do not handle them. Approach: Make comparison operators polymorphic Extend Lt , Gt , Lte , Gte in arith.rs to handle Native::Zdt and Native::Str / Native::Sym values, following the same pattern as EQ in eq.rs which already handles multiple types polymorphically: fn execute(&self, ..., args: &[Ref]) -> Result<(), ExecutionError> { let x = machine.nav(view).resolve_native(&args[0])?; let y = machine.nav(view).resolve_native(&args[1])?; match (x, y) { (Native::Num(nx), Native::Num(ny)) => { // existing numeric comparison } (Native::Zdt(dx), Native::Zdt(dy)) => { machine_return_bool(machine, view, dx < dy) } (Native::Str(sx), Native::Str(sy)) => { machine_return_bool(machine, view, sx < sy) } (Native::Sym(sx), Native::Sym(sy)) => { // compare by resolved string, not by SymbolId machine_return_bool(machine, view, pool.resolve(sx) < pool.resolve(sy)) } _ => Err(ExecutionError::TypeMismatch(...)) } } This means min , max , and sort-by(key, lt) in the prelude automatically work for ZDTs and strings with no prelude changes. Note : Symbol comparison uses the resolved string (lexicographic), not the interned ID (which reflects insertion order). This ensures sort-keys produces alphabetical output regardless of interning order. Files changed src/eval/stg/arith.rs \u2014 extend Lt , Gt , Lte , Gte to handle Native::Zdt , Native::Str , and Native::Sym Testing New harness test: ZDT comparison operators return correct booleans New harness test: string comparison operators return correct booleans New harness test: symbol comparison with lexicographic ordering New harness test: min / max with ZDT values New harness test: min / max with string values Existing numeric comparison tests unchanged 3. Version Assertions (eu-nbc) Problem Scripts should be able to declare which eucalypt version they require, with fail-fast behaviour if requirements are not met. Approach: REQUIRES intrinsic with semver crate Add semver to Cargo.toml dependencies. New intrinsic REQUIRES that: Takes one string argument (version constraint, e.g. \">=0.2.0\" ) Parses the constraint with semver::VersionReq::parse() Parses the current eucalypt version with semver::Version::parse() (stripping the .dev suffix from e.g. 0.2.0.dev \u2192 0.2.0 ) If the constraint matches, returns unit (no-op) If it does not match, returns an error: \"eucalypt version 0.2.0 does not satisfy requirement >=0.3.0\" If either string fails to parse, returns a clear parse error Files changed Cargo.toml \u2014 add semver dependency src/eval/stg/version.rs (new) \u2014 Requires intrinsic implementation src/eval/stg/mod.rs \u2014 register module and intrinsic src/eval/intrinsics.rs \u2014 catalogue entry lib/prelude.eu \u2014 expose as eu.requires: __REQUIRES Usage eu.requires(\">=0.2.0\") result: do-stuff Testing Harness test: eu.requires(\">=0.0.1\") \u2014 passes Harness error test: eu.requires(\">=99.0.0\") \u2014 fails with clear message Harness test: eu.requires(\"^0.2\") \u2014 verify caret ranges work 4. Base64 Encoding Functions (eu-dyx) Problem No built-in base64 encode/decode capability. Approach: Two new intrinsics using base64 crate Add base64 crate to Cargo.toml . Two intrinsics: BASE64_ENCODE \u2014 takes a string, returns its base64 encoding (standard alphabet, with padding) BASE64_DECODE \u2014 takes a base64 string, returns the decoded string. Errors if the input is not valid base64 or the decoded bytes are not valid UTF-8. Both follow the existing string intrinsic pattern: str_arg() to extract the argument, machine_return_str() to return the result. Files changed Cargo.toml \u2014 add base64 dependency src/eval/stg/encoding.rs (new) \u2014 Base64Encode and Base64Decode intrinsic implementations src/eval/stg/mod.rs \u2014 register module and intrinsics src/eval/intrinsics.rs \u2014 two catalogue entries lib/prelude.eu \u2014 expose as str.base64-encode: __BASE64_ENCODE and str.base64-decode: __BASE64_DECODE Usage str.base64-encode(\"hello world\") # => \"aGVsbG8gd29ybGQ=\" str.base64-decode(\"aGVsbG8gd29ybGQ=\") # => \"hello world\" Testing Harness test: encode then decode roundtrip Harness test: known test vectors Harness error test: invalid base64 input to decode 5. Hash Built-ins (eu-dd5) Problem No built-in cryptographic hash capability. Approach: SHA-256 intrinsic using sha2 crate Add sha2 crate to Cargo.toml . One intrinsic: SHA256 \u2014 takes a string, returns its SHA-256 hash as a lowercase hex string Implemented in the same encoding.rs module as base64 (both are encoding/hashing operations). If other hash algorithms are needed later, they follow the same pattern in the same module. Files changed Cargo.toml \u2014 add sha2 dependency src/eval/stg/encoding.rs \u2014 add Sha256Hash intrinsic src/eval/stg/mod.rs \u2014 register intrinsic src/eval/intrinsics.rs \u2014 catalogue entry lib/prelude.eu \u2014 expose as str.sha256: __SHA256 Usage str.sha256(\"hello world\") # => \"b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9\" Testing Harness test: known SHA-256 test vector for \"hello world\" Harness test: empty string hash (known value) Harness test: verify output is lowercase hex, 64 characters Beads Existing beads to update with design references: eu-dlr \u2014 Catch {name: name} recursion (section 1) eu-u1m \u2014 min/max for ZDTs (section 2) eu-nbc \u2014 Version assertions (section 3) eu-dyx \u2014 Base64 encoding functions (section 4) eu-dd5 \u2014 Hash built-ins (section 5) All five are independent and can be implemented in any order.","title":"Quick Wins Design"},{"location":"plans/2026-02-03-quick-wins-design/#quick-wins-design","text":"Date: 2026-02-03","title":"Quick Wins Design"},{"location":"plans/2026-02-03-quick-wins-design/#overview","text":"Five small, well-defined features that follow established patterns in the codebase. Each is independent and can be implemented in any order. Catch {name: name} recursion (eu-dlr) \u2014 static + runtime cycle detection min/max for ZDTs (eu-u1m) \u2014 polymorphic comparison operators Version assertions (eu-nbc) \u2014 eu.requires(\">=0.2.0\") Base64 encoding (eu-dyx) \u2014 encode/decode intrinsics Hash built-ins (eu-dd5) \u2014 SHA-256 intrinsic","title":"Overview"},{"location":"plans/2026-02-03-quick-wins-design/#1-catch-name-name-recursion-eu-dlr","text":"","title":"1. Catch {name: name} Recursion (eu-dlr)"},{"location":"plans/2026-02-03-quick-wins-design/#problem","text":"The pattern {name: name} is a common mistake for developers coming from other languages. Eucalypt currently enters an infinite loop instead of providing a helpful error message. The thunk enters itself, pushes an Update continuation, and re-enters endlessly.","title":"Problem"},{"location":"plans/2026-02-03-quick-wins-design/#approach-two-layers-of-protection","text":"","title":"Approach: Two layers of protection"},{"location":"plans/2026-02-03-quick-wins-design/#layer-1-static-check-in-cook-phase","text":"After soup processing / varification, walk each Let binding's body. If the body is a bare Var::Free(fv) where fv.pretty_name matches the binding's own name (and there is no intervening scope that shadows it), emit a compile error: \"binding 'name' refers to itself\". Also detect simple mutual cycles like {a: b, b: a} within the same let block by building a reference graph among the block's bindings and checking for cycles. Location : New function in src/core/cook/mod.rs or a small new module src/core/verify/self_ref.rs . Runs after varification, before inline/simplify. Performance : Gate with before/after timing using the existing Timings infrastructure. The cook phase already has a timing entry. If the check is measurable, scope it to only check LetType::DefaultBlockLet bindings (blocks, where the mistake actually occurs) rather than all let expressions.","title":"Layer 1: Static check in cook phase"},{"location":"plans/2026-02-03-quick-wins-design/#layer-2-runtime-blackhole","text":"When entering a thunk (the code path in vm.rs that pushes Continuation::Update ), overwrite the binding in the environment with a BlackHole sentinel before evaluation. If evaluation re-enters the same binding, it hits the BlackHole and raises ExecutionError::BlackHole . When the Update continuation fires successfully, the BlackHole is replaced with the computed value (which Update already does). The ExecutionError::BlackHole variant already exists but is currently unused. This catches any cycle that the static check misses (indirect cycles through multiple levels of indirection). Important : This does not affect benign recursion. Functions are compiled as lambda forms (already in WHNF), not thunks. They never get an Update continuation, so the BlackHole overwrite never happens.","title":"Layer 2: Runtime BlackHole"},{"location":"plans/2026-02-03-quick-wins-design/#files-changed","text":"src/core/cook/mod.rs or src/core/verify/self_ref.rs \u2014 static cycle detection src/eval/machine/vm.rs \u2014 BlackHole overwrite at thunk entry src/eval/memory/syntax.rs \u2014 BlackHole sentinel value src/eval/machine/error.rs \u2014 improve BlackHole error message with binding context","title":"Files changed"},{"location":"plans/2026-02-03-quick-wins-design/#testing","text":"New harness error test for {name: name} \u2014 compile error, not hang New harness error test for {a: b, b: a} \u2014 compile error New harness error test for indirect runtime cycle \u2014 BlackHole error Existing recursive programs (fibonacci etc.) continue to work Before/after timing of cook phase to verify no performance regression","title":"Testing"},{"location":"plans/2026-02-03-quick-wins-design/#2-minmax-for-zdts-eu-u1m","text":"","title":"2. min/max for ZDTs (eu-u1m)"},{"location":"plans/2026-02-03-quick-wins-design/#problem_1","text":"The prelude min and max functions use < and > operators, which only work on numbers. ZDT values ( DateTime<FixedOffset> ) implement PartialOrd and Ord via chrono but the comparison intrinsics do not handle them.","title":"Problem"},{"location":"plans/2026-02-03-quick-wins-design/#approach-make-comparison-operators-polymorphic","text":"Extend Lt , Gt , Lte , Gte in arith.rs to handle Native::Zdt and Native::Str / Native::Sym values, following the same pattern as EQ in eq.rs which already handles multiple types polymorphically: fn execute(&self, ..., args: &[Ref]) -> Result<(), ExecutionError> { let x = machine.nav(view).resolve_native(&args[0])?; let y = machine.nav(view).resolve_native(&args[1])?; match (x, y) { (Native::Num(nx), Native::Num(ny)) => { // existing numeric comparison } (Native::Zdt(dx), Native::Zdt(dy)) => { machine_return_bool(machine, view, dx < dy) } (Native::Str(sx), Native::Str(sy)) => { machine_return_bool(machine, view, sx < sy) } (Native::Sym(sx), Native::Sym(sy)) => { // compare by resolved string, not by SymbolId machine_return_bool(machine, view, pool.resolve(sx) < pool.resolve(sy)) } _ => Err(ExecutionError::TypeMismatch(...)) } } This means min , max , and sort-by(key, lt) in the prelude automatically work for ZDTs and strings with no prelude changes. Note : Symbol comparison uses the resolved string (lexicographic), not the interned ID (which reflects insertion order). This ensures sort-keys produces alphabetical output regardless of interning order.","title":"Approach: Make comparison operators polymorphic"},{"location":"plans/2026-02-03-quick-wins-design/#files-changed_1","text":"src/eval/stg/arith.rs \u2014 extend Lt , Gt , Lte , Gte to handle Native::Zdt , Native::Str , and Native::Sym","title":"Files changed"},{"location":"plans/2026-02-03-quick-wins-design/#testing_1","text":"New harness test: ZDT comparison operators return correct booleans New harness test: string comparison operators return correct booleans New harness test: symbol comparison with lexicographic ordering New harness test: min / max with ZDT values New harness test: min / max with string values Existing numeric comparison tests unchanged","title":"Testing"},{"location":"plans/2026-02-03-quick-wins-design/#3-version-assertions-eu-nbc","text":"","title":"3. Version Assertions (eu-nbc)"},{"location":"plans/2026-02-03-quick-wins-design/#problem_2","text":"Scripts should be able to declare which eucalypt version they require, with fail-fast behaviour if requirements are not met.","title":"Problem"},{"location":"plans/2026-02-03-quick-wins-design/#approach-requires-intrinsic-with-semver-crate","text":"Add semver to Cargo.toml dependencies. New intrinsic REQUIRES that: Takes one string argument (version constraint, e.g. \">=0.2.0\" ) Parses the constraint with semver::VersionReq::parse() Parses the current eucalypt version with semver::Version::parse() (stripping the .dev suffix from e.g. 0.2.0.dev \u2192 0.2.0 ) If the constraint matches, returns unit (no-op) If it does not match, returns an error: \"eucalypt version 0.2.0 does not satisfy requirement >=0.3.0\" If either string fails to parse, returns a clear parse error","title":"Approach: REQUIRES intrinsic with semver crate"},{"location":"plans/2026-02-03-quick-wins-design/#files-changed_2","text":"Cargo.toml \u2014 add semver dependency src/eval/stg/version.rs (new) \u2014 Requires intrinsic implementation src/eval/stg/mod.rs \u2014 register module and intrinsic src/eval/intrinsics.rs \u2014 catalogue entry lib/prelude.eu \u2014 expose as eu.requires: __REQUIRES","title":"Files changed"},{"location":"plans/2026-02-03-quick-wins-design/#usage","text":"eu.requires(\">=0.2.0\") result: do-stuff","title":"Usage"},{"location":"plans/2026-02-03-quick-wins-design/#testing_2","text":"Harness test: eu.requires(\">=0.0.1\") \u2014 passes Harness error test: eu.requires(\">=99.0.0\") \u2014 fails with clear message Harness test: eu.requires(\"^0.2\") \u2014 verify caret ranges work","title":"Testing"},{"location":"plans/2026-02-03-quick-wins-design/#4-base64-encoding-functions-eu-dyx","text":"","title":"4. Base64 Encoding Functions (eu-dyx)"},{"location":"plans/2026-02-03-quick-wins-design/#problem_3","text":"No built-in base64 encode/decode capability.","title":"Problem"},{"location":"plans/2026-02-03-quick-wins-design/#approach-two-new-intrinsics-using-base64-crate","text":"Add base64 crate to Cargo.toml . Two intrinsics: BASE64_ENCODE \u2014 takes a string, returns its base64 encoding (standard alphabet, with padding) BASE64_DECODE \u2014 takes a base64 string, returns the decoded string. Errors if the input is not valid base64 or the decoded bytes are not valid UTF-8. Both follow the existing string intrinsic pattern: str_arg() to extract the argument, machine_return_str() to return the result.","title":"Approach: Two new intrinsics using base64 crate"},{"location":"plans/2026-02-03-quick-wins-design/#files-changed_3","text":"Cargo.toml \u2014 add base64 dependency src/eval/stg/encoding.rs (new) \u2014 Base64Encode and Base64Decode intrinsic implementations src/eval/stg/mod.rs \u2014 register module and intrinsics src/eval/intrinsics.rs \u2014 two catalogue entries lib/prelude.eu \u2014 expose as str.base64-encode: __BASE64_ENCODE and str.base64-decode: __BASE64_DECODE","title":"Files changed"},{"location":"plans/2026-02-03-quick-wins-design/#usage_1","text":"str.base64-encode(\"hello world\") # => \"aGVsbG8gd29ybGQ=\" str.base64-decode(\"aGVsbG8gd29ybGQ=\") # => \"hello world\"","title":"Usage"},{"location":"plans/2026-02-03-quick-wins-design/#testing_3","text":"Harness test: encode then decode roundtrip Harness test: known test vectors Harness error test: invalid base64 input to decode","title":"Testing"},{"location":"plans/2026-02-03-quick-wins-design/#5-hash-built-ins-eu-dd5","text":"","title":"5. Hash Built-ins (eu-dd5)"},{"location":"plans/2026-02-03-quick-wins-design/#problem_4","text":"No built-in cryptographic hash capability.","title":"Problem"},{"location":"plans/2026-02-03-quick-wins-design/#approach-sha-256-intrinsic-using-sha2-crate","text":"Add sha2 crate to Cargo.toml . One intrinsic: SHA256 \u2014 takes a string, returns its SHA-256 hash as a lowercase hex string Implemented in the same encoding.rs module as base64 (both are encoding/hashing operations). If other hash algorithms are needed later, they follow the same pattern in the same module.","title":"Approach: SHA-256 intrinsic using sha2 crate"},{"location":"plans/2026-02-03-quick-wins-design/#files-changed_4","text":"Cargo.toml \u2014 add sha2 dependency src/eval/stg/encoding.rs \u2014 add Sha256Hash intrinsic src/eval/stg/mod.rs \u2014 register intrinsic src/eval/intrinsics.rs \u2014 catalogue entry lib/prelude.eu \u2014 expose as str.sha256: __SHA256","title":"Files changed"},{"location":"plans/2026-02-03-quick-wins-design/#usage_2","text":"str.sha256(\"hello world\") # => \"b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9\"","title":"Usage"},{"location":"plans/2026-02-03-quick-wins-design/#testing_4","text":"Harness test: known SHA-256 test vector for \"hello world\" Harness test: empty string hash (known value) Harness test: verify output is lowercase hex, 64 characters","title":"Testing"},{"location":"plans/2026-02-03-quick-wins-design/#beads","text":"Existing beads to update with design references: eu-dlr \u2014 Catch {name: name} recursion (section 1) eu-u1m \u2014 min/max for ZDTs (section 2) eu-nbc \u2014 Version assertions (section 3) eu-dyx \u2014 Base64 encoding functions (section 4) eu-dd5 \u2014 Hash built-ins (section 5) All five are independent and can be implemented in any order.","title":"Beads"},{"location":"plans/2026-02-03-stg-case-optimisation-design/","text":"STG Case Optimisation Design Date: 2026-02-03 Overview Four independent optimisations to STG case expression handling, grouped under an STG Optimisation epic alongside the existing eu-bpe (inlining into STG wrappers) bead. The four case optimisations can be implemented in any order. Each is a self-contained change that preserves semantics and is validated by existing harness tests. Optimisations Redundant single-branch case elimination (eu-knx, compile-time) Case-of-known-constructor folding (new, compile-time) O(1) tag dispatch (new, runtime) Skip empty env frames for 0-arity branches (new, runtime) Success criteria All existing harness tests pass after each change No semantic changes to program output Measurable reduction in VM ticks, allocation count, or dispatch overhead for relevant workloads 1. Redundant Single-Branch Case Elimination (eu-knx) Problem Pointless case statements are generated when pattern matching on a constructor with only one branch handling that same constructor: Case scrutinee Constructor(a, b, c) -> body(a, b, c) This should be simplified to direct field access without case analysis. Location New pass in src/eval/stg/optimiser.rs , or extension of the existing AllocationPruner . Transformation Replace the single-branch case with a force (evaluate scrutinee to WHNF) followed by direct binding of the constructor's fields into the body: // Before: Case scrutinee BoxedNumber(x) -> body(x) // After: force(scrutinee, body) The force construct evaluates the scrutinee to WHNF, then the body receives the fields as local bindings -- which is exactly what force already does (a case with empty branches and a fallback that receives the value). For constructors with known arity, the fields are already bound by position in the environment frame, so the body's local references remain valid. When NOT to transform Multiple branches -- not a single-constructor case Fallback present alongside the branch -- suggests the scrutinee's type is not statically known Zero-arity constructors like BoolTrue or ListNil -- already trivial, but could still be simplified (case becomes force + discard) Implementation Add a pass over StgSyn that pattern-matches Case { branches, fallback } where branches.len() == 1 and fallback.is_none() . Replace with the equivalent force + rebinding. Run after the existing AllocationPruner in the optimisation pipeline. Testing Unit test in optimiser.rs : construct a single-branch case, run the pass, verify it produces force All harness tests pass (correctness preservation) Boolean-heavy harness tests should show slight tick reduction 2. Case-of-Known-Constructor Folding Problem When the scrutinee of a case expression is a literal data constructor application (not a variable or function call), we know at compile time which branch will be taken. The entire case can be replaced with the matching branch body. Location Same optimiser pass as the single-branch elimination, or a separate pass in src/eval/stg/optimiser.rs . Transformation // Before: Case DataCon(BoolTrue) BoolTrue -> expr_a BoolFalse -> expr_b // After: expr_a For constructors with arguments: // Before: Case DataCon(BoxedNumber, [42]) BoxedNumber(n) -> body(n) // After: let n = 42 in body(n) When this arises Constant folding after inlining -- an inlined function returns a known constructor, which is then cased on Boolean expressions with literal arguments -- if true then a else b Generated code from the desugarer that constructs then immediately destructs When NOT to transform Scrutinee is a variable reference, function application, or anything requiring evaluation Scrutinee is a let binding wrapping a constructor -- would need to look through the binding (keep it simple, do not chase through lets) Implementation Pattern-match Case { scrutinee, branches, fallback } where scrutinee is StgSyn::Cons { tag, args } (or the equivalent data construction form). Look up tag in branches , substitute args as local bindings in the matched body. If no branch matches, use fallback . If neither matches, leave the case unchanged. Testing Unit test: construct a case with a literal constructor scrutinee, verify folding produces the correct branch body All harness tests pass May not fire often in current code unless combined with inlining, but is a standard compiler transform worth having 3. O(1) Tag Dispatch Problem match_tag() in src/eval/machine/cont.rs:61-68 does a linear scan through branch pairs: pub fn match_tag(tag: Tag, branches: &[(Tag, RefPtr<HeapSyn>)]) -> Option<RefPtr<HeapSyn>> { for (t, body) in branches { if *t == tag { return Some(*body); } } None } Tags are u8 (0-255). With 12 defined constructors and typical branches of 2-3 entries, the linear scan is fast in practice. But this is on the hot path -- every case dispatch calls it. Approach Small inline array indexed by tag range. At continuation construction time, compute min_tag and max_tag from the branches. Allocate an array of size max_tag - min_tag + 1 , populate by tag offset. Lookup is table[tag - min_tag] . For typical cases ( BoolTrue =1/ BoolFalse =2, or ListNil =6/ ListCons =7), this is a 2-entry array with O(1) lookup. Worst case for all 12 constructors is a 12-entry array. Alternatives considered 256-entry array : O(1) but 2KB per continuation. Wasteful for typical 2-3 branch cases. Sorted + binary search : O(log n) but more overhead than linear scan for 2-3 entries. Location src/eval/machine/cont.rs and src/eval/machine/vm.rs . Implementation Change Continuation::Branch to store min_tag: Tag , branch_table: Array<Option<RefPtr<HeapSyn>>> instead of the pairs array. fallback stays unchanged. Construction: in vm.rs where Continuation::Branch is created (line 223-237), build the indexed table from the branch pairs. Lookup: replace match_tag(tag, branches) with bounds check + direct index: if tag >= min_tag && (tag - min_tag) < table.len() { table[(tag - min_tag) as usize] } else { None } . Remove match_tag() function. Risk Low. The change is localised to continuation construction and dispatch. The only subtlety is ensuring the Array allocation for the table goes through the GC-managed heap correctly (it already does for the current pairs array). Testing All harness tests pass (semantic equivalence) Unit test: construct branches with various tag combinations, verify correct dispatch Criterion benchmarks may show slight improvement since case dispatch is on the hot path 4. Skip Empty Env Frames for 0-Arity Branches Problem When a case branch matches a 0-arity constructor ( BoolTrue , BoolFalse , ListNil , Unit ), the current code allocates an environment frame with zero bindings. There is an existing TODO at vm.rs:426 : \"skip empty frames\". let closures = args.iter() // args is empty .map(|r| self.nav(view).resolve(r)) .collect::<Result<Vec<SynClosure>, _>>()?; let len = closures.len(); // len is 0 self.closure = SynClosure::new( body, view.from_closures(closures.into_iter(), len, environment, self.annotation)?, // ^^^ allocates an env frame with 0 bindings ); Location src/eval/machine/vm.rs , in return_data() (around line 418-432). Implementation Before calling from_closures , check if args is empty. If so, reuse the parent environment directly: if args.is_empty() { self.closure = SynClosure::new(body, environment); } else { let closures = args.iter() .map(|r| self.nav(view).resolve(r)) .collect::<Result<Vec<SynClosure>, _>>()?; let len = closures.len(); self.closure = SynClosure::new( body, view.from_closures( closures.into_iter(), len, environment, self.annotation )?, ); } Why this is safe The branch body's local references ( L(0) , L(1) , etc.) refer to bindings in the environment frame. If the constructor has 0 arity, the body will not reference any locally-bound constructor fields -- it can only reference bindings from the enclosing scope, which are already in environment . No new frame is needed. Impact Boolean dispatch is extremely common in eucalypt (every if , and , or , not uses BoolTrue / BoolFalse branches). Each avoided frame saves a heap allocation. This reduces both allocation pressure and GC load. Testing All harness tests pass Boolean-heavy programs should show reduced machine_allocs count in -S output Criterion benchmarks should show reduced allocation overhead Existing Bead: STG Inline into Wrappers (eu-bpe) This bead is underspecified and needs investigation before it can be designed. It remains in the epic as-is. Investigation should identify: What specific wrapper functions exist in the STG implementation Which inlining opportunities would provide meaningful performance gains How inlining interacts with the case optimisations above eu-bpe is not blocked by any of the four case optimisations and can be investigated independently. Beads Existing: eu-knx -- STG: Case optimisation (redundant single-branch elimination). Update description with design from section 1. eu-bpe -- STG: Inline into STG wrappers. Remains as-is pending investigation. New beads to create: Case-of-known-constructor folding -- compile-time, section 2 O(1) tag dispatch -- runtime, section 3 Skip empty env frames for 0-arity branches -- runtime, section 4 STG Optimisation epic -- parent for all five beads","title":"STG Case Optimisation Design"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#stg-case-optimisation-design","text":"Date: 2026-02-03","title":"STG Case Optimisation Design"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#overview","text":"Four independent optimisations to STG case expression handling, grouped under an STG Optimisation epic alongside the existing eu-bpe (inlining into STG wrappers) bead. The four case optimisations can be implemented in any order. Each is a self-contained change that preserves semantics and is validated by existing harness tests.","title":"Overview"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#optimisations","text":"Redundant single-branch case elimination (eu-knx, compile-time) Case-of-known-constructor folding (new, compile-time) O(1) tag dispatch (new, runtime) Skip empty env frames for 0-arity branches (new, runtime)","title":"Optimisations"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#success-criteria","text":"All existing harness tests pass after each change No semantic changes to program output Measurable reduction in VM ticks, allocation count, or dispatch overhead for relevant workloads","title":"Success criteria"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#1-redundant-single-branch-case-elimination-eu-knx","text":"","title":"1. Redundant Single-Branch Case Elimination (eu-knx)"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#problem","text":"Pointless case statements are generated when pattern matching on a constructor with only one branch handling that same constructor: Case scrutinee Constructor(a, b, c) -> body(a, b, c) This should be simplified to direct field access without case analysis.","title":"Problem"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#location","text":"New pass in src/eval/stg/optimiser.rs , or extension of the existing AllocationPruner .","title":"Location"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#transformation","text":"Replace the single-branch case with a force (evaluate scrutinee to WHNF) followed by direct binding of the constructor's fields into the body: // Before: Case scrutinee BoxedNumber(x) -> body(x) // After: force(scrutinee, body) The force construct evaluates the scrutinee to WHNF, then the body receives the fields as local bindings -- which is exactly what force already does (a case with empty branches and a fallback that receives the value). For constructors with known arity, the fields are already bound by position in the environment frame, so the body's local references remain valid.","title":"Transformation"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#when-not-to-transform","text":"Multiple branches -- not a single-constructor case Fallback present alongside the branch -- suggests the scrutinee's type is not statically known Zero-arity constructors like BoolTrue or ListNil -- already trivial, but could still be simplified (case becomes force + discard)","title":"When NOT to transform"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#implementation","text":"Add a pass over StgSyn that pattern-matches Case { branches, fallback } where branches.len() == 1 and fallback.is_none() . Replace with the equivalent force + rebinding. Run after the existing AllocationPruner in the optimisation pipeline.","title":"Implementation"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#testing","text":"Unit test in optimiser.rs : construct a single-branch case, run the pass, verify it produces force All harness tests pass (correctness preservation) Boolean-heavy harness tests should show slight tick reduction","title":"Testing"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#2-case-of-known-constructor-folding","text":"","title":"2. Case-of-Known-Constructor Folding"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#problem_1","text":"When the scrutinee of a case expression is a literal data constructor application (not a variable or function call), we know at compile time which branch will be taken. The entire case can be replaced with the matching branch body.","title":"Problem"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#location_1","text":"Same optimiser pass as the single-branch elimination, or a separate pass in src/eval/stg/optimiser.rs .","title":"Location"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#transformation_1","text":"// Before: Case DataCon(BoolTrue) BoolTrue -> expr_a BoolFalse -> expr_b // After: expr_a For constructors with arguments: // Before: Case DataCon(BoxedNumber, [42]) BoxedNumber(n) -> body(n) // After: let n = 42 in body(n)","title":"Transformation"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#when-this-arises","text":"Constant folding after inlining -- an inlined function returns a known constructor, which is then cased on Boolean expressions with literal arguments -- if true then a else b Generated code from the desugarer that constructs then immediately destructs","title":"When this arises"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#when-not-to-transform_1","text":"Scrutinee is a variable reference, function application, or anything requiring evaluation Scrutinee is a let binding wrapping a constructor -- would need to look through the binding (keep it simple, do not chase through lets)","title":"When NOT to transform"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#implementation_1","text":"Pattern-match Case { scrutinee, branches, fallback } where scrutinee is StgSyn::Cons { tag, args } (or the equivalent data construction form). Look up tag in branches , substitute args as local bindings in the matched body. If no branch matches, use fallback . If neither matches, leave the case unchanged.","title":"Implementation"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#testing_1","text":"Unit test: construct a case with a literal constructor scrutinee, verify folding produces the correct branch body All harness tests pass May not fire often in current code unless combined with inlining, but is a standard compiler transform worth having","title":"Testing"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#3-o1-tag-dispatch","text":"","title":"3. O(1) Tag Dispatch"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#problem_2","text":"match_tag() in src/eval/machine/cont.rs:61-68 does a linear scan through branch pairs: pub fn match_tag(tag: Tag, branches: &[(Tag, RefPtr<HeapSyn>)]) -> Option<RefPtr<HeapSyn>> { for (t, body) in branches { if *t == tag { return Some(*body); } } None } Tags are u8 (0-255). With 12 defined constructors and typical branches of 2-3 entries, the linear scan is fast in practice. But this is on the hot path -- every case dispatch calls it.","title":"Problem"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#approach","text":"Small inline array indexed by tag range. At continuation construction time, compute min_tag and max_tag from the branches. Allocate an array of size max_tag - min_tag + 1 , populate by tag offset. Lookup is table[tag - min_tag] . For typical cases ( BoolTrue =1/ BoolFalse =2, or ListNil =6/ ListCons =7), this is a 2-entry array with O(1) lookup. Worst case for all 12 constructors is a 12-entry array.","title":"Approach"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#alternatives-considered","text":"256-entry array : O(1) but 2KB per continuation. Wasteful for typical 2-3 branch cases. Sorted + binary search : O(log n) but more overhead than linear scan for 2-3 entries.","title":"Alternatives considered"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#location_2","text":"src/eval/machine/cont.rs and src/eval/machine/vm.rs .","title":"Location"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#implementation_2","text":"Change Continuation::Branch to store min_tag: Tag , branch_table: Array<Option<RefPtr<HeapSyn>>> instead of the pairs array. fallback stays unchanged. Construction: in vm.rs where Continuation::Branch is created (line 223-237), build the indexed table from the branch pairs. Lookup: replace match_tag(tag, branches) with bounds check + direct index: if tag >= min_tag && (tag - min_tag) < table.len() { table[(tag - min_tag) as usize] } else { None } . Remove match_tag() function.","title":"Implementation"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#risk","text":"Low. The change is localised to continuation construction and dispatch. The only subtlety is ensuring the Array allocation for the table goes through the GC-managed heap correctly (it already does for the current pairs array).","title":"Risk"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#testing_2","text":"All harness tests pass (semantic equivalence) Unit test: construct branches with various tag combinations, verify correct dispatch Criterion benchmarks may show slight improvement since case dispatch is on the hot path","title":"Testing"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#4-skip-empty-env-frames-for-0-arity-branches","text":"","title":"4. Skip Empty Env Frames for 0-Arity Branches"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#problem_3","text":"When a case branch matches a 0-arity constructor ( BoolTrue , BoolFalse , ListNil , Unit ), the current code allocates an environment frame with zero bindings. There is an existing TODO at vm.rs:426 : \"skip empty frames\". let closures = args.iter() // args is empty .map(|r| self.nav(view).resolve(r)) .collect::<Result<Vec<SynClosure>, _>>()?; let len = closures.len(); // len is 0 self.closure = SynClosure::new( body, view.from_closures(closures.into_iter(), len, environment, self.annotation)?, // ^^^ allocates an env frame with 0 bindings );","title":"Problem"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#location_3","text":"src/eval/machine/vm.rs , in return_data() (around line 418-432).","title":"Location"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#implementation_3","text":"Before calling from_closures , check if args is empty. If so, reuse the parent environment directly: if args.is_empty() { self.closure = SynClosure::new(body, environment); } else { let closures = args.iter() .map(|r| self.nav(view).resolve(r)) .collect::<Result<Vec<SynClosure>, _>>()?; let len = closures.len(); self.closure = SynClosure::new( body, view.from_closures( closures.into_iter(), len, environment, self.annotation )?, ); }","title":"Implementation"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#why-this-is-safe","text":"The branch body's local references ( L(0) , L(1) , etc.) refer to bindings in the environment frame. If the constructor has 0 arity, the body will not reference any locally-bound constructor fields -- it can only reference bindings from the enclosing scope, which are already in environment . No new frame is needed.","title":"Why this is safe"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#impact","text":"Boolean dispatch is extremely common in eucalypt (every if , and , or , not uses BoolTrue / BoolFalse branches). Each avoided frame saves a heap allocation. This reduces both allocation pressure and GC load.","title":"Impact"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#testing_3","text":"All harness tests pass Boolean-heavy programs should show reduced machine_allocs count in -S output Criterion benchmarks should show reduced allocation overhead","title":"Testing"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#existing-bead-stg-inline-into-wrappers-eu-bpe","text":"This bead is underspecified and needs investigation before it can be designed. It remains in the epic as-is. Investigation should identify: What specific wrapper functions exist in the STG implementation Which inlining opportunities would provide meaningful performance gains How inlining interacts with the case optimisations above eu-bpe is not blocked by any of the four case optimisations and can be investigated independently.","title":"Existing Bead: STG Inline into Wrappers (eu-bpe)"},{"location":"plans/2026-02-03-stg-case-optimisation-design/#beads","text":"Existing: eu-knx -- STG: Case optimisation (redundant single-branch elimination). Update description with design from section 1. eu-bpe -- STG: Inline into STG wrappers. Remains as-is pending investigation. New beads to create: Case-of-known-constructor folding -- compile-time, section 2 O(1) tag dispatch -- runtime, section 3 Skip empty env frames for 0-arity branches -- runtime, section 4 STG Optimisation epic -- parent for all five beads","title":"Beads"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/","text":"Benchmarks in Tester \u2014 Design Problem Eucalypt users have no way to measure the performance cost of their code. The tester already captures VM metrics (ticks, allocs, max-stack) in evidence, but these are buried in YAML and not surfaced to the user. Solution Extend the tester to recognise bench- prefixed targets, execute them, and report deterministic VM metrics (ticks, allocs, max-stack) directly to the user. Why VM Metrics, Not Wall-Clock Time Ticks and allocs are deterministic and perfectly reproducible. They're unaffected by system load, require no multiple iterations or warmup, and directly measure algorithmic cost. Wall-clock timing is relevant for runtime/GC engineering (covered by the GC Immix benchmarking infrastructure) but not for users comparing eucalypt implementations. Benchmark Targets Targets prefixed with bench- are recognised as benchmarks, mirroring the existing test- prefix convention: ` { target: :bench-sort } sort-benchmark: { data: range(1000) reverse result: data sort } A file can mix test- and bench- targets freely. Output Stats printed alongside normal test results: BENCH bench-sort (yaml) ticks: 12,340 allocs: 891 max-stack: 42 Correctness Integration If a bench target also has a RESULT key, it's validated as usual. Stats are always reported regardless. Bench targets with RESULT contribute to the overall pass/fail count. Bench targets without RESULT are report-only and don't affect the summary. Implementation Test plan analysis ( testplan.rs ) Recognise bench- prefixed targets alongside test- prefixed ones. Include them in the test plan with a flag marking them as benchmarks. Execution ( tester.rs ) No changes \u2014 bench targets execute exactly like any other target. Statistics are already captured. Validation ( lib/test.eu ) For bench targets, report stats. If RESULT is present, validate it as normal. If absent, skip validation \u2014 the target is report-only. Output After each bench target executes, print the stats block (ticks, allocs, max-stack) to stderr. Normal PASS/FAIL output continues to work for bench targets that have RESULT . HTML report ( lib/test.eu report generation) Include benchmark stats in the existing HTML report, so users get a persistent record alongside test results. Migration of existing bench files The 5 existing files in harness/test/bench/ ( 001_naive_fib.eu through 005_drop_cons.eu ) currently have no bench- prefixed targets. They'd need their targets renamed to bench- prefix to opt into the new reporting. This can be done incrementally. Out of Scope Wall-clock timing (covered by GC Immix benchmarking infrastructure) Baseline comparison / regression detection (separate concern) Multiple iterations / statistical analysis (unnecessary with deterministic metrics)","title":"Benchmarks in Tester \u2014 Design"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#benchmarks-in-tester-design","text":"","title":"Benchmarks in Tester \u2014 Design"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#problem","text":"Eucalypt users have no way to measure the performance cost of their code. The tester already captures VM metrics (ticks, allocs, max-stack) in evidence, but these are buried in YAML and not surfaced to the user.","title":"Problem"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#solution","text":"Extend the tester to recognise bench- prefixed targets, execute them, and report deterministic VM metrics (ticks, allocs, max-stack) directly to the user.","title":"Solution"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#why-vm-metrics-not-wall-clock-time","text":"Ticks and allocs are deterministic and perfectly reproducible. They're unaffected by system load, require no multiple iterations or warmup, and directly measure algorithmic cost. Wall-clock timing is relevant for runtime/GC engineering (covered by the GC Immix benchmarking infrastructure) but not for users comparing eucalypt implementations.","title":"Why VM Metrics, Not Wall-Clock Time"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#benchmark-targets","text":"Targets prefixed with bench- are recognised as benchmarks, mirroring the existing test- prefix convention: ` { target: :bench-sort } sort-benchmark: { data: range(1000) reverse result: data sort } A file can mix test- and bench- targets freely.","title":"Benchmark Targets"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#output","text":"Stats printed alongside normal test results: BENCH bench-sort (yaml) ticks: 12,340 allocs: 891 max-stack: 42","title":"Output"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#correctness-integration","text":"If a bench target also has a RESULT key, it's validated as usual. Stats are always reported regardless. Bench targets with RESULT contribute to the overall pass/fail count. Bench targets without RESULT are report-only and don't affect the summary.","title":"Correctness Integration"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#implementation","text":"","title":"Implementation"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#test-plan-analysis-testplanrs","text":"Recognise bench- prefixed targets alongside test- prefixed ones. Include them in the test plan with a flag marking them as benchmarks.","title":"Test plan analysis (testplan.rs)"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#execution-testerrs","text":"No changes \u2014 bench targets execute exactly like any other target. Statistics are already captured.","title":"Execution (tester.rs)"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#validation-libtesteu","text":"For bench targets, report stats. If RESULT is present, validate it as normal. If absent, skip validation \u2014 the target is report-only.","title":"Validation (lib/test.eu)"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#output_1","text":"After each bench target executes, print the stats block (ticks, allocs, max-stack) to stderr. Normal PASS/FAIL output continues to work for bench targets that have RESULT .","title":"Output"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#html-report-libtesteu-report-generation","text":"Include benchmark stats in the existing HTML report, so users get a persistent record alongside test results.","title":"HTML report (lib/test.eu report generation)"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#migration-of-existing-bench-files","text":"The 5 existing files in harness/test/bench/ ( 001_naive_fib.eu through 005_drop_cons.eu ) currently have no bench- prefixed targets. They'd need their targets renamed to bench- prefix to opt into the new reporting. This can be done incrementally.","title":"Migration of existing bench files"},{"location":"plans/2026-02-04-benchmarks-in-tester-design/#out-of-scope","text":"Wall-clock timing (covered by GC Immix benchmarking infrastructure) Baseline comparison / regression detection (separate concern) Multiple iterations / statistical analysis (unnecessary with deterministic metrics)","title":"Out of Scope"},{"location":"plans/2026-02-04-deep-find-design/","text":"Deep Find \u2014 Data Querying for Eucalypt Problem Navigating deeply nested data in eucalypt requires spelling out full paths. When exploring unfamiliar JSON/YAML or extracting values from complex structures, this is tedious and brittle: data.response.body.items.0.pricing.price Design Decisions No root reference We deliberately do not provide a way to reference the entire top-level namespace (the feature proposed in eu-lt0). Users must use named imports, e.g. data=response.json , keeping the reachability scope bounded. This preserves the compiler's ability to perform dead code elimination \u2014 a root reference would hold everything live and defeat DCE entirely. Since eucalypt intentionally blurs the line between data and code (blocks can contain lambdas, YAML files can contain executable eucalypt), any reference to the root is effectively a reference to all code. Named imports are sufficient and DCE-safe. Library functions, not syntax Deep find is implemented as prelude library functions, not new syntax. This avoids grammar changes, is consistent with eucalypt's existing style, and can be extended incrementally. Note that // syntax would conflict with existing metadata/assertion functionality. Prelude first, intrinsics if needed Initial implementation is pure eucalypt using existing block intrinsics ( elements , keys , recursive walk). Rust intrinsics are a future optimisation only if profiling shows performance problems on realistic data sizes. Phase 1 \u2014 Core Functions Three prelude functions for searching block structures by key name at any depth: deep-find(key) \u2014 returns a list of all values matching the key, depth-first document order deep-find-first(key, default) \u2014 returns the first match, or the default if none found (follows the lookup-or pattern) deep-find-paths(key) \u2014 returns a list of paths to matching keys All operate on a named import or any block value. eu data=response.json -e \"data deep-find(\\\"price\\\")\" # [10, 20, 30] eu data=response.json -e \"data deep-find-first(\\\"email\\\", \\\"unknown\\\")\" # \"alice@example.com\" Traversal Depth-first, document order \u2014 matches appear in the order their keys would be encountered reading the source top to bottom. Phase 2 \u2014 Query String DSL A deep-query function that accepts a simple path expression as a string, parsed at runtime: data deep-query(\"items.*.price\") Query language (minimal) Pattern Meaning foo Match key foo at any depth (same as deep-find ) foo.bar Match bar directly inside foo , at any depth foo.*.bar Match bar inside any direct child of foo foo.**.bar Match bar at any depth inside foo * matches a single level. ** matches zero or more levels. Bare foo is sugar for **.foo . Variants deep-query(pattern) \u2014 list of values deep-query-first(pattern, default) \u2014 first match or default deep-query-paths(pattern) \u2014 list of paths Implementation Phase 2 is a pure eucalypt implementation that parses the pattern string into segments, then uses the phase 1 primitives as building blocks for the recursive walk. Pattern parsing happens on each call \u2014 no caching. A Rust intrinsic (phase 3) could follow if profiling shows the string parsing overhead matters in hot loops. Phase 3 \u2014 Rust Intrinsics (if needed) Only pursued if profiling shows performance problems. Would replace the prelude implementations with equivalent Rust intrinsics registered via the standard intrinsic catalogue. Interaction with Dead Code Elimination These functions do not worsen DCE because they operate on named imports or explicit block values \u2014 the compiler already considers everything reachable from that binding as live. eu-8d8 (dead code elimination inside blocks) remains an independent compiler concern. The deep-find functions are consumers of block data, not a factor in what the compiler can prune. Testing Harness tests with nested JSON/YAML fixtures Edge cases: empty blocks, lists within blocks, no matches, duplicate keys at different depths Performance: test against a moderately large JSON document (~1MB) to establish baseline","title":"Deep Find \u2014 Data Querying for Eucalypt"},{"location":"plans/2026-02-04-deep-find-design/#deep-find-data-querying-for-eucalypt","text":"","title":"Deep Find \u2014 Data Querying for Eucalypt"},{"location":"plans/2026-02-04-deep-find-design/#problem","text":"Navigating deeply nested data in eucalypt requires spelling out full paths. When exploring unfamiliar JSON/YAML or extracting values from complex structures, this is tedious and brittle: data.response.body.items.0.pricing.price","title":"Problem"},{"location":"plans/2026-02-04-deep-find-design/#design-decisions","text":"","title":"Design Decisions"},{"location":"plans/2026-02-04-deep-find-design/#no-root-reference","text":"We deliberately do not provide a way to reference the entire top-level namespace (the feature proposed in eu-lt0). Users must use named imports, e.g. data=response.json , keeping the reachability scope bounded. This preserves the compiler's ability to perform dead code elimination \u2014 a root reference would hold everything live and defeat DCE entirely. Since eucalypt intentionally blurs the line between data and code (blocks can contain lambdas, YAML files can contain executable eucalypt), any reference to the root is effectively a reference to all code. Named imports are sufficient and DCE-safe.","title":"No root reference"},{"location":"plans/2026-02-04-deep-find-design/#library-functions-not-syntax","text":"Deep find is implemented as prelude library functions, not new syntax. This avoids grammar changes, is consistent with eucalypt's existing style, and can be extended incrementally. Note that // syntax would conflict with existing metadata/assertion functionality.","title":"Library functions, not syntax"},{"location":"plans/2026-02-04-deep-find-design/#prelude-first-intrinsics-if-needed","text":"Initial implementation is pure eucalypt using existing block intrinsics ( elements , keys , recursive walk). Rust intrinsics are a future optimisation only if profiling shows performance problems on realistic data sizes.","title":"Prelude first, intrinsics if needed"},{"location":"plans/2026-02-04-deep-find-design/#phase-1-core-functions","text":"Three prelude functions for searching block structures by key name at any depth: deep-find(key) \u2014 returns a list of all values matching the key, depth-first document order deep-find-first(key, default) \u2014 returns the first match, or the default if none found (follows the lookup-or pattern) deep-find-paths(key) \u2014 returns a list of paths to matching keys All operate on a named import or any block value. eu data=response.json -e \"data deep-find(\\\"price\\\")\" # [10, 20, 30] eu data=response.json -e \"data deep-find-first(\\\"email\\\", \\\"unknown\\\")\" # \"alice@example.com\"","title":"Phase 1 \u2014 Core Functions"},{"location":"plans/2026-02-04-deep-find-design/#traversal","text":"Depth-first, document order \u2014 matches appear in the order their keys would be encountered reading the source top to bottom.","title":"Traversal"},{"location":"plans/2026-02-04-deep-find-design/#phase-2-query-string-dsl","text":"A deep-query function that accepts a simple path expression as a string, parsed at runtime: data deep-query(\"items.*.price\")","title":"Phase 2 \u2014 Query String DSL"},{"location":"plans/2026-02-04-deep-find-design/#query-language-minimal","text":"Pattern Meaning foo Match key foo at any depth (same as deep-find ) foo.bar Match bar directly inside foo , at any depth foo.*.bar Match bar inside any direct child of foo foo.**.bar Match bar at any depth inside foo * matches a single level. ** matches zero or more levels. Bare foo is sugar for **.foo .","title":"Query language (minimal)"},{"location":"plans/2026-02-04-deep-find-design/#variants","text":"deep-query(pattern) \u2014 list of values deep-query-first(pattern, default) \u2014 first match or default deep-query-paths(pattern) \u2014 list of paths","title":"Variants"},{"location":"plans/2026-02-04-deep-find-design/#implementation","text":"Phase 2 is a pure eucalypt implementation that parses the pattern string into segments, then uses the phase 1 primitives as building blocks for the recursive walk. Pattern parsing happens on each call \u2014 no caching. A Rust intrinsic (phase 3) could follow if profiling shows the string parsing overhead matters in hot loops.","title":"Implementation"},{"location":"plans/2026-02-04-deep-find-design/#phase-3-rust-intrinsics-if-needed","text":"Only pursued if profiling shows performance problems. Would replace the prelude implementations with equivalent Rust intrinsics registered via the standard intrinsic catalogue.","title":"Phase 3 \u2014 Rust Intrinsics (if needed)"},{"location":"plans/2026-02-04-deep-find-design/#interaction-with-dead-code-elimination","text":"These functions do not worsen DCE because they operate on named imports or explicit block values \u2014 the compiler already considers everything reachable from that binding as live. eu-8d8 (dead code elimination inside blocks) remains an independent compiler concern. The deep-find functions are consumers of block data, not a factor in what the compiler can prune.","title":"Interaction with Dead Code Elimination"},{"location":"plans/2026-02-04-deep-find-design/#testing","text":"Harness tests with nested JSON/YAML fixtures Edge cases: empty blocks, lists within blocks, no matches, duplicate keys at different depths Performance: test against a moderately large JSON document (~1MB) to establish baseline","title":"Testing"},{"location":"plans/2026-02-04-error-message-tests-design/","text":"Error Message Tests \u2014 Design Problem The 24 error test files in harness/test/errors/ have no validation. They trigger errors but nothing checks that the right error was produced. Error messages can silently regress. Solution Sidecar .expect files alongside each error test, containing expected exit code and/or stderr regex. The existing tester infrastructure already captures exit code and stderr in evidence \u2014 this extends validation to check them. Sidecar Format For 006_div_by_zero.eu , create 006_div_by_zero.eu.expect : exit: 1 stderr: \"division by zero\" exit \u2014 integer, exact equality check against process exit code stderr \u2014 regex pattern, matched against the full stderr output Both fields optional; if present, both must pass If no sidecar exists, the test is treated as unvalidated (warning/skip) This approach uses sidecar files for all error tests rather than in-file metadata. This is necessary because some error tests deliberately have broken syntax or malformed metadata \u2014 the parsing failure is the thing being tested, so we cannot rely on reading metadata from the file itself. Integration with Existing Tester The tester pipeline is currently: load plan \u2192 run \u2192 generate evidence \u2192 validate \u2192 summarise. Error tests slot in with minimal changes. Test plan analysis ( testplan.rs ) When analysing a file from errors/ , check for a .expect sidecar. If found, mark the test plan as an error test with the parsed expectations. Execution ( tester.rs ) Run the file exactly as now. The evidence already captures exit , stdout , and stderr . No changes needed here. Validation ( lib/test.eu or Rust-side) For error tests, skip the normal RESULT -based validation. Instead: If exit specified: check evidence.exit == expected If stderr specified: regex match against joined stderr lines Pass only if all specified checks pass Harness runner ( harness_test.rs ) Error tests get generated test functions just like normal tests, so they appear in cargo test output individually. Missing sidecars Error test files without a .expect sidecar are treated as unvalidated \u2014 the test runs but is marked as a warning/skip rather than a pass or fail. This allows incremental migration of the existing 24 error tests. Implementation Plan Phase 1: Sidecar parsing and validation logic Extend testplan.rs to detect .expect files. Add error test validation path in the tester that checks exit code equality and stderr regex matching. Phase 2: Harness integration Generate cargo test functions for error tests in harness_test.rs , so error tests appear alongside normal harness tests. Phase 3: Write .expect sidecars for existing error tests Migrate the ~20 active error tests (excluding x -prefixed disabled ones). Start with a few known-good smoke tests to validate the infrastructure: One parse error (e.g. 014_unterm_strlit.eu ) One runtime error (e.g. 006_div_by_zero.eu ) One assertion failure (e.g. 010_assert.eu ) Then migrate the remainder. Interaction with Benchmarks (eu-01v) Error tests and benchmark targets are orthogonal. .expect sidecars apply only to files in errors/ . bench- prefix targets apply only to non-error test files. If both features are present on the same file, .expect validation takes precedence (error tests exit early, making benchmark stats meaningless). Out of Scope No changes to success test infrastructure No changes to the evidence format (it already captures everything needed) No HTML report changes (error tests show pass/fail like normal tests) No new metadata keys in .eu files","title":"Error Message Tests \u2014 Design"},{"location":"plans/2026-02-04-error-message-tests-design/#error-message-tests-design","text":"","title":"Error Message Tests \u2014 Design"},{"location":"plans/2026-02-04-error-message-tests-design/#problem","text":"The 24 error test files in harness/test/errors/ have no validation. They trigger errors but nothing checks that the right error was produced. Error messages can silently regress.","title":"Problem"},{"location":"plans/2026-02-04-error-message-tests-design/#solution","text":"Sidecar .expect files alongside each error test, containing expected exit code and/or stderr regex. The existing tester infrastructure already captures exit code and stderr in evidence \u2014 this extends validation to check them.","title":"Solution"},{"location":"plans/2026-02-04-error-message-tests-design/#sidecar-format","text":"For 006_div_by_zero.eu , create 006_div_by_zero.eu.expect : exit: 1 stderr: \"division by zero\" exit \u2014 integer, exact equality check against process exit code stderr \u2014 regex pattern, matched against the full stderr output Both fields optional; if present, both must pass If no sidecar exists, the test is treated as unvalidated (warning/skip) This approach uses sidecar files for all error tests rather than in-file metadata. This is necessary because some error tests deliberately have broken syntax or malformed metadata \u2014 the parsing failure is the thing being tested, so we cannot rely on reading metadata from the file itself.","title":"Sidecar Format"},{"location":"plans/2026-02-04-error-message-tests-design/#integration-with-existing-tester","text":"The tester pipeline is currently: load plan \u2192 run \u2192 generate evidence \u2192 validate \u2192 summarise. Error tests slot in with minimal changes.","title":"Integration with Existing Tester"},{"location":"plans/2026-02-04-error-message-tests-design/#test-plan-analysis-testplanrs","text":"When analysing a file from errors/ , check for a .expect sidecar. If found, mark the test plan as an error test with the parsed expectations.","title":"Test plan analysis (testplan.rs)"},{"location":"plans/2026-02-04-error-message-tests-design/#execution-testerrs","text":"Run the file exactly as now. The evidence already captures exit , stdout , and stderr . No changes needed here.","title":"Execution (tester.rs)"},{"location":"plans/2026-02-04-error-message-tests-design/#validation-libtesteu-or-rust-side","text":"For error tests, skip the normal RESULT -based validation. Instead: If exit specified: check evidence.exit == expected If stderr specified: regex match against joined stderr lines Pass only if all specified checks pass","title":"Validation (lib/test.eu or Rust-side)"},{"location":"plans/2026-02-04-error-message-tests-design/#harness-runner-harness_testrs","text":"Error tests get generated test functions just like normal tests, so they appear in cargo test output individually.","title":"Harness runner (harness_test.rs)"},{"location":"plans/2026-02-04-error-message-tests-design/#missing-sidecars","text":"Error test files without a .expect sidecar are treated as unvalidated \u2014 the test runs but is marked as a warning/skip rather than a pass or fail. This allows incremental migration of the existing 24 error tests.","title":"Missing sidecars"},{"location":"plans/2026-02-04-error-message-tests-design/#implementation-plan","text":"","title":"Implementation Plan"},{"location":"plans/2026-02-04-error-message-tests-design/#phase-1-sidecar-parsing-and-validation-logic","text":"Extend testplan.rs to detect .expect files. Add error test validation path in the tester that checks exit code equality and stderr regex matching.","title":"Phase 1: Sidecar parsing and validation logic"},{"location":"plans/2026-02-04-error-message-tests-design/#phase-2-harness-integration","text":"Generate cargo test functions for error tests in harness_test.rs , so error tests appear alongside normal harness tests.","title":"Phase 2: Harness integration"},{"location":"plans/2026-02-04-error-message-tests-design/#phase-3-write-expect-sidecars-for-existing-error-tests","text":"Migrate the ~20 active error tests (excluding x -prefixed disabled ones). Start with a few known-good smoke tests to validate the infrastructure: One parse error (e.g. 014_unterm_strlit.eu ) One runtime error (e.g. 006_div_by_zero.eu ) One assertion failure (e.g. 010_assert.eu ) Then migrate the remainder.","title":"Phase 3: Write .expect sidecars for existing error tests"},{"location":"plans/2026-02-04-error-message-tests-design/#interaction-with-benchmarks-eu-01v","text":"Error tests and benchmark targets are orthogonal. .expect sidecars apply only to files in errors/ . bench- prefix targets apply only to non-error test files. If both features are present on the same file, .expect validation takes precedence (error tests exit early, making benchmark stats meaningless).","title":"Interaction with Benchmarks (eu-01v)"},{"location":"plans/2026-02-04-error-message-tests-design/#out-of-scope","text":"No changes to success test infrastructure No changes to the evidence format (it already captures everything needed) No HTML report changes (error tests show pass/fail like normal tests) No new metadata keys in .eu files","title":"Out of Scope"},{"location":"plans/2026-02-04-sorting-blocks-design/","text":"Sorting Blocks \u2014 Design Problem Users want blocks to render with keys in alphabetical order for clean, deterministic JSON/YAML output. Currently blocks preserve insertion order. Scope This feature is purely about output ordering \u2014 reordering block pairs by key for presentation. Performance improvements (O(1) lookup via indexmap) are deferred to eu-brj. Solution A prelude-level sort-keys function that reorders block pairs alphabetically. This requires string/symbol comparison intrinsics which don't currently exist. String Comparison Intrinsics New intrinsics for lexicographic string comparison: STR_LT \u2014 string less-than STR_GT \u2014 string greater-than STR_LTE \u2014 string less-than-or-equal STR_GTE \u2014 string greater-than-or-equal These accept both strings and symbols, comparing by their underlying text. Implemented in src/eval/stg/string.rs following the pattern of numeric comparison intrinsics. Prelude Functions Exposed inside the str block (consistent with str.split , str.upper , str.base64-encode etc.): str.lt(a, b) \u2014 true if a < b lexicographically str.gt(a, b) \u2014 true if a > b lexicographically str.lte(a, b) \u2014 true if a <= b str.gte(a, b) \u2014 true if a >= b And the main feature: sort-keys(b) \u2014 returns block with pairs reordered by key alphabetically Implementation String comparison intrinsics ( src/eval/stg/string.rs ) Follow the pattern of LT / GT in arith.rs . Accept two arguments, extract as strings or symbols, use Rust's str::cmp() for lexicographic ordering, return boolean. // Pseudocode fn execute(&self, ..., args: &[Ref]) -> Result<(), ExecutionError> { let a = str_or_sym_arg(machine, view, &args[0])?; let b = str_or_sym_arg(machine, view, &args[1])?; let result = a.as_str() < b.as_str(); // or >, <=, >= machine_return_bool(machine, view, result) } Prelude functions ( lib/prelude.eu ) Inside the existing str block in the prelude: str: { # ... existing functions ... lt(a, b): a '__STR_LT(b) gt(a, b): a '__STR_GT(b) lte(a, b): a '__STR_LTE(b) gte(a, b): a '__STR_GTE(b) } sort-keys(b): b elements qsort(pair-key-lt) block pair-key-lt(p1, p2): first(p1) str.lt(first(p2)) Note: str.lt as a higher-order argument ( qsort(str.lt) ) is a static dot access \u2014 DCE correctly tracks it and can still eliminate unused str members. Testing Harness tests for string comparison: empty strings, single chars, unicode, mixed case Harness tests for symbol comparison: :a vs :b , :foo vs :bar Harness tests for sort-keys : empty block, single key, multiple keys, already sorted Note : With polymorphic lt / gt (eu-u1m), sort-keys could also use lt directly. The str.lt variants remain as explicit typed alternatives and are used in sort-keys for clarity. Out of Scope Performance improvements for block lookup (eu-brj) Custom sort order / locale-aware comparison","title":"Sorting Blocks \u2014 Design"},{"location":"plans/2026-02-04-sorting-blocks-design/#sorting-blocks-design","text":"","title":"Sorting Blocks \u2014 Design"},{"location":"plans/2026-02-04-sorting-blocks-design/#problem","text":"Users want blocks to render with keys in alphabetical order for clean, deterministic JSON/YAML output. Currently blocks preserve insertion order.","title":"Problem"},{"location":"plans/2026-02-04-sorting-blocks-design/#scope","text":"This feature is purely about output ordering \u2014 reordering block pairs by key for presentation. Performance improvements (O(1) lookup via indexmap) are deferred to eu-brj.","title":"Scope"},{"location":"plans/2026-02-04-sorting-blocks-design/#solution","text":"A prelude-level sort-keys function that reorders block pairs alphabetically. This requires string/symbol comparison intrinsics which don't currently exist.","title":"Solution"},{"location":"plans/2026-02-04-sorting-blocks-design/#string-comparison-intrinsics","text":"New intrinsics for lexicographic string comparison: STR_LT \u2014 string less-than STR_GT \u2014 string greater-than STR_LTE \u2014 string less-than-or-equal STR_GTE \u2014 string greater-than-or-equal These accept both strings and symbols, comparing by their underlying text. Implemented in src/eval/stg/string.rs following the pattern of numeric comparison intrinsics.","title":"String Comparison Intrinsics"},{"location":"plans/2026-02-04-sorting-blocks-design/#prelude-functions","text":"Exposed inside the str block (consistent with str.split , str.upper , str.base64-encode etc.): str.lt(a, b) \u2014 true if a < b lexicographically str.gt(a, b) \u2014 true if a > b lexicographically str.lte(a, b) \u2014 true if a <= b str.gte(a, b) \u2014 true if a >= b And the main feature: sort-keys(b) \u2014 returns block with pairs reordered by key alphabetically","title":"Prelude Functions"},{"location":"plans/2026-02-04-sorting-blocks-design/#implementation","text":"","title":"Implementation"},{"location":"plans/2026-02-04-sorting-blocks-design/#string-comparison-intrinsics-srcevalstgstringrs","text":"Follow the pattern of LT / GT in arith.rs . Accept two arguments, extract as strings or symbols, use Rust's str::cmp() for lexicographic ordering, return boolean. // Pseudocode fn execute(&self, ..., args: &[Ref]) -> Result<(), ExecutionError> { let a = str_or_sym_arg(machine, view, &args[0])?; let b = str_or_sym_arg(machine, view, &args[1])?; let result = a.as_str() < b.as_str(); // or >, <=, >= machine_return_bool(machine, view, result) }","title":"String comparison intrinsics (src/eval/stg/string.rs)"},{"location":"plans/2026-02-04-sorting-blocks-design/#prelude-functions-libpreludeeu","text":"Inside the existing str block in the prelude: str: { # ... existing functions ... lt(a, b): a '__STR_LT(b) gt(a, b): a '__STR_GT(b) lte(a, b): a '__STR_LTE(b) gte(a, b): a '__STR_GTE(b) } sort-keys(b): b elements qsort(pair-key-lt) block pair-key-lt(p1, p2): first(p1) str.lt(first(p2)) Note: str.lt as a higher-order argument ( qsort(str.lt) ) is a static dot access \u2014 DCE correctly tracks it and can still eliminate unused str members.","title":"Prelude functions (lib/prelude.eu)"},{"location":"plans/2026-02-04-sorting-blocks-design/#testing","text":"Harness tests for string comparison: empty strings, single chars, unicode, mixed case Harness tests for symbol comparison: :a vs :b , :foo vs :bar Harness tests for sort-keys : empty block, single key, multiple keys, already sorted Note : With polymorphic lt / gt (eu-u1m), sort-keys could also use lt directly. The str.lt variants remain as explicit typed alternatives and are used in sort-keys for clarity.","title":"Testing"},{"location":"plans/2026-02-04-sorting-blocks-design/#out-of-scope","text":"Performance improvements for block lookup (eu-brj) Custom sort order / locale-aware comparison","title":"Out of Scope"},{"location":"plans/2026-02-04-string-intrinsics-design/","text":"String Intrinsics Optimisation \u2014 Design Problem String intrinsics that return lists (SPLIT, MATCH, MATCHES, LETTERS) materialise the entire result as a Vec<String> in Rust, then copy every string onto the heap. This creates N intermediate Rust String allocations that are immediately discarded after being copied to the heap. Approach: Two Phases Phase 1 (now): Eliminate the intermediate Vec<String> by providing a streaming return path. Instead of collecting all results into a Vec then building the heap list, build the heap list directly from an iterator. The input string is still copied from the heap into Rust via str_arg() . Phase 2 (after GC evacuation): Add a block pinning mechanism to the GC, then introduce str_arg_ref() which returns a &str borrowed directly from the heap. This eliminates the input copy. Requires pinning because evacuation (GC phase 3) can move objects, and concurrent mutators could trigger GC while the intrinsic holds the borrow. Why Not Zero-Copy Now? During intrinsic execution, output allocations can trigger GC. With the planned evacuation optimisation (eu-181d phase 3), GC could move the input string while the intrinsic holds a &str into it. A pinning mechanism is needed to prevent this, and that belongs in the GC epic. Future concurrency would also require pinning for the same reason. Phase 1: Streaming Return New helper: machine_return_str_iter() Replace the current machine_return_str_list(machine, view, vec) pattern with machine_return_str_iter(machine, view, iter) that accepts an iterator and builds the heap cons list directly. The current machine_return_str_list builds the list in reverse from a Vec. The streaming version collects the iterator into a temporary Vec<Ref> of heap-allocated string refs (small pointers, not full strings), then builds the cons list in reverse as now. This eliminates the Vec<String> \u2014 the strings go straight from the iterator to the heap. Intrinsic migration Intrinsic Current After SPLIT re.split().map(to_string).collect() then machine_return_str_list(vec) re.split() fed to machine_return_str_iter(iter) MATCH re.captures().map(to_string).collect() then machine_return_str_list(vec) re.captures() fed to machine_return_str_iter(iter) MATCHES re.find_iter().map(to_string).collect() then machine_return_str_list(vec) re.find_iter() fed to machine_return_str_iter(iter) LETTERS chars().map(to_string).collect() then machine_return_str_list(vec) chars() fed to machine_return_str_iter(iter) JOIN, UPPER, LOWER, FMT, STR produce single strings \u2014 no change needed. Phase 2: Zero-Copy Input (After GC Epic) Block pinning mechanism Add a pin/unpin API to MutatorHeapView : pin(ref) returns a PinGuard \u2014 marks the ref's block as non-evacuatable Guard unpins on drop (RAII pattern) Evacuation skips pinned blocks New helper: str_arg_ref() Returns &str borrowed directly from the heap string, with the block pinned for the duration. The intrinsic flow becomes: pin input block \u2192 &str from heap \u2192 regex \u2192 &str slices \u2192 heap allocations \u2192 drop pin Zero intermediate Rust String allocations for the entire pipeline. Dependency: Blocked by GC Immix phase 3 (evacuation), since pinning is only necessary once objects can move. Decision needed : The pinning API must be designed explicitly before phase 2 work begins. The GC Immix design does not yet specify the pin/unpin API. Key questions: RAII guard lifetime, nesting semantics (can an intrinsic allocate while holding a pin?), interaction with evacuation's reference-updating scan. Out of Scope JOIN optimisation (single string output, already efficient) UPPER/LOWER/FMT/STR (single string output) General heap-backed allocation for Rust structs (separate future concern) Testing Phase 1: Existing harness tests cover all string intrinsics. Verify no regressions. Use bench targets (once eu-01v lands) to measure ticks/allocs reduction. Phase 2: GC stress tests with string-heavy workloads to verify pinning works under evacuation.","title":"String Intrinsics Optimisation \u2014 Design"},{"location":"plans/2026-02-04-string-intrinsics-design/#string-intrinsics-optimisation-design","text":"","title":"String Intrinsics Optimisation \u2014 Design"},{"location":"plans/2026-02-04-string-intrinsics-design/#problem","text":"String intrinsics that return lists (SPLIT, MATCH, MATCHES, LETTERS) materialise the entire result as a Vec<String> in Rust, then copy every string onto the heap. This creates N intermediate Rust String allocations that are immediately discarded after being copied to the heap.","title":"Problem"},{"location":"plans/2026-02-04-string-intrinsics-design/#approach-two-phases","text":"Phase 1 (now): Eliminate the intermediate Vec<String> by providing a streaming return path. Instead of collecting all results into a Vec then building the heap list, build the heap list directly from an iterator. The input string is still copied from the heap into Rust via str_arg() . Phase 2 (after GC evacuation): Add a block pinning mechanism to the GC, then introduce str_arg_ref() which returns a &str borrowed directly from the heap. This eliminates the input copy. Requires pinning because evacuation (GC phase 3) can move objects, and concurrent mutators could trigger GC while the intrinsic holds the borrow.","title":"Approach: Two Phases"},{"location":"plans/2026-02-04-string-intrinsics-design/#why-not-zero-copy-now","text":"During intrinsic execution, output allocations can trigger GC. With the planned evacuation optimisation (eu-181d phase 3), GC could move the input string while the intrinsic holds a &str into it. A pinning mechanism is needed to prevent this, and that belongs in the GC epic. Future concurrency would also require pinning for the same reason.","title":"Why Not Zero-Copy Now?"},{"location":"plans/2026-02-04-string-intrinsics-design/#phase-1-streaming-return","text":"","title":"Phase 1: Streaming Return"},{"location":"plans/2026-02-04-string-intrinsics-design/#new-helper-machine_return_str_iter","text":"Replace the current machine_return_str_list(machine, view, vec) pattern with machine_return_str_iter(machine, view, iter) that accepts an iterator and builds the heap cons list directly. The current machine_return_str_list builds the list in reverse from a Vec. The streaming version collects the iterator into a temporary Vec<Ref> of heap-allocated string refs (small pointers, not full strings), then builds the cons list in reverse as now. This eliminates the Vec<String> \u2014 the strings go straight from the iterator to the heap.","title":"New helper: machine_return_str_iter()"},{"location":"plans/2026-02-04-string-intrinsics-design/#intrinsic-migration","text":"Intrinsic Current After SPLIT re.split().map(to_string).collect() then machine_return_str_list(vec) re.split() fed to machine_return_str_iter(iter) MATCH re.captures().map(to_string).collect() then machine_return_str_list(vec) re.captures() fed to machine_return_str_iter(iter) MATCHES re.find_iter().map(to_string).collect() then machine_return_str_list(vec) re.find_iter() fed to machine_return_str_iter(iter) LETTERS chars().map(to_string).collect() then machine_return_str_list(vec) chars() fed to machine_return_str_iter(iter) JOIN, UPPER, LOWER, FMT, STR produce single strings \u2014 no change needed.","title":"Intrinsic migration"},{"location":"plans/2026-02-04-string-intrinsics-design/#phase-2-zero-copy-input-after-gc-epic","text":"","title":"Phase 2: Zero-Copy Input (After GC Epic)"},{"location":"plans/2026-02-04-string-intrinsics-design/#block-pinning-mechanism","text":"Add a pin/unpin API to MutatorHeapView : pin(ref) returns a PinGuard \u2014 marks the ref's block as non-evacuatable Guard unpins on drop (RAII pattern) Evacuation skips pinned blocks","title":"Block pinning mechanism"},{"location":"plans/2026-02-04-string-intrinsics-design/#new-helper-str_arg_ref","text":"Returns &str borrowed directly from the heap string, with the block pinned for the duration. The intrinsic flow becomes: pin input block \u2192 &str from heap \u2192 regex \u2192 &str slices \u2192 heap allocations \u2192 drop pin Zero intermediate Rust String allocations for the entire pipeline. Dependency: Blocked by GC Immix phase 3 (evacuation), since pinning is only necessary once objects can move. Decision needed : The pinning API must be designed explicitly before phase 2 work begins. The GC Immix design does not yet specify the pin/unpin API. Key questions: RAII guard lifetime, nesting semantics (can an intrinsic allocate while holding a pin?), interaction with evacuation's reference-updating scan.","title":"New helper: str_arg_ref()"},{"location":"plans/2026-02-04-string-intrinsics-design/#out-of-scope","text":"JOIN optimisation (single string output, already efficient) UPPER/LOWER/FMT/STR (single string output) General heap-backed allocation for Rust structs (separate future concern)","title":"Out of Scope"},{"location":"plans/2026-02-04-string-intrinsics-design/#testing","text":"Phase 1: Existing harness tests cover all string intrinsics. Verify no regressions. Use bench targets (once eu-01v lands) to measure ticks/allocs reduction. Phase 2: GC stress tests with string-heavy workloads to verify pinning works under evacuation.","title":"Testing"},{"location":"plans/2026-02-05-block-indexing-design/","text":"Block Indexing \u2014 Design Problem Block lookup is O(n) \u2014 LOOKUPOR walks a cons-list comparing keys on each access. For medium/large blocks with repeated lookups, this is slow. Solution Lazy indexing \u2014 blocks build a hash index on first lookup if they exceed a size threshold. Index is stored in the heap alongside the block. Key Properties Transparent to users \u2014 no API changes Construction unchanged \u2014 blocks still built as lists Small blocks (< threshold) use linear search \u2014 fast enough Large blocks build index on first lookup \u2014 O(1) thereafter Purely functional \u2014 new blocks from merge start without index Depends on eu-4af (symbol interning) for efficient HashMap<SymbolId, usize> Heap Representation Current Block structure : Block(list) \u2014 single field pointing to cons-list of key-value pairs New Block structure : Block(list, index) \u2014 two fields: - list: cons-list of key-value pairs (unchanged) - index: null OR pointer to NativeIndex NativeIndex : A new Native variant holding HashMap<SymbolId, usize> : - Key: interned symbol ID - Value: position in the list (0-indexed) Threshold : 16 keys. Blocks with fewer than 16 elements use linear search; 16+ elements trigger index building on first lookup. Index Building On first LOOKUPOR call, if block has 16+ elements and no index: Walk the list once, recording (symbol_id \u2192 position) mappings Allocate NativeIndex on heap Store reference in block's index field Use index for this and subsequent lookups LOOKUPOR Implementation Current flow (in src/eval/stg/block.rs ): 1. Unbox the key symbol 2. Walk list with MatchesKey on each element 3. Return value when found, or default if exhausted New flow : 1. Check if block has index 2. If indexed: hash lookup \u2192 position \u2192 fetch value at position 3. If not indexed: - If size < threshold: linear search (as now) - If size >= threshold: build index, store in block, then hash lookup Position-based access : The index maps symbol \u2192 position. Initial implementation walks list to position. Array-based optimisation can follow if profiling shows benefit. LOOKUP : Delegates to LOOKUPOR with a panic default, so gets indexing for free. Dependencies eu-4af (symbol interning) \u2014 required for HashMap<SymbolId, usize> . Without interning, we'd hash strings which is slower. Testing Correctness : Existing harness tests should pass unchanged \u2014 behaviour is identical, only performance differs. Threshold boundary : Test blocks with 15, 16, 17 keys. Repeated lookups : Verify second lookup is faster (index built). Miss behaviour : Lookup of non-existent key returns default. After merge : Merged block lookups work (new block, no index). Benchmarking Lookup time for blocks of various sizes (10, 50, 100, 500 keys) Single lookup vs repeated lookups on same block Before/after comparison for representative workloads Out of Scope Array-based positional access (future optimisation) Index for blocks below threshold Explicit IOSM API (prelude already has iosm namespace for explicit use)","title":"Block Indexing \u2014 Design"},{"location":"plans/2026-02-05-block-indexing-design/#block-indexing-design","text":"","title":"Block Indexing \u2014 Design"},{"location":"plans/2026-02-05-block-indexing-design/#problem","text":"Block lookup is O(n) \u2014 LOOKUPOR walks a cons-list comparing keys on each access. For medium/large blocks with repeated lookups, this is slow.","title":"Problem"},{"location":"plans/2026-02-05-block-indexing-design/#solution","text":"Lazy indexing \u2014 blocks build a hash index on first lookup if they exceed a size threshold. Index is stored in the heap alongside the block.","title":"Solution"},{"location":"plans/2026-02-05-block-indexing-design/#key-properties","text":"Transparent to users \u2014 no API changes Construction unchanged \u2014 blocks still built as lists Small blocks (< threshold) use linear search \u2014 fast enough Large blocks build index on first lookup \u2014 O(1) thereafter Purely functional \u2014 new blocks from merge start without index Depends on eu-4af (symbol interning) for efficient HashMap<SymbolId, usize>","title":"Key Properties"},{"location":"plans/2026-02-05-block-indexing-design/#heap-representation","text":"Current Block structure : Block(list) \u2014 single field pointing to cons-list of key-value pairs New Block structure : Block(list, index) \u2014 two fields: - list: cons-list of key-value pairs (unchanged) - index: null OR pointer to NativeIndex NativeIndex : A new Native variant holding HashMap<SymbolId, usize> : - Key: interned symbol ID - Value: position in the list (0-indexed) Threshold : 16 keys. Blocks with fewer than 16 elements use linear search; 16+ elements trigger index building on first lookup.","title":"Heap Representation"},{"location":"plans/2026-02-05-block-indexing-design/#index-building","text":"On first LOOKUPOR call, if block has 16+ elements and no index: Walk the list once, recording (symbol_id \u2192 position) mappings Allocate NativeIndex on heap Store reference in block's index field Use index for this and subsequent lookups","title":"Index Building"},{"location":"plans/2026-02-05-block-indexing-design/#lookupor-implementation","text":"Current flow (in src/eval/stg/block.rs ): 1. Unbox the key symbol 2. Walk list with MatchesKey on each element 3. Return value when found, or default if exhausted New flow : 1. Check if block has index 2. If indexed: hash lookup \u2192 position \u2192 fetch value at position 3. If not indexed: - If size < threshold: linear search (as now) - If size >= threshold: build index, store in block, then hash lookup Position-based access : The index maps symbol \u2192 position. Initial implementation walks list to position. Array-based optimisation can follow if profiling shows benefit. LOOKUP : Delegates to LOOKUPOR with a panic default, so gets indexing for free.","title":"LOOKUPOR Implementation"},{"location":"plans/2026-02-05-block-indexing-design/#dependencies","text":"eu-4af (symbol interning) \u2014 required for HashMap<SymbolId, usize> . Without interning, we'd hash strings which is slower.","title":"Dependencies"},{"location":"plans/2026-02-05-block-indexing-design/#testing","text":"Correctness : Existing harness tests should pass unchanged \u2014 behaviour is identical, only performance differs. Threshold boundary : Test blocks with 15, 16, 17 keys. Repeated lookups : Verify second lookup is faster (index built). Miss behaviour : Lookup of non-existent key returns default. After merge : Merged block lookups work (new block, no index).","title":"Testing"},{"location":"plans/2026-02-05-block-indexing-design/#benchmarking","text":"Lookup time for blocks of various sizes (10, 50, 100, 500 keys) Single lookup vs repeated lookups on same block Before/after comparison for representative workloads","title":"Benchmarking"},{"location":"plans/2026-02-05-block-indexing-design/#out-of-scope","text":"Array-based positional access (future optimisation) Index for blocks below threshold Explicit IOSM API (prelude already has iosm namespace for explicit use)","title":"Out of Scope"},{"location":"plans/2026-02-05-dce-inside-blocks-design/","text":"DCE Inside Blocks \u2014 Design Problem Dead code elimination currently marks ALL block members as reachable. For namespace-style blocks like str and cal , this means every function is retained even when only a few are used. This bloats the compiled program. Solution Static-access DCE \u2014 track which block members are accessed via static dot syntax ( ns.member ) and eliminate unreferenced members when only static access patterns are used. Key Properties Conservative \u2014 only eliminate when provably safe Static access only \u2014 ns.member patterns, not dynamic lookup Namespace focus \u2014 greatest benefit for prelude-style blocks Opt-out available \u2014 --no-dce flag for debugging No new errors \u2014 purely an optimisation, same semantics Syntax Examples # Before DCE: both `used` and `unused` retained ns: { used: 42, unused: 99 } result: ns.used # After DCE: only `used` retained ns: { used: 42 } result: ns.used Safe vs Unsafe Patterns Safe patterns (DCE applies): Pattern Example Reasoning Static dot access ns.member Known member at compile time Chained access ns.foo.bar Still statically resolvable Binding let x: ns.member Static access in binding Unsafe patterns (preserve all members): Pattern Example Reasoning Dynamic lookup ns lookup(key) Key unknown at compile time Iteration ns elements All members potentially accessed Merge ns merge(other) Block structure modified Passed as argument f(ns) Unknown usage in callee Block as value ns alone Entire block escapes Implementation Location : Extend src/core/simplify/prune.rs New component : BlockAccessTracker struct BlockAccessTracker { /// Blocks that have ONLY static access patterns static_only: HashSet<LocalId>, /// For static-only blocks: which members are accessed accessed_members: HashMap<LocalId, HashSet<String>>, // Note: uses String keys (pre-interning identifiers in the core // AST). SymbolId is a runtime/VM concept; DCE runs at compile time // before symbols are interned. } Algorithm : First pass : Identify blocks and track access patterns Mark block as \"static-only\" initially On block.member access: record member in accessed_members On dynamic access: remove block from static_only set Second pass : Mark reachability For static-only blocks: only mark accessed members as reachable For other blocks: mark all members reachable (current behaviour) Integration with existing DCE : The current prune.rs has mark_block_reachable which marks ALL members. Add a check: fn mark_block_reachable(&mut self, block_id: LocalId) { if self.tracker.is_static_only(block_id) { // Only mark accessed members for member in self.tracker.accessed_members(block_id) { self.mark_reachable(member); } } else { // Current behaviour: mark all members for member in block.all_members() { self.mark_reachable(member); } } } Edge Cases Nested blocks : outer.inner.member \u2014 track access through the full chain. If outer is static-only and inner is accessed, then check if inner is also static-only. Shadowing : let ns: other \u2014 stop tracking original ns from that point. The shadowed binding may have different access patterns. Re-export : { x: ns.foo } \u2014 foo is marked as used in ns . Other members of ns can be eliminated if no other access exists. Testing Correctness tests (harness): - Programs produce same results with DCE enabled - Dynamic access patterns preserve all members # Static access - DCE safe test-dce-static: { ns: { used: 42, unused: 99 } result: ns.used RESULT: 42 } # Dynamic access - must preserve all test-dce-dynamic: { ns: { a: 1, b: 2 } key: :a result: ns lookup(key) RESULT: 1 } # Block escapes - must preserve all test-dce-escape: { ns: { x: 1, y: 2 } result: ns elements length RESULT: 2 } Verification : Correctness tests plus code review. A --debug-dce flag can be added later to print eliminated members if needed. Error Handling No new errors \u2014 DCE is purely an optimisation If analysis is uncertain, default to preserving members (safe fallback) Feature flag --no-dce disables optimisation for debugging Out of Scope DCE for dynamically accessed blocks (would need runtime analysis) Cross-module DCE (only same compilation unit) Metrics/statistics collection (avoid performance impact) Automatic verification of elimination (trust + code review) Dependencies None \u2014 can be implemented independently.","title":"DCE Inside Blocks \u2014 Design"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#dce-inside-blocks-design","text":"","title":"DCE Inside Blocks \u2014 Design"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#problem","text":"Dead code elimination currently marks ALL block members as reachable. For namespace-style blocks like str and cal , this means every function is retained even when only a few are used. This bloats the compiled program.","title":"Problem"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#solution","text":"Static-access DCE \u2014 track which block members are accessed via static dot syntax ( ns.member ) and eliminate unreferenced members when only static access patterns are used.","title":"Solution"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#key-properties","text":"Conservative \u2014 only eliminate when provably safe Static access only \u2014 ns.member patterns, not dynamic lookup Namespace focus \u2014 greatest benefit for prelude-style blocks Opt-out available \u2014 --no-dce flag for debugging No new errors \u2014 purely an optimisation, same semantics","title":"Key Properties"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#syntax-examples","text":"# Before DCE: both `used` and `unused` retained ns: { used: 42, unused: 99 } result: ns.used # After DCE: only `used` retained ns: { used: 42 } result: ns.used","title":"Syntax Examples"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#safe-vs-unsafe-patterns","text":"Safe patterns (DCE applies): Pattern Example Reasoning Static dot access ns.member Known member at compile time Chained access ns.foo.bar Still statically resolvable Binding let x: ns.member Static access in binding Unsafe patterns (preserve all members): Pattern Example Reasoning Dynamic lookup ns lookup(key) Key unknown at compile time Iteration ns elements All members potentially accessed Merge ns merge(other) Block structure modified Passed as argument f(ns) Unknown usage in callee Block as value ns alone Entire block escapes","title":"Safe vs Unsafe Patterns"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#implementation","text":"Location : Extend src/core/simplify/prune.rs New component : BlockAccessTracker struct BlockAccessTracker { /// Blocks that have ONLY static access patterns static_only: HashSet<LocalId>, /// For static-only blocks: which members are accessed accessed_members: HashMap<LocalId, HashSet<String>>, // Note: uses String keys (pre-interning identifiers in the core // AST). SymbolId is a runtime/VM concept; DCE runs at compile time // before symbols are interned. } Algorithm : First pass : Identify blocks and track access patterns Mark block as \"static-only\" initially On block.member access: record member in accessed_members On dynamic access: remove block from static_only set Second pass : Mark reachability For static-only blocks: only mark accessed members as reachable For other blocks: mark all members reachable (current behaviour) Integration with existing DCE : The current prune.rs has mark_block_reachable which marks ALL members. Add a check: fn mark_block_reachable(&mut self, block_id: LocalId) { if self.tracker.is_static_only(block_id) { // Only mark accessed members for member in self.tracker.accessed_members(block_id) { self.mark_reachable(member); } } else { // Current behaviour: mark all members for member in block.all_members() { self.mark_reachable(member); } } }","title":"Implementation"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#edge-cases","text":"Nested blocks : outer.inner.member \u2014 track access through the full chain. If outer is static-only and inner is accessed, then check if inner is also static-only. Shadowing : let ns: other \u2014 stop tracking original ns from that point. The shadowed binding may have different access patterns. Re-export : { x: ns.foo } \u2014 foo is marked as used in ns . Other members of ns can be eliminated if no other access exists.","title":"Edge Cases"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#testing","text":"Correctness tests (harness): - Programs produce same results with DCE enabled - Dynamic access patterns preserve all members # Static access - DCE safe test-dce-static: { ns: { used: 42, unused: 99 } result: ns.used RESULT: 42 } # Dynamic access - must preserve all test-dce-dynamic: { ns: { a: 1, b: 2 } key: :a result: ns lookup(key) RESULT: 1 } # Block escapes - must preserve all test-dce-escape: { ns: { x: 1, y: 2 } result: ns elements length RESULT: 2 } Verification : Correctness tests plus code review. A --debug-dce flag can be added later to print eliminated members if needed.","title":"Testing"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#error-handling","text":"No new errors \u2014 DCE is purely an optimisation If analysis is uncertain, default to preserving members (safe fallback) Feature flag --no-dce disables optimisation for debugging","title":"Error Handling"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#out-of-scope","text":"DCE for dynamically accessed blocks (would need runtime analysis) Cross-module DCE (only same compilation unit) Metrics/statistics collection (avoid performance impact) Automatic verification of elimination (trust + code review)","title":"Out of Scope"},{"location":"plans/2026-02-05-dce-inside-blocks-design/#dependencies","text":"None \u2014 can be implemented independently.","title":"Dependencies"},{"location":"plans/2026-02-05-prelude-docs-design/","text":"Prelude Docs Generator \u2014 Design Problem The prelude has ~176 documented items but no way to render them as user-facing documentation. Users can't easily discover available functions. Solution A general-purpose documentation generator written in eucalypt that extracts metadata from any eucalypt code and renders it as Markdown. Key Properties Eucalypt program \u2014 \"eat your own dog food\", demonstrates capabilities General-purpose \u2014 works on any eucalypt code, not just prelude Markdown output \u2014 viewable anywhere, convertible to HTML later Single file \u2014 one markdown file with sections for namespaces Structured metadata \u2014 recognizes doc: , export: , example: , see-also: Omit undocumented \u2014 only documented items appear in output Usage # Document the prelude eu lib/doc-gen.eu target=lib/prelude.eu -f text > prelude-reference.md # Document a user library eu lib/doc-gen.eu target=my-library.eu -f text > my-library-docs.md Metadata Format Recognized metadata keys : Key Purpose Example doc: Description ` { doc: \"Map function over list\" } export: :suppress Hide from docs ` { export: :suppress } example: Usage example ` { example: \"[1,2,3] map(_ * 2)\" } see-also: Related functions ` { see-also: [:filter, :fold] } Shorthand : ` \"description\" is equivalent to ` { doc: \"description\" } Nested blocks : Blocks within blocks become sections. cal.parse appears under a \"cal\" heading. Output Format # Library Reference ## cal Calendar functions ### cal.parse Parse ISO8601 date string **Example:** ```eu cal.parse(\"2024-01-15\") See also: cal.format str String functions str.upper ... **Rendering rules**: - Top-level documented items get H2 headings - Nested items (e.g., `cal.parse`) get H3 under their parent's H2 - Doc string becomes description paragraph - `example:` renders as fenced code block - `see-also:` renders as links (anchor format) - Items sorted alphabetically within sections ## Implementation **Location**: `lib/doc-gen.eu` **Core functions**: ```eu ` { doc: \"Generate markdown documentation for a block\" } generate-docs(target): ... ` { doc: \"Extract documented items from a block\" } extract-docs(b): b elements filter(has-doc?) map(extract-item) ` { doc: \"Check if item has documentation (not suppressed)\" } has-doc?(kv): kv second meta has(:doc) and(not(is-suppressed?(kv))) ` { doc: \"Check if item is suppressed\" } is-suppressed?(kv): kv second meta lookup-or(:export, :public) eq(:suppress) ` { doc: \"Render item as markdown\" } render-item(name, m): ... Intrinsics used : meta(x) \u2014 get metadata block for value elements(b) \u2014 get key-value pairs from block lookup-or \u2014 safe metadata access Dependencies : Only prelude functions \u2014 no new intrinsics needed. Testing Harness test with fixture file containing various metadata patterns Verify output contains expected sections and formatting Test nested blocks render correctly Test suppressed items are omitted Out of Scope Multiple output files (future enhancement) HTML generation (use mdbook or similar) Automatic signature extraction (would need type inference) Interactive examples (would need eval capability)","title":"Prelude Docs Generator \u2014 Design"},{"location":"plans/2026-02-05-prelude-docs-design/#prelude-docs-generator-design","text":"","title":"Prelude Docs Generator \u2014 Design"},{"location":"plans/2026-02-05-prelude-docs-design/#problem","text":"The prelude has ~176 documented items but no way to render them as user-facing documentation. Users can't easily discover available functions.","title":"Problem"},{"location":"plans/2026-02-05-prelude-docs-design/#solution","text":"A general-purpose documentation generator written in eucalypt that extracts metadata from any eucalypt code and renders it as Markdown.","title":"Solution"},{"location":"plans/2026-02-05-prelude-docs-design/#key-properties","text":"Eucalypt program \u2014 \"eat your own dog food\", demonstrates capabilities General-purpose \u2014 works on any eucalypt code, not just prelude Markdown output \u2014 viewable anywhere, convertible to HTML later Single file \u2014 one markdown file with sections for namespaces Structured metadata \u2014 recognizes doc: , export: , example: , see-also: Omit undocumented \u2014 only documented items appear in output","title":"Key Properties"},{"location":"plans/2026-02-05-prelude-docs-design/#usage","text":"# Document the prelude eu lib/doc-gen.eu target=lib/prelude.eu -f text > prelude-reference.md # Document a user library eu lib/doc-gen.eu target=my-library.eu -f text > my-library-docs.md","title":"Usage"},{"location":"plans/2026-02-05-prelude-docs-design/#metadata-format","text":"Recognized metadata keys : Key Purpose Example doc: Description ` { doc: \"Map function over list\" } export: :suppress Hide from docs ` { export: :suppress } example: Usage example ` { example: \"[1,2,3] map(_ * 2)\" } see-also: Related functions ` { see-also: [:filter, :fold] } Shorthand : ` \"description\" is equivalent to ` { doc: \"description\" } Nested blocks : Blocks within blocks become sections. cal.parse appears under a \"cal\" heading.","title":"Metadata Format"},{"location":"plans/2026-02-05-prelude-docs-design/#output-format","text":"# Library Reference ## cal Calendar functions ### cal.parse Parse ISO8601 date string **Example:** ```eu cal.parse(\"2024-01-15\") See also: cal.format","title":"Output Format"},{"location":"plans/2026-02-05-prelude-docs-design/#str","text":"String functions","title":"str"},{"location":"plans/2026-02-05-prelude-docs-design/#strupper","text":"... **Rendering rules**: - Top-level documented items get H2 headings - Nested items (e.g., `cal.parse`) get H3 under their parent's H2 - Doc string becomes description paragraph - `example:` renders as fenced code block - `see-also:` renders as links (anchor format) - Items sorted alphabetically within sections ## Implementation **Location**: `lib/doc-gen.eu` **Core functions**: ```eu ` { doc: \"Generate markdown documentation for a block\" } generate-docs(target): ... ` { doc: \"Extract documented items from a block\" } extract-docs(b): b elements filter(has-doc?) map(extract-item) ` { doc: \"Check if item has documentation (not suppressed)\" } has-doc?(kv): kv second meta has(:doc) and(not(is-suppressed?(kv))) ` { doc: \"Check if item is suppressed\" } is-suppressed?(kv): kv second meta lookup-or(:export, :public) eq(:suppress) ` { doc: \"Render item as markdown\" } render-item(name, m): ... Intrinsics used : meta(x) \u2014 get metadata block for value elements(b) \u2014 get key-value pairs from block lookup-or \u2014 safe metadata access Dependencies : Only prelude functions \u2014 no new intrinsics needed.","title":"str.upper"},{"location":"plans/2026-02-05-prelude-docs-design/#testing","text":"Harness test with fixture file containing various metadata patterns Verify output contains expected sections and formatting Test nested blocks render correctly Test suppressed items are omitted","title":"Testing"},{"location":"plans/2026-02-05-prelude-docs-design/#out-of-scope","text":"Multiple output files (future enhancement) HTML generation (use mdbook or similar) Automatic signature extraction (would need type inference) Interactive examples (would need eval capability)","title":"Out of Scope"},{"location":"plans/2026-02-05-set-implementation-design/","text":"Set Implementation \u2014 Design Problem Eucalypt lacks a native set data type. Users must use lists and manually handle deduplication and membership testing, which is inefficient and verbose. Solution A native Set type with O(1) operations, stored as HashSet<Primitive> in Rust, with prelude functions for construction and set algebra. Key Properties Native type \u2014 new Native::Set variant Primitive elements \u2014 numbers, strings, symbols only O(1) operations \u2014 hash-based membership and mutation Set algebra \u2014 union, intersection, difference Serializes as array \u2014 JSON/YAML output is [1, 2, 3] Syntax Examples empty: \u2205 # empty set (0-arity operator) nums: [1, 2, 2, 3] set.from-list # {1, 2, 3} has-two: nums set.contains?(2) # true evens: [2, 4, 6] set.from-list common: nums set.intersect(evens) # {2} all: nums set.union(evens) # {1, 2, 3, 4, 6} Native Type New Native variant in src/eval/memory/syntax.rs : pub enum Native { Sym(RefPtr<HeapString>), // becomes Sym(SymbolId) after eu-4af Str(RefPtr<HeapString>), Num(Number), Zdt(DateTime<FixedOffset>), Set(RefPtr<HeapSet>), // NEW } Ordering : eu-4af (symbol interning) lands first, so Sym is Sym(SymbolId) when sets are implemented. HeapSet structure in src/eval/memory/set.rs : pub struct HeapSet { elements: HashSet<Primitive>, } #[derive(Clone, Hash, PartialEq, Eq)] pub enum Primitive { Num(OrderedNumber), // wrapper for hashable number Str(String), Sym(SymbolId), // interned symbol ID (eu-4af) } Set intrinsics access the symbol pool via IntrinsicMachine::symbol_pool() for display/construction. Note on numbers : Rust's f64 isn't Hash . An OrderedNumber wrapper handles this by implementing Hash for the numeric representation. GC integration : HeapSet allocated on heap. GC scans don't trace into set contents since primitives contain no heap references. Intrinsics New module src/eval/stg/set.rs : Intrinsic Args Description SET.EMPTY 0 Returns empty set SET.FROM_LIST 1 List \u2192 Set SET.TO_LIST 1 Set \u2192 List SET.ADD 2 Add element to set SET.REMOVE 2 Remove element from set SET.CONTAINS 2 Check membership \u2192 bool SET.SIZE 1 Number of elements SET.UNION 2 Union of two sets SET.INTERSECT 2 Intersection of two sets SET.DIFF 2 Difference (set1 - set2) Prelude API ` { doc: \"Set operations\" } set: { from-list: __SET.FROM_LIST to-list: __SET.TO_LIST add: __SET.ADD remove: __SET.REMOVE contains?: __SET.CONTAINS size: __SET.SIZE empty?: s -> s size eq(0) union: __SET.UNION intersect: __SET.INTERSECT diff: __SET.DIFF } ` { doc: \"Empty set\" } \u2205: __SET.EMPTY Rendering Sets render as JSON/YAML arrays, sorted for deterministic output. Numbers sort numerically, strings and symbols sort lexicographically. Numbers sort before strings, strings before symbols. [3, 1, 2, 2] set.from-list # renders as [1, 2, 3] [\"c\", \"a\"] set.from-list # renders as [\"a\", \"c\"] The renderer handles Native::Set by collecting elements, sorting via Primitive::cmp() , and emitting as a list. Testing Construction : set.from-list deduplicates, \u2205 is empty Membership : set.contains? returns true/false correctly Mutation : set.add adds new, ignores existing; set.remove removes Algebra : union , intersect , diff produce correct results Round-trip : set.from-list(set.to-list(s)) equals original Rendering : Set outputs as valid JSON array Edge cases : Empty sets, single element, duplicate adds Error Handling Adding non-primitive to set \u2192 runtime error Set operations on non-sets \u2192 runtime error Out of Scope Block/nested structure membership (complex equality) Set literal syntax {1, 2, 3} (future enhancement) Subset/disjoint predicates (future enhancement) Ordered sets (future enhancement) Dependencies eu-7kc (0-arity operators) \u2713 \u2014 needed for \u2205 eu-4af (symbol interning) \u2014 Primitive::Sym uses SymbolId","title":"Set Implementation \u2014 Design"},{"location":"plans/2026-02-05-set-implementation-design/#set-implementation-design","text":"","title":"Set Implementation \u2014 Design"},{"location":"plans/2026-02-05-set-implementation-design/#problem","text":"Eucalypt lacks a native set data type. Users must use lists and manually handle deduplication and membership testing, which is inefficient and verbose.","title":"Problem"},{"location":"plans/2026-02-05-set-implementation-design/#solution","text":"A native Set type with O(1) operations, stored as HashSet<Primitive> in Rust, with prelude functions for construction and set algebra.","title":"Solution"},{"location":"plans/2026-02-05-set-implementation-design/#key-properties","text":"Native type \u2014 new Native::Set variant Primitive elements \u2014 numbers, strings, symbols only O(1) operations \u2014 hash-based membership and mutation Set algebra \u2014 union, intersection, difference Serializes as array \u2014 JSON/YAML output is [1, 2, 3]","title":"Key Properties"},{"location":"plans/2026-02-05-set-implementation-design/#syntax-examples","text":"empty: \u2205 # empty set (0-arity operator) nums: [1, 2, 2, 3] set.from-list # {1, 2, 3} has-two: nums set.contains?(2) # true evens: [2, 4, 6] set.from-list common: nums set.intersect(evens) # {2} all: nums set.union(evens) # {1, 2, 3, 4, 6}","title":"Syntax Examples"},{"location":"plans/2026-02-05-set-implementation-design/#native-type","text":"New Native variant in src/eval/memory/syntax.rs : pub enum Native { Sym(RefPtr<HeapString>), // becomes Sym(SymbolId) after eu-4af Str(RefPtr<HeapString>), Num(Number), Zdt(DateTime<FixedOffset>), Set(RefPtr<HeapSet>), // NEW } Ordering : eu-4af (symbol interning) lands first, so Sym is Sym(SymbolId) when sets are implemented. HeapSet structure in src/eval/memory/set.rs : pub struct HeapSet { elements: HashSet<Primitive>, } #[derive(Clone, Hash, PartialEq, Eq)] pub enum Primitive { Num(OrderedNumber), // wrapper for hashable number Str(String), Sym(SymbolId), // interned symbol ID (eu-4af) } Set intrinsics access the symbol pool via IntrinsicMachine::symbol_pool() for display/construction. Note on numbers : Rust's f64 isn't Hash . An OrderedNumber wrapper handles this by implementing Hash for the numeric representation. GC integration : HeapSet allocated on heap. GC scans don't trace into set contents since primitives contain no heap references.","title":"Native Type"},{"location":"plans/2026-02-05-set-implementation-design/#intrinsics","text":"New module src/eval/stg/set.rs : Intrinsic Args Description SET.EMPTY 0 Returns empty set SET.FROM_LIST 1 List \u2192 Set SET.TO_LIST 1 Set \u2192 List SET.ADD 2 Add element to set SET.REMOVE 2 Remove element from set SET.CONTAINS 2 Check membership \u2192 bool SET.SIZE 1 Number of elements SET.UNION 2 Union of two sets SET.INTERSECT 2 Intersection of two sets SET.DIFF 2 Difference (set1 - set2)","title":"Intrinsics"},{"location":"plans/2026-02-05-set-implementation-design/#prelude-api","text":"` { doc: \"Set operations\" } set: { from-list: __SET.FROM_LIST to-list: __SET.TO_LIST add: __SET.ADD remove: __SET.REMOVE contains?: __SET.CONTAINS size: __SET.SIZE empty?: s -> s size eq(0) union: __SET.UNION intersect: __SET.INTERSECT diff: __SET.DIFF } ` { doc: \"Empty set\" } \u2205: __SET.EMPTY","title":"Prelude API"},{"location":"plans/2026-02-05-set-implementation-design/#rendering","text":"Sets render as JSON/YAML arrays, sorted for deterministic output. Numbers sort numerically, strings and symbols sort lexicographically. Numbers sort before strings, strings before symbols. [3, 1, 2, 2] set.from-list # renders as [1, 2, 3] [\"c\", \"a\"] set.from-list # renders as [\"a\", \"c\"] The renderer handles Native::Set by collecting elements, sorting via Primitive::cmp() , and emitting as a list.","title":"Rendering"},{"location":"plans/2026-02-05-set-implementation-design/#testing","text":"Construction : set.from-list deduplicates, \u2205 is empty Membership : set.contains? returns true/false correctly Mutation : set.add adds new, ignores existing; set.remove removes Algebra : union , intersect , diff produce correct results Round-trip : set.from-list(set.to-list(s)) equals original Rendering : Set outputs as valid JSON array Edge cases : Empty sets, single element, duplicate adds","title":"Testing"},{"location":"plans/2026-02-05-set-implementation-design/#error-handling","text":"Adding non-primitive to set \u2192 runtime error Set operations on non-sets \u2192 runtime error","title":"Error Handling"},{"location":"plans/2026-02-05-set-implementation-design/#out-of-scope","text":"Block/nested structure membership (complex equality) Set literal syntax {1, 2, 3} (future enhancement) Subset/disjoint predicates (future enhancement) Ordered sets (future enhancement)","title":"Out of Scope"},{"location":"plans/2026-02-05-set-implementation-design/#dependencies","text":"eu-7kc (0-arity operators) \u2713 \u2014 needed for \u2205 eu-4af (symbol interning) \u2014 Primitive::Sym uses SymbolId","title":"Dependencies"},{"location":"plans/2026-02-05-sorting-lists-design/","text":"Sorting Lists \u2014 Design Problem The prelude has qsort(lt, xs) for sorting with a custom comparator, but no convenience functions for common sorting tasks. Users must write: [3, 1, 2] qsort(lt) [\"c\", \"a\"] qsort(str.lt) people qsort(\u03bb(a, b): a.age lt(b.age)) Solution Type-specific sort functions and sort-by conveniences, all built on qsort . Core Sort Functions Type-specific sorting using existing comparison infrastructure: sort-nums(xs): xs qsort(lt) sort-strs(xs): xs qsort(str.lt) sort-zdts(xs): xs qsort(lt) # requires polymorphic lt from eu-u1m Usage: [3, 1, 2] sort-nums # [1, 2, 3] [\"c\", \"a\", \"b\"] sort-strs # [\"a\", \"b\", \"c\"] [t\"2024-01-01\", t\"2023-06-15\"] sort-zdts # [t\"2023-06-15\", t\"2024-01-01\"] Descending order via composition with existing reverse : [3, 1, 2] sort-nums reverse # [3, 2, 1] Sort-By Functions Generic sort-by with key extractor and comparator: sort-by(key-fn, cmp, xs): xs qsort(\u03bb(a, b): key-fn(a) cmp(key-fn(b))) Usage: people: [{name: \"Bob\", age: 30}, {name: \"Alice\", age: 25}] people sort-by(_.age, lt) # sorted by age ascending people sort-by(_.name, str.lt) # sorted by name ascending Convenience wrappers: sort-by-num(key-fn, xs): xs sort-by(key-fn, lt) sort-by-str(key-fn, xs): xs sort-by(key-fn, str.lt) sort-by-zdt(key-fn, xs): xs sort-by(key-fn, lt) Usage: people sort-by-num(_.age) # sorted by age people sort-by-str(_.name) # sorted by name events sort-by-zdt(_.date) # sorted by date Implementation All functions are pure prelude \u2014 no new intrinsics needed. ` \"`sort-nums(xs)` - sort list of numbers ascending\" sort-nums(xs): xs qsort(lt) ` \"`sort-strs(xs)` - sort list of strings/symbols ascending\" sort-strs(xs): xs qsort(str.lt) ` \"`sort-zdts(xs)` - sort list of ZDTs ascending\" sort-zdts(xs): xs qsort(lt) ` \"`sort-by(key-fn, cmp, xs)` - sort list by key using comparator\" sort-by(key-fn, cmp, xs): xs qsort(\u03bb(a, b): key-fn(a) cmp(key-fn(b))) ` \"`sort-by-num(key-fn, xs)` - sort list by numeric key\" sort-by-num(key-fn, xs): xs sort-by(key-fn, lt) ` \"`sort-by-str(key-fn, xs)` - sort list by string key\" sort-by-str(key-fn, xs): xs sort-by(key-fn, str.lt) ` \"`sort-by-zdt(key-fn, xs)` - sort list by ZDT key\" sort-by-zdt(key-fn, xs): xs sort-by(key-fn, lt) Dependencies str.lt from eu-da3 (string comparison intrinsics) Polymorphic lt for ZDTs from eu-u1m Testing Sort functions: sort-nums : empty, single, sorted, reverse, duplicates, negatives, floats sort-strs : empty, single, case sensitivity, unicode, symbols sort-zdts : different dates, same date different times, timezones Sort-by functions: sort-by-num : blocks sorted by numeric field sort-by-str : blocks sorted by string field sort-by-zdt : blocks sorted by date field Nested keys: sort-by-num(_.stats.score, items) Edge cases: Empty lists return empty Single element unchanged Equal elements (note: order not guaranteed \u2014 qsort is not stable) Descending: sort-nums reverse produces descending sort-by-num(key) reverse works Out of Scope Stable sort guarantees (the prelude qsort is a classic quicksort partition which is not stable \u2014 equal elements may be reordered) Custom comparator builders (users have qsort for that) In-place sorting (eucalypt is functional)","title":"Sorting Lists \u2014 Design"},{"location":"plans/2026-02-05-sorting-lists-design/#sorting-lists-design","text":"","title":"Sorting Lists \u2014 Design"},{"location":"plans/2026-02-05-sorting-lists-design/#problem","text":"The prelude has qsort(lt, xs) for sorting with a custom comparator, but no convenience functions for common sorting tasks. Users must write: [3, 1, 2] qsort(lt) [\"c\", \"a\"] qsort(str.lt) people qsort(\u03bb(a, b): a.age lt(b.age))","title":"Problem"},{"location":"plans/2026-02-05-sorting-lists-design/#solution","text":"Type-specific sort functions and sort-by conveniences, all built on qsort .","title":"Solution"},{"location":"plans/2026-02-05-sorting-lists-design/#core-sort-functions","text":"Type-specific sorting using existing comparison infrastructure: sort-nums(xs): xs qsort(lt) sort-strs(xs): xs qsort(str.lt) sort-zdts(xs): xs qsort(lt) # requires polymorphic lt from eu-u1m Usage: [3, 1, 2] sort-nums # [1, 2, 3] [\"c\", \"a\", \"b\"] sort-strs # [\"a\", \"b\", \"c\"] [t\"2024-01-01\", t\"2023-06-15\"] sort-zdts # [t\"2023-06-15\", t\"2024-01-01\"] Descending order via composition with existing reverse : [3, 1, 2] sort-nums reverse # [3, 2, 1]","title":"Core Sort Functions"},{"location":"plans/2026-02-05-sorting-lists-design/#sort-by-functions","text":"Generic sort-by with key extractor and comparator: sort-by(key-fn, cmp, xs): xs qsort(\u03bb(a, b): key-fn(a) cmp(key-fn(b))) Usage: people: [{name: \"Bob\", age: 30}, {name: \"Alice\", age: 25}] people sort-by(_.age, lt) # sorted by age ascending people sort-by(_.name, str.lt) # sorted by name ascending Convenience wrappers: sort-by-num(key-fn, xs): xs sort-by(key-fn, lt) sort-by-str(key-fn, xs): xs sort-by(key-fn, str.lt) sort-by-zdt(key-fn, xs): xs sort-by(key-fn, lt) Usage: people sort-by-num(_.age) # sorted by age people sort-by-str(_.name) # sorted by name events sort-by-zdt(_.date) # sorted by date","title":"Sort-By Functions"},{"location":"plans/2026-02-05-sorting-lists-design/#implementation","text":"All functions are pure prelude \u2014 no new intrinsics needed. ` \"`sort-nums(xs)` - sort list of numbers ascending\" sort-nums(xs): xs qsort(lt) ` \"`sort-strs(xs)` - sort list of strings/symbols ascending\" sort-strs(xs): xs qsort(str.lt) ` \"`sort-zdts(xs)` - sort list of ZDTs ascending\" sort-zdts(xs): xs qsort(lt) ` \"`sort-by(key-fn, cmp, xs)` - sort list by key using comparator\" sort-by(key-fn, cmp, xs): xs qsort(\u03bb(a, b): key-fn(a) cmp(key-fn(b))) ` \"`sort-by-num(key-fn, xs)` - sort list by numeric key\" sort-by-num(key-fn, xs): xs sort-by(key-fn, lt) ` \"`sort-by-str(key-fn, xs)` - sort list by string key\" sort-by-str(key-fn, xs): xs sort-by(key-fn, str.lt) ` \"`sort-by-zdt(key-fn, xs)` - sort list by ZDT key\" sort-by-zdt(key-fn, xs): xs sort-by(key-fn, lt)","title":"Implementation"},{"location":"plans/2026-02-05-sorting-lists-design/#dependencies","text":"str.lt from eu-da3 (string comparison intrinsics) Polymorphic lt for ZDTs from eu-u1m","title":"Dependencies"},{"location":"plans/2026-02-05-sorting-lists-design/#testing","text":"Sort functions: sort-nums : empty, single, sorted, reverse, duplicates, negatives, floats sort-strs : empty, single, case sensitivity, unicode, symbols sort-zdts : different dates, same date different times, timezones Sort-by functions: sort-by-num : blocks sorted by numeric field sort-by-str : blocks sorted by string field sort-by-zdt : blocks sorted by date field Nested keys: sort-by-num(_.stats.score, items) Edge cases: Empty lists return empty Single element unchanged Equal elements (note: order not guaranteed \u2014 qsort is not stable) Descending: sort-nums reverse produces descending sort-by-num(key) reverse works","title":"Testing"},{"location":"plans/2026-02-05-sorting-lists-design/#out-of-scope","text":"Stable sort guarantees (the prelude qsort is a classic quicksort partition which is not stable \u2014 equal elements may be reordered) Custom comparator builders (users have qsort for that) In-place sorting (eucalypt is functional)","title":"Out of Scope"},{"location":"plans/2026-02-05-symbol-interning-design/","text":"Symbol Interning \u2014 Design Problem Symbols are stored as individual heap strings. The same symbol appearing N times results in N separate heap allocations. Comparison requires full string comparison. Solution Symbol interning \u2014 a per-machine pool where each unique symbol string is stored once, referenced by a compact SymbolId(u32) . Key Properties Per-machine pool \u2014 owned by VM, clear lifetime, no thread safety Interning at heap load \u2014 loader interns instead of allocating Native::Sym(SymbolId) \u2014 symbols are just IDs, strings live in pool Fast comparison \u2014 id1 == id2 instead of string comparison Pool structure \u2014 HashMap<String, SymbolId> + Vec<String> Benefits Memory: each unique symbol stored once Speed: integer comparison vs string comparison Enables: block indexing (eu-brj) with HashMap<SymbolId, usize> Data Structures SymbolId \u2014 newtype wrapper for type safety: #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)] pub struct SymbolId(u32); SymbolPool \u2014 owned by the machine: pub struct SymbolPool { /// String \u2192 ID lookup for interning to_id: HashMap<String, SymbolId>, /// ID \u2192 String lookup for resolving to_str: Vec<String>, } impl SymbolPool { pub fn intern(&mut self, s: &str) -> SymbolId { if let Some(&id) = self.to_id.get(s) { return id; } let id = SymbolId(self.to_str.len() as u32); self.to_str.push(s.to_string()); self.to_id.insert(s.to_string(), id); id } pub fn resolve(&self, id: SymbolId) -> &str { &self.to_str[id.0 as usize] } } Native::Sym change : // Before Native::Sym(RefPtr<HeapString>) // After Native::Sym(SymbolId) Integration Points Loader ( src/eval/memory/loader.rs ) Currently allocates HeapString for each symbol. Change to intern: // Before stg::syntax::Native::Sym(s) => { let ptr = self.alloc(HeapString::from_str(mem, s.as_str()))?; memory::syntax::Ref::V(memory::syntax::Native::Sym(ptr)) } // After stg::syntax::Native::Sym(s) => { let id = symbol_pool.intern(s.as_str()); memory::syntax::Ref::V(memory::syntax::Native::Sym(id)) } Equality ( src/eval/stg/eq.rs ) Currently compares strings. Change to integer comparison: // Before (Native::Sym(sx), Native::Sym(sy)) => str_eq(view, sx, sy) // After (Native::Sym(id_x), Native::Sym(id_y)) => id_x == id_y Display and errors Anywhere that needs the symbol string calls pool.resolve(id) . This includes error messages, rendering, and debugging output. Block key matching MatchesKey in block.rs compares SymbolIds directly instead of string comparison. Machine Ownership The SymbolPool is owned by the machine/evaluator: pub struct Machine { // ... existing fields symbol_pool: SymbolPool, } Pool access via IntrinsicMachine trait \u2014 add symbol_pool(&self) -> &SymbolPool and symbol_pool_mut(&mut self) -> &mut SymbolPool methods. Intrinsics already receive the machine reference, so no signature changes needed. Intrinsics that don't use symbols are unaffected. Testing Correctness : Existing harness tests pass unchanged. Interning : Same symbol interned twice returns same ID. Resolve round-trip : pool.resolve(pool.intern(\"foo\")) == \"foo\" . Comparison : Two symbols with same string have equal IDs. Memory : Verify single allocation per unique symbol. Benchmarking Memory usage before/after on symbol-heavy programs Symbol comparison performance Block lookup performance (benefits from fast key comparison) Out of Scope Thread-safe pool (single-threaded execution) Garbage collection of unused symbols (pool lives with machine) Interning at parse time (would require larger changes)","title":"Symbol Interning \u2014 Design"},{"location":"plans/2026-02-05-symbol-interning-design/#symbol-interning-design","text":"","title":"Symbol Interning \u2014 Design"},{"location":"plans/2026-02-05-symbol-interning-design/#problem","text":"Symbols are stored as individual heap strings. The same symbol appearing N times results in N separate heap allocations. Comparison requires full string comparison.","title":"Problem"},{"location":"plans/2026-02-05-symbol-interning-design/#solution","text":"Symbol interning \u2014 a per-machine pool where each unique symbol string is stored once, referenced by a compact SymbolId(u32) .","title":"Solution"},{"location":"plans/2026-02-05-symbol-interning-design/#key-properties","text":"Per-machine pool \u2014 owned by VM, clear lifetime, no thread safety Interning at heap load \u2014 loader interns instead of allocating Native::Sym(SymbolId) \u2014 symbols are just IDs, strings live in pool Fast comparison \u2014 id1 == id2 instead of string comparison Pool structure \u2014 HashMap<String, SymbolId> + Vec<String>","title":"Key Properties"},{"location":"plans/2026-02-05-symbol-interning-design/#benefits","text":"Memory: each unique symbol stored once Speed: integer comparison vs string comparison Enables: block indexing (eu-brj) with HashMap<SymbolId, usize>","title":"Benefits"},{"location":"plans/2026-02-05-symbol-interning-design/#data-structures","text":"SymbolId \u2014 newtype wrapper for type safety: #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)] pub struct SymbolId(u32); SymbolPool \u2014 owned by the machine: pub struct SymbolPool { /// String \u2192 ID lookup for interning to_id: HashMap<String, SymbolId>, /// ID \u2192 String lookup for resolving to_str: Vec<String>, } impl SymbolPool { pub fn intern(&mut self, s: &str) -> SymbolId { if let Some(&id) = self.to_id.get(s) { return id; } let id = SymbolId(self.to_str.len() as u32); self.to_str.push(s.to_string()); self.to_id.insert(s.to_string(), id); id } pub fn resolve(&self, id: SymbolId) -> &str { &self.to_str[id.0 as usize] } } Native::Sym change : // Before Native::Sym(RefPtr<HeapString>) // After Native::Sym(SymbolId)","title":"Data Structures"},{"location":"plans/2026-02-05-symbol-interning-design/#integration-points","text":"","title":"Integration Points"},{"location":"plans/2026-02-05-symbol-interning-design/#loader-srcevalmemoryloaderrs","text":"Currently allocates HeapString for each symbol. Change to intern: // Before stg::syntax::Native::Sym(s) => { let ptr = self.alloc(HeapString::from_str(mem, s.as_str()))?; memory::syntax::Ref::V(memory::syntax::Native::Sym(ptr)) } // After stg::syntax::Native::Sym(s) => { let id = symbol_pool.intern(s.as_str()); memory::syntax::Ref::V(memory::syntax::Native::Sym(id)) }","title":"Loader (src/eval/memory/loader.rs)"},{"location":"plans/2026-02-05-symbol-interning-design/#equality-srcevalstgeqrs","text":"Currently compares strings. Change to integer comparison: // Before (Native::Sym(sx), Native::Sym(sy)) => str_eq(view, sx, sy) // After (Native::Sym(id_x), Native::Sym(id_y)) => id_x == id_y","title":"Equality (src/eval/stg/eq.rs)"},{"location":"plans/2026-02-05-symbol-interning-design/#display-and-errors","text":"Anywhere that needs the symbol string calls pool.resolve(id) . This includes error messages, rendering, and debugging output.","title":"Display and errors"},{"location":"plans/2026-02-05-symbol-interning-design/#block-key-matching","text":"MatchesKey in block.rs compares SymbolIds directly instead of string comparison.","title":"Block key matching"},{"location":"plans/2026-02-05-symbol-interning-design/#machine-ownership","text":"The SymbolPool is owned by the machine/evaluator: pub struct Machine { // ... existing fields symbol_pool: SymbolPool, } Pool access via IntrinsicMachine trait \u2014 add symbol_pool(&self) -> &SymbolPool and symbol_pool_mut(&mut self) -> &mut SymbolPool methods. Intrinsics already receive the machine reference, so no signature changes needed. Intrinsics that don't use symbols are unaffected.","title":"Machine Ownership"},{"location":"plans/2026-02-05-symbol-interning-design/#testing","text":"Correctness : Existing harness tests pass unchanged. Interning : Same symbol interned twice returns same ID. Resolve round-trip : pool.resolve(pool.intern(\"foo\")) == \"foo\" . Comparison : Two symbols with same string have equal IDs. Memory : Verify single allocation per unique symbol.","title":"Testing"},{"location":"plans/2026-02-05-symbol-interning-design/#benchmarking","text":"Memory usage before/after on symbol-heavy programs Symbol comparison performance Block lookup performance (benefits from fast key comparison)","title":"Benchmarking"},{"location":"plans/2026-02-05-symbol-interning-design/#out-of-scope","text":"Thread-safe pool (single-threaded execution) Garbage collection of unused symbols (pool lives with machine) Interning at parse time (would require larger changes)","title":"Out of Scope"},{"location":"plans/2026-02-05-zdt-literal-design/","text":"ZDT Literal \u2014 Design Problem Creating ZDT (zoned datetime) values requires verbose function calls: meeting: cal.parse(\"2023-01-15T10:30:00Z\") birthday: cal.parse(\"1990-06-15\") Users want cleaner syntax for datetime literals. Solution A prefixed string literal t\"...\" for ZDT values, consistent with existing c\"...\" (c-strings) and r\"...\" (raw strings). meeting: t\"2023-01-15T10:30:00Z\" birthday: t\"1990-06-15\" deadline: t\"2023-12-31T23:59:59+05:00\" Supported Formats Matching YAML timestamp flexibility for familiarity: Format Example Result ISO8601 with Z t\"2023-01-15T10:30:00Z\" 2023-01-15T10:30:00Z ISO8601 with offset t\"2023-01-15T10:30:00+05:00\" 2023-01-15T10:30:00+05:00 ISO8601 no timezone t\"2023-01-15T10:30:00\" 2023-01-15T10:30:00Z (UTC) Space separator t\"2023-01-15 10:30:00\" 2023-01-15T10:30:00Z (UTC) Fractional seconds t\"2023-01-15T10:30:00.123Z\" 2023-01-15T10:30:00.123Z Date only t\"2023-01-15\" 2023-01-15T00:00:00Z (midnight UTC) Defaults Missing timezone \u2192 UTC Date only \u2192 midnight (00:00:00) UTC These match the existing YAML timestamp import behaviour. Compile-Time Validation Invalid dates are rejected at parse time: bad1: t\"2023-02-30\" # error: February has 28/29 days bad2: t\"2023-13-01\" # error: month must be 1-12 bad3: t\"2023-01-32\" # error: day out of range bad4: t\"2023-01-15T25:00\" # error: hour must be 0-23 bad5: t\"not-a-date\" # error: invalid timestamp format leap: t\"2024-02-29\" # OK - 2024 is a leap year Error messages include source location and specific reason: error: invalid ZDT literal --> file.eu:3:10 | 3 | deadline: t\"2023-02-30\" | ^^^^^^^^^^^^^ February 2023 has only 28 days Implementation Lexer ( src/syntax/rowan/lex.rs ) Add T_STRING token type alongside existing C_STRING and R_STRING . When the lexer sees t\" , it: Extracts the string content Validates it as a timestamp using chrono Emits T_STRING token (or error if invalid) No interpolation support needed \u2014 just T_STRING , not T_STRING_PATTERN . Syntax kinds ( src/syntax/rowan/kind.rs ) T_STRING, // t\"2023-01-15T10:30:00Z\" AST and code generation A t\"...\" literal desugars to a ZDT native value during compilation \u2014 the same internal representation used by cal.parse(...) and YAML timestamp imports. The chrono DateTime<FixedOffset> is computed at compile time and embedded directly. No runtime parsing occurs. Testing Harness tests Valid formats: All supported YAML-style formats Timezone defaulting: Literals without timezone produce UTC Date-only: Produces midnight UTC Leap years: t\"2024-02-29\" valid, t\"2023-02-29\" invalid Edge cases: Month boundaries, year boundaries, maximum values Equivalence: t\"2023-01-15T10:30:00Z\" equals cal.parse(\"2023-01-15T10:30:00Z\") Timezone comparison: t\"2023-01-15T10:00:00Z\" equals t\"2023-01-15T10:00:00+00:00\" (chrono compares absolute instants) Cross-timezone ordering: t\"2023-01-15T08:00:00Z\" < t\"2023-01-15T12:00:00+02:00\" (08:00 UTC is before 10:00 UTC) Error tests Using .expect sidecar mechanism: Invalid day: t\"2023-02-30\" Invalid month: t\"2023-13-01\" Invalid hour: t\"2023-01-15T25:00:00Z\" Malformed: t\"not-a-date\" Empty: t\"\" Out of Scope Interpolation in ZDT literals (not needed) Duration literals (separate future feature) Period literals (separate future feature) Time-only literals (separate future feature)","title":"ZDT Literal \u2014 Design"},{"location":"plans/2026-02-05-zdt-literal-design/#zdt-literal-design","text":"","title":"ZDT Literal \u2014 Design"},{"location":"plans/2026-02-05-zdt-literal-design/#problem","text":"Creating ZDT (zoned datetime) values requires verbose function calls: meeting: cal.parse(\"2023-01-15T10:30:00Z\") birthday: cal.parse(\"1990-06-15\") Users want cleaner syntax for datetime literals.","title":"Problem"},{"location":"plans/2026-02-05-zdt-literal-design/#solution","text":"A prefixed string literal t\"...\" for ZDT values, consistent with existing c\"...\" (c-strings) and r\"...\" (raw strings). meeting: t\"2023-01-15T10:30:00Z\" birthday: t\"1990-06-15\" deadline: t\"2023-12-31T23:59:59+05:00\"","title":"Solution"},{"location":"plans/2026-02-05-zdt-literal-design/#supported-formats","text":"Matching YAML timestamp flexibility for familiarity: Format Example Result ISO8601 with Z t\"2023-01-15T10:30:00Z\" 2023-01-15T10:30:00Z ISO8601 with offset t\"2023-01-15T10:30:00+05:00\" 2023-01-15T10:30:00+05:00 ISO8601 no timezone t\"2023-01-15T10:30:00\" 2023-01-15T10:30:00Z (UTC) Space separator t\"2023-01-15 10:30:00\" 2023-01-15T10:30:00Z (UTC) Fractional seconds t\"2023-01-15T10:30:00.123Z\" 2023-01-15T10:30:00.123Z Date only t\"2023-01-15\" 2023-01-15T00:00:00Z (midnight UTC)","title":"Supported Formats"},{"location":"plans/2026-02-05-zdt-literal-design/#defaults","text":"Missing timezone \u2192 UTC Date only \u2192 midnight (00:00:00) UTC These match the existing YAML timestamp import behaviour.","title":"Defaults"},{"location":"plans/2026-02-05-zdt-literal-design/#compile-time-validation","text":"Invalid dates are rejected at parse time: bad1: t\"2023-02-30\" # error: February has 28/29 days bad2: t\"2023-13-01\" # error: month must be 1-12 bad3: t\"2023-01-32\" # error: day out of range bad4: t\"2023-01-15T25:00\" # error: hour must be 0-23 bad5: t\"not-a-date\" # error: invalid timestamp format leap: t\"2024-02-29\" # OK - 2024 is a leap year Error messages include source location and specific reason: error: invalid ZDT literal --> file.eu:3:10 | 3 | deadline: t\"2023-02-30\" | ^^^^^^^^^^^^^ February 2023 has only 28 days","title":"Compile-Time Validation"},{"location":"plans/2026-02-05-zdt-literal-design/#implementation","text":"","title":"Implementation"},{"location":"plans/2026-02-05-zdt-literal-design/#lexer-srcsyntaxrowanlexrs","text":"Add T_STRING token type alongside existing C_STRING and R_STRING . When the lexer sees t\" , it: Extracts the string content Validates it as a timestamp using chrono Emits T_STRING token (or error if invalid) No interpolation support needed \u2014 just T_STRING , not T_STRING_PATTERN .","title":"Lexer (src/syntax/rowan/lex.rs)"},{"location":"plans/2026-02-05-zdt-literal-design/#syntax-kinds-srcsyntaxrowankindrs","text":"T_STRING, // t\"2023-01-15T10:30:00Z\"","title":"Syntax kinds (src/syntax/rowan/kind.rs)"},{"location":"plans/2026-02-05-zdt-literal-design/#ast-and-code-generation","text":"A t\"...\" literal desugars to a ZDT native value during compilation \u2014 the same internal representation used by cal.parse(...) and YAML timestamp imports. The chrono DateTime<FixedOffset> is computed at compile time and embedded directly. No runtime parsing occurs.","title":"AST and code generation"},{"location":"plans/2026-02-05-zdt-literal-design/#testing","text":"","title":"Testing"},{"location":"plans/2026-02-05-zdt-literal-design/#harness-tests","text":"Valid formats: All supported YAML-style formats Timezone defaulting: Literals without timezone produce UTC Date-only: Produces midnight UTC Leap years: t\"2024-02-29\" valid, t\"2023-02-29\" invalid Edge cases: Month boundaries, year boundaries, maximum values Equivalence: t\"2023-01-15T10:30:00Z\" equals cal.parse(\"2023-01-15T10:30:00Z\") Timezone comparison: t\"2023-01-15T10:00:00Z\" equals t\"2023-01-15T10:00:00+00:00\" (chrono compares absolute instants) Cross-timezone ordering: t\"2023-01-15T08:00:00Z\" < t\"2023-01-15T12:00:00+02:00\" (08:00 UTC is before 10:00 UTC)","title":"Harness tests"},{"location":"plans/2026-02-05-zdt-literal-design/#error-tests","text":"Using .expect sidecar mechanism: Invalid day: t\"2023-02-30\" Invalid month: t\"2023-13-01\" Invalid hour: t\"2023-01-15T25:00:00Z\" Malformed: t\"not-a-date\" Empty: t\"\"","title":"Error tests"},{"location":"plans/2026-02-05-zdt-literal-design/#out-of-scope","text":"Interpolation in ZDT literals (not needed) Duration literals (separate future feature) Period literals (separate future feature) Time-only literals (separate future feature)","title":"Out of Scope"},{"location":"plans/2026-02-06-agent-team-plan/","text":"Eucalypt v0.3.0 Agent Team Implementation Plan Context Eucalypt v0.3.0 has 16 design plans covering VM improvements, prelude extensions, tooling enhancements, and an LSP server. All plans have been reviewed and cross-checked. The work is tracked in beads with dependency graphs. We need a team of autonomous agents to implement this in parallel, with a review/architect agent ensuring quality and coherence. Team Structure Name Role Worktree Scope Niamh VM, GC, memory, STG, DCE eucalypt-vm eu-4af, eu-brj, eu-xg2x, eu-8d8, eu-181d, eu-40jb Callum Prelude, intrinsics, features, ZDT literal eucalypt-prelude Quick wins, eu-da3, eu-ert, eu-kfe, eu-kd1, eu-z0l Fricka Tester, benchmarks, docs gen eucalypt-tooling eu-twg, eu-01v, eu-ber Ravi LSP server + editors eucalypt-lsp eu-307 (all phases) Seren Architect, PR review, docs main repo PR gate, cross-stream, docs freshness, eu-ptu Orchestrator (me): delegate, unblock, manage beads, handle escalations. Git Strategy Worktree Setup Each coding agent gets its own git worktree branched from master. This avoids checkout conflicts and gives each agent its own target/ directory for independent builds. # Create worktree directory mkdir -p /Users/greg/dev/curvelogic/eucalypt-worktrees # Create one worktree per coding agent, each starting from master git worktree add /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-vm master git worktree add /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-prelude master git worktree add /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-tooling master git worktree add /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-lsp master The reviewer works from the main repo directory. Branch-per-Task Flow Each bead task gets its own branch: feature/eu-XXXX-short-description Agent workflow per task: 1. In their worktree: git fetch origin && git checkout -b feature/eu-XXXX-desc origin/master 2. Implement, test, commit (multiple commits fine) 3. Push: git push -u origin feature/eu-XXXX-desc 4. Create PR: gh pr create --title \"...\" --body \"...\" 5. Notify reviewer via SendMessage 6. Wait for merge (reviewer handles this) 7. After merge: git fetch origin && git checkout -b feature/eu-YYYY-next origin/master Merge Sequencing Seren squash-merges each PR (one clean commit per bead task) Squash commit message format: feat(eu-XXXX): Short description of change with a body summarising what was done and why After each merge, Seren runs full test suite on master If merge conflicts arise, Seren resolves or asks the authoring agent Agents always branch from latest origin/master for each new task Agent Operating Guidelines Shared Rules (All Agents) WORKTREE: You work EXCLUSIVELY in your assigned worktree directory. All file paths and all cargo/git commands use that directory. QUALITY GATE: Before marking any task done, you MUST pass: 1. cargo fmt --all 2. cargo clippy --all-targets -- -D warnings 3. cargo test If any step fails, fix and retry (up to 5 attempts). After 5 failures on the same issue, escalate to reviewer. BRANCHES: One branch per bead task. Branch from latest origin/master. Never commit directly to master. COMMITS: Clear, descriptive messages. Include bead ID. Example: \"feat(eu-4af): Add SymbolId and SymbolPool data structures\" UK ENGLISH: All comments, docs, error messages use UK English. PUBLIC-READY: All code will be merged to the public eucalypt repo after 0.3.0. Write everything as if it's public from day one: - No internal jargon, no references to private repos or tools - Clean, helpful error messages that make sense to external users - Comments explain \"why\", not \"what\" \u2014 assume a competent Rust reader - Doc metadata on prelude functions (` \"description\") is user-facing CLIPPY: Fix ALL warnings. Never use #[allow()] to suppress. DECISION AUTHORITY: You CAN decide without escalating: - Implementation details not covered by the spec (choose sensibly, document in PR description) - Internal API additions (helper functions, internal modules) - Performance trade-offs within the spec's stated approach - Test strategy and coverage details beyond what the spec lists - Error message wording (following UK English, clear and helpful) - Minor refactoring needed to land your feature cleanly ESCALATION: Send a message to Seren ONLY when: - You've failed 5 fix attempts on the same issue - You discover a cross-stream conflict affecting another agent - You need to change a user-visible API not covered by the spec - The spec is fundamentally unclear about the desired behaviour (not just missing detail \u2014 missing intent) DESIGN PLANS: Read docs/plans/*.md for your assigned features BEFORE starting implementation. These are the source of truth. OPPORTUNISTIC P3 FIXES: When you are already modifying a file, fix nearby issues from these beads if the fix is trivial and local: - eu-0yz: Replace unwrap() with expect(\"descriptive message\") - eu-2254: (Niamh only) Lazy iteration for stack_trace if touching vm.rs - eu-mrg: Remove or resolve TODO/FIXME/HACK comments you encounter Do NOT go hunting for these. Only fix them if they're in your path. NO BEADS MANAGEMENT: The orchestrator manages beads. You report task completion via SendMessage to the orchestrator. Do not run bd commands. BEAD LIFECYCLE (orchestrator handles all of this): - in_progress: when orchestrator assigns/signals an agent to start - close (sub-bead): after Seren confirms squash-merge to master and master CI passes - close (parent epic): when all child sub-beads are closed - bd sync: orchestrator runs periodically and at session end Niamh (VM Specialist) WORKTREE: /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-vm TASK ORDER (strictly serial \u2014 each depends on the previous): 1. eu-4af: Symbol interning (eu-e1p3 \u2192 eu-jo9y \u2192 eu-pq9i \u2192 eu-uaoj \u2192 eu-yvpz \u2192 eu-eysh \u2192 eu-r9bv) 2. eu-brj: Block indexing (eu-z5br \u2192 eu-wnkl \u2192 eu-pb3e \u2192 eu-piho) 3. eu-xg2x: STG optimisations (eu-knx, eu-otxv, eu-354y, eu-drh0 \u2014 any order within) 4. eu-8d8: DCE inside blocks (eu-qmw3 \u2192 eu-eezd \u2192 eu-j642) 5. eu-40jb: String intrinsics phase 1 (eu-cb9k \u2192 eu-nq77) 6. eu-181d: GC Immix \u2014 all 3 phases, hyper-incrementally: Phase 1: Benchmarking infrastructure (statistics, --statistics-file, Criterion benchmarks, gc-bench.sh, workflow docs) Phase 2: Lazy sweeping \u2014 implement behind conditional, validate with benchmarks, then switch on in a separate small commit Phase 3: Evacuation \u2014 land in stages: a. Forwarding pointer support in AllocHeader (inert, tested in isolation) b. Evacuation candidate selection logic (inert, unit tested) c. Evacuation during marking (disabled by default, tested via unit tests) d. Reference updating in GcScannable (disabled, tested per-type) e. Enable opportunistic evacuation (small commit, full validation) Each sub-step is a separate branch/PR, validated independently. Code can land before it's active. The switch-on is always a separate, small, reviewable commit. BENCHMARKING DISCIPLINE: - After eu-4af: benchmark symbol-heavy programs before/after - After eu-brj: benchmark block lookup before/after - After each STG optimisation: check ticks/allocs in -S output - Before/after every GC phase: full gc-bench.sh comparison - Record all results in PR descriptions SPECIAL RULES: - Memory code is safety-critical. Document all unsafe blocks. - When changing GcScannable, verify ALL implementors are updated. - Symbol interning changes Native::Sym everywhere \u2014 grep exhaustively. - After completing eu-4af, notify orchestrator so eu-kfe can start. - GC changes: never enable new behaviour in the same PR that adds the code. Implement \u2192 validate \u2192 enable are separate PRs. Callum (Prelude & Features) WORKTREE: /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-prelude TASK ORDER: Phase 1 (immediate, no dependencies): - eu-u1m: Polymorphic lt/gt for ZDTs + strings + symbols - eu-nbc: Version assertions (eu.requires) - eu-dyx: Base64 encode/decode - eu-dd5: SHA-256 hash Phase 2 (after eu-u1m): - eu-da3: String comparison intrinsics (str.lt etc.) + sort-keys - eu-dlr: Catch {name: name} recursion (depends on eu-twg for error tests) Phase 3 (after eu-da3 + eu-u1m): - eu-ert: Sorting lists (sort-nums, sort-strs, sort-by etc.) Phase 4 (after eu-4af merges to master): - eu-kfe: Set implementation (Primitive::Sym uses SymbolId) Phase 5 (independent, can start any time): - eu-kd1: ZDT literal syntax (eu-yfcw \u2192 eu-7y85 \u2192 eu-qarv, then tests) Phase 6 (independent, lower priority): - eu-z0l: Deep find functions NAMING CONVENTION: - String comparison goes INSIDE str block: str.lt, str.gt, str.lte, str.gte - New intrinsics follow existing pattern in src/eval/stg/ - Prelude functions have doc metadata: ` \"description\" INTRINSIC PATTERN: Read an existing intrinsic (e.g., src/eval/stg/string.rs) and follow the exact pattern: struct, name(), execute(), CallGlobal trait, register in src/eval/stg/mod.rs. HARNESS TESTS: Every new feature gets harness tests in harness/test/. Follow the existing numbered naming: find the highest number, increment by 1. Fricka (Tooling & Infrastructure) WORKTREE: /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-tooling TASK ORDER: Phase 1 (independent, start immediately): - eu-twg: Error message tests (eu-aiux \u2192 eu-icdc \u2192 eu-i1pi) - eu-01v: Benchmarks in tester (eu-s5n5 \u2192 eu-asdm \u2192 eu-2c1x \u2192 eu-1yxc \u2192 eu-f2pk) Phase 2 (after prelude stabilises): - eu-ber: Prelude docs generator (eu-980b \u2192 eu-j2pu \u2192 eu-9ez6) TESTER CHANGES: The tester code is in src/driver/tester.rs and lib/test.eu. Test plan analysis is in testplan.rs (or similar). Read the existing code thoroughly before modifying. ERROR TESTS: - .expect sidecars go alongside .eu files in harness/test/errors/ - Format: YAML with exit: and stderr: fields - Start with 3 smoke tests, then migrate the rest BENCHMARKS: - bench- prefixed targets in test files - Report ticks, allocs, max-stack to stderr - Stats already captured in evidence \u2014 surface them Ravi (LSP Server) WORKTREE: /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-lsp TASK ORDER (phase 1 only for initial sprint): 1. eu-307.1: LSP scaffold (eu lsp subcommand, lsp-server, stdio loop) 2. eu-307.2: Diagnostics from Rowan parser 3. eu-307.3: Document symbols 4. eu-307.4: Folding and selection ranges 5. eu-307.5: VS Code extension DEPENDENCIES: - Add lsp-server and lsp-types to Cargo.toml - Wire eu lsp subcommand into src/driver/options.rs clap structure - New module: src/driver/lsp.rs (and src/driver/lsp/ submodules) TESTING: - Unit tests for each LSP feature - Manual testing via VS Code extension during development - Automated test: spawn eu lsp, send JSON-RPC messages, check responses SCOPE: Phase 1 only. Do NOT start phase 2 (semantic features) without explicit go-ahead from orchestrator. Seren (Reviewer / Architect) DIRECTORY: /Users/greg/dev/curvelogic/eucalypt (main repo) ROLE: Architect, PR reviewer, documentation guardian. PR REVIEW PROCESS: 1. When notified of a PR, fetch and check out the branch 2. Read the diff against master 3. Verify: a. Code matches the design plan in docs/plans/ b. All tests pass (cargo test) c. Clippy clean (cargo clippy --all-targets -- -D warnings) d. Formatting clean (cargo fmt --all -- --check) e. UK English in comments/docs/errors f. No #[allow()] suppressions g. Harness tests added for new features h. No unintended changes to other features 4. If issues found: send specific feedback to the authoring agent 5. If clean: squash-merge to master via gh pr merge --squash 6. After merge: verify master CI passes 7. Notify orchestrator that eu-XXXX has merged (triggers bead close) ARCHITECTURAL OVERSIGHT: - Watch for cross-stream conflicts (e.g., two agents modifying intrinsics.rs) - Ensure symbol interning changes propagate correctly - Flag if an agent deviates from the design plan - Check that new intrinsics follow the established pattern DOCUMENTATION OVERSIGHT: - Flag if harness tests are missing for new features - Check that prelude doc metadata (` \"description\") is present - Note if docs/plans/ need updating based on implementation changes - Track whether docs/ user-facing docs need refresh DOCS REFRESH (eu-ptu): - After the bulk of features have landed, assess and fix the docs build - Refresh user-facing documentation to reflect 0.3.0 features - This feeds into Phase 5 documentation replacement work DECISION AUTHORITY: You CAN decide without escalating: - Architectural details not specified in the plans (choose the simpler approach, document in PR review comments) - Merge conflict resolution for trivial conflicts (adjacent lines, import ordering in mod.rs) - Requesting minor code changes from agents (naming, structure, error handling) - Approving reasonable implementation choices agents made beyond what the spec covers ESCALATION: Escalate to orchestrator ONLY when: - Semantic merge conflict between two agents' work - An agent's PR fundamentally deviates from the design plan - Two rounds of feedback haven't resolved an issue - Cross-stream dependency needs re-ordering MERGE CONFLICT RESOLUTION: - Trivial conflicts (e.g., adjacent lines in mod.rs): resolve yourself - Semantic conflicts (overlapping features): escalate to orchestrator ESCALATION TRIAGE: When a coding agent escalates to you (not a PR): 1. Build/clippy failure after 5 attempts: - Read the agent's code and error output - Suggest a concrete fix or alternative approach - If you can identify the root cause, send it back with instructions - If you cannot: escalate to orchestrator 2. Unclear spec intent: - Read the relevant design plan in docs/plans/ - If the intent is recoverable from context, make a decision and document it in a message back to the agent - If the spec is genuinely ambiguous about user-visible behaviour: escalate to orchestrator 3. Cross-stream conflict: - Escalate to orchestrator immediately (they coordinate agents) 4. User-visible API change not in spec: - Escalate to orchestrator (they decide scope changes) Orchestrator Decision Authority The orchestrator (me) has FULL authority to make decisions without consulting the user, provided the decision: - Has only private/internal impact (code structure, implementation approach, task ordering, agent coordination) - Is REVERSIBLE (can be undone in a subsequent PR or commit) This includes: - Resolving spec ambiguities (choosing between reasonable interpretations) - Re-ordering tasks or shifting work between agents - Approving minor scope adjustments (e.g., adding a helper function not in the spec, renaming an internal API) - Deciding on implementation approaches when agents or Seren escalate - Resolving cross-stream conflicts by choosing which agent yields - Adjusting GC phase ordering if benchmarks suggest a different sequence - Changing branch/merge order for dependency reasons Decisions that require user consultation (NOT reversible or public-facing): - Dropping a planned feature from 0.3.0 scope - Changing user-visible syntax or semantics beyond what specs define - Altering the public merge-back content decisions - Any change to the release process or version numbering Coordination Protocol Cross-Stream Dependencies Dependency Producer Consumer Signal eu-4af (symbol interning) done Niamh Callum (for eu-kfe) orchestrator notifies Callum eu-u1m (polymorphic lt) done Callum Callum (for eu-ert) internal, same agent eu-da3 (string comparison) done Callum Callum (for eu-ert) internal, same agent eu-twg (error tests) done Fricka Callum (for eu-dlr) orchestrator notifies Callum Communication Flow Niamh/Callum/Fricka/Ravi \u2192 Seren (PR ready) Seren \u2192 authoring agent (feedback / merge confirmation) any agent \u2192 orchestrator (task complete / escalation) orchestrator \u2192 any agent (unblock / new assignment) orchestrator \u2192 Seren (priority guidance) Coding agents do NOT message each other directly. All cross-stream coordination goes through the orchestrator. Ralph-Loop Iteration Pattern For each task, agents follow this cycle: 1. Read the design plan 2. Implement 3. cargo fmt --all 4. cargo clippy --all-targets -- -D warnings 5. cargo test 6. If 3-5 fail: fix and go to 3 (up to 5 iterations) 7. If pass: commit, push, create PR, notify Seren 8. If 5 iterations exhausted: escalate to Seren Seren has a similar loop for merge issues: 1. Review PR 2. If issues: send specific feedback, agent fixes, re-review (up to 2 rounds) 3. If clean: merge to master, verify CI 4. If 2 rounds don't resolve: escalate to orchestrator Launch Sequence Step 0: Pre-launch Fixes (orchestrator, before spawning agents) Fix CI clippy to use --all-targets in .github/workflows/build-rust.yaml (line 63: cargo clippy -- -D warnings \u2192 cargo clippy --all-targets -- -D warnings ) This aligns CI with the CLAUDE.md pre-commit checklist and what agents validate locally. Commit and push this fix to master before creating worktrees. Step 1: Infrastructure Setup (orchestrator) Create worktree directories Create worktrees from master (after the CI fix is on master) Create the team via TeamCreate Set up initial task list from beads Step 2: Spawn Agents (orchestrator) Spawn all 5 agents with their briefs Each agent starts by reading their assigned design plans Coding agents begin their first tasks immediately Step 3: Steady State Agents work autonomously, creating PRs Reviewer processes PRs as they arrive Orchestrator monitors progress, manages beads, unblocks dependencies When eu-4af merges, orchestrator signals Callum to start eu-kfe Step 4: Completion All PRs merged to master Full test suite passes on master Orchestrator closes beads and syncs Orchestrator shuts down team Verification After all work is merged: 1. cargo test \u2014 full suite passes 2. cargo clippy --all-targets -- -D warnings \u2014 clean 3. cargo fmt --all -- --check \u2014 clean 4. cargo build --release \u2014 builds successfully 5. target/release/eu harness/test -T \u2014 release binary passes harness 6. All beads for targeted features are closed 7. bd sync \u2014 beads fully synced Context Management Large tasks (especially eu-181d GC Immix with ~15+ sub-steps) risk exhausting agent context windows. Mitigation: One branch/PR per sub-bead : Each sub-bead is a self-contained unit of work. The agent completes it, creates a PR, and moves on. Re-briefing between sub-beads : When an agent finishes a sub-bead, the orchestrator can send a fresh brief for the next one, including any relevant context from the completed work. Agent resumption : If an agent is suspended or loses context, the orchestrator re-spawns it with a focused prompt referencing the design plan and the current state of the branch. Design plans as ground truth : The docs/plans/ files are the authoritative reference. Agents re-read them at the start of each sub-bead rather than relying on accumulated context. PR descriptions as memory : Each completed PR's description captures what was done and why. This serves as durable context for subsequent work. For Niamh's GC Immix work specifically: each of the 5 evacuation sub-steps (a-e) is a separate PR. If context runs out mid-step, the orchestrator re-briefs Niamh with just that sub-step's scope. Phase 5: Public Merge-Back (eu-4bkv) After all 0.3.0 work is verified on master, merge the private repo back to the public curvelogic/eucalypt repository. Pre-Merge Content Decisions Item Decision .beads/ directory Exclude \u2014 start fresh with GitHub issues on public repo openspec/ directory Exclude \u2014 proposals are internal planning artefacts .claude/ , .gemini/ , .opencode/ Exclude \u2014 AI tooling config is developer-specific docs/plans/ design documents Remove \u2014 replace with proper user-facing documentation Commit history Squash/rebase to clean public history (see below) Documentation Replacement Before the merge, replace docs/plans/ with public-facing documentation: CHANGELOG.md \u2014 Comprehensive 0.3.0 release notes covering all new features, improvements, and breaking changes. Grouped by category (language features, performance, tooling, prelude additions). Updated README.md \u2014 Reflect new capabilities (LSP, ZDT literals, sets, sorting, deep-find, etc.). Update installation and usage sections. Architecture guide ( docs/architecture.md ) \u2014 High-level overview of the codebase for contributors. Cover the compilation pipeline, STG machine, GC, prelude, and test infrastructure. Generated prelude reference \u2014 Output from eu-ber (Fricka's docs gen work). Auto-generated from prelude doc metadata. LSP setup guide ( docs/lsp.md ) \u2014 How to configure Emacs and VS Code with the language server. Merge-Back Steps Create clean branch : git checkout -b release/0.3.0 master Remove excluded content : rm -rf .beads/ openspec/ .claude/ .gemini/ .opencode/ docs/plans/ Add new documentation : CHANGELOG.md, updated README, architecture guide, LSP guide (as listed above) Verify build : cargo test && cargo clippy --all-targets -- -D warnings Squash to single commit against upstream/master : one commit titled \"Eucalypt 0.3.0\" with a comprehensive message summarising all changes. Push to public : git push upstream release/0.3.0 Create PR on curvelogic/eucalypt for review After merge : Tag as v0.3.0 , create GitHub release CI/CD on Public Repo The public repo's CI/CD workflows are identical to the private repo (zero drift). Three workflow files: build-rust.yaml \u2014 On every push/PR: check, test (ubuntu + macOS), fmt, clippy. On master only: builds release binaries (linux amd64 + macOS), generates changelog via Claude API, creates draft GitHub release. release.yaml \u2014 On published release: updates latest branch, generates homebrew formula. docs.yaml \u2014 Documentation workflow. Changes from 0.3.0 that affect CI : - Clippy flag change: cargo clippy -- -D warnings \u2192 cargo clippy --all-targets -- -D warnings (Step 0 fix, carried in the squashed commit) - New crate dependencies (lsp-server, lsp-types, semver, base64, sha2) \u2014 these are in Cargo.toml and will be picked up automatically - New eu lsp subcommand \u2014 no CI impact - No other workflow changes needed The CI will work on public without modification after the merge. Changelog and Release Process The public repo auto-generates release notes from git log latest..HEAD via the Claude API (using .github/release-notes-prompt.md ). Since we're squashing to a single commit, the auto-generated changelog would only see that one commit. Strategy : Write a comprehensive squash commit message that lists all major changes in a structured format. The Claude API will use this to generate polished release notes. The commit message should include: Eucalypt 0.3.0 Major release with performance improvements, new language features, expanded prelude, developer tooling, and LSP support. ## Performance - Symbol interning: SymbolId(u32) replaces heap-allocated symbols - Block indexing: O(1) lookup for blocks with 16+ keys - STG case optimisations: known-constructor, tag-only, dead-alt, wrapper - DCE inside blocks: static-access dead code elimination - GC Immix completion: lazy sweeping and opportunistic evacuation ## Language Features - ZDT literals: t\"2023-01-15T10:30:00Z\" syntax - Set data type: {| :a, :b, :c |} with full set operations - Polymorphic comparison: lt/gt work for numbers, strings, symbols, ZDTs ## Prelude - Sorting: sort-nums, sort-strs, sort-by, sort-by-num, sort-by-str - String comparison: str.lt, str.gt, str.lte, str.gte, sort-keys - Deep find: deep-find, deep-find-first, deep-find-paths, deep-query - Utilities: eu.requires, str.base64-encode, str.base64-decode, str.sha256 - Catch {name: name} self-reference cycles ## Tooling - LSP server: eu lsp with diagnostics, document symbols, folding - VS Code extension with TextMate grammar - Error message test infrastructure (.expect sidecars) - Benchmark support in test harness - Generated prelude reference documentation ## String Intrinsics - Streaming return for SPLIT, MATCH, MATCHES, LETTERS After the squash commit is pushed to public master, CI will: 1. Run check/test/fmt/clippy (should all pass) 2. Build release binaries 3. Generate changelog from the commit message via Claude API 4. Create a draft GitHub release The orchestrator then: 1. Reviews the draft release notes 2. Edits if needed (the Claude API output is a starting point) 3. Publishes the release 4. release.yaml runs: updates latest branch, generates homebrew formula Who Does This Seren prepares the documentation (steps 1-4). The orchestrator handles the git operations (steps 5-8) since interactive rebase and force-push require human judgement. Timing and Approval Gate This phase starts ONLY after: 1. Step 4 (Completion) of the main implementation is fully verified 2. The orchestrator presents a summary of all completed work to the user 3. The user explicitly approves proceeding with the public merge-back No agent or the orchestrator may touch the public repository (upstream) until the user has reviewed the final state of the private repo and given the go-ahead. This is a hard gate \u2014 not a formality. During 0.3.0 development (Steps 0-4), the user is not consulted unless they are actively watching and showing interest. The orchestrator operates autonomously within its decision authority. Risk Mitigation Risk Mitigation Merge conflicts in shared files (intrinsics.rs, mod.rs, prelude.eu) Reviewer merges sequentially; agents always branch from latest master Agent diverges from design plan Reviewer checks every PR against the plan GC/VM changes break correctness Niamh runs full test suite; benchmarks before/after Agents spin on unfixable issues 5-iteration cap, then escalate Beads get out of sync Only orchestrator manages beads Context exhaustion on long tasks Each task is one branch/PR; agents can be resumed Concurrent cargo builds in worktrees Separate worktrees have separate target dirs Agent context exhaustion on large tasks One branch/PR per sub-bead; orchestrator re-briefs between sub-beads Public merge-back loses important context Design decisions captured in docs, not just plans; public-ready code from start Internal references leak to public PUBLIC-READY rule in shared guidelines; Seren checks during review","title":"Eucalypt v0.3.0 Agent Team Implementation Plan"},{"location":"plans/2026-02-06-agent-team-plan/#eucalypt-v030-agent-team-implementation-plan","text":"","title":"Eucalypt v0.3.0 Agent Team Implementation Plan"},{"location":"plans/2026-02-06-agent-team-plan/#context","text":"Eucalypt v0.3.0 has 16 design plans covering VM improvements, prelude extensions, tooling enhancements, and an LSP server. All plans have been reviewed and cross-checked. The work is tracked in beads with dependency graphs. We need a team of autonomous agents to implement this in parallel, with a review/architect agent ensuring quality and coherence.","title":"Context"},{"location":"plans/2026-02-06-agent-team-plan/#team-structure","text":"Name Role Worktree Scope Niamh VM, GC, memory, STG, DCE eucalypt-vm eu-4af, eu-brj, eu-xg2x, eu-8d8, eu-181d, eu-40jb Callum Prelude, intrinsics, features, ZDT literal eucalypt-prelude Quick wins, eu-da3, eu-ert, eu-kfe, eu-kd1, eu-z0l Fricka Tester, benchmarks, docs gen eucalypt-tooling eu-twg, eu-01v, eu-ber Ravi LSP server + editors eucalypt-lsp eu-307 (all phases) Seren Architect, PR review, docs main repo PR gate, cross-stream, docs freshness, eu-ptu Orchestrator (me): delegate, unblock, manage beads, handle escalations.","title":"Team Structure"},{"location":"plans/2026-02-06-agent-team-plan/#git-strategy","text":"","title":"Git Strategy"},{"location":"plans/2026-02-06-agent-team-plan/#worktree-setup","text":"Each coding agent gets its own git worktree branched from master. This avoids checkout conflicts and gives each agent its own target/ directory for independent builds. # Create worktree directory mkdir -p /Users/greg/dev/curvelogic/eucalypt-worktrees # Create one worktree per coding agent, each starting from master git worktree add /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-vm master git worktree add /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-prelude master git worktree add /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-tooling master git worktree add /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-lsp master The reviewer works from the main repo directory.","title":"Worktree Setup"},{"location":"plans/2026-02-06-agent-team-plan/#branch-per-task-flow","text":"Each bead task gets its own branch: feature/eu-XXXX-short-description Agent workflow per task: 1. In their worktree: git fetch origin && git checkout -b feature/eu-XXXX-desc origin/master 2. Implement, test, commit (multiple commits fine) 3. Push: git push -u origin feature/eu-XXXX-desc 4. Create PR: gh pr create --title \"...\" --body \"...\" 5. Notify reviewer via SendMessage 6. Wait for merge (reviewer handles this) 7. After merge: git fetch origin && git checkout -b feature/eu-YYYY-next origin/master","title":"Branch-per-Task Flow"},{"location":"plans/2026-02-06-agent-team-plan/#merge-sequencing","text":"Seren squash-merges each PR (one clean commit per bead task) Squash commit message format: feat(eu-XXXX): Short description of change with a body summarising what was done and why After each merge, Seren runs full test suite on master If merge conflicts arise, Seren resolves or asks the authoring agent Agents always branch from latest origin/master for each new task","title":"Merge Sequencing"},{"location":"plans/2026-02-06-agent-team-plan/#agent-operating-guidelines","text":"","title":"Agent Operating Guidelines"},{"location":"plans/2026-02-06-agent-team-plan/#shared-rules-all-agents","text":"WORKTREE: You work EXCLUSIVELY in your assigned worktree directory. All file paths and all cargo/git commands use that directory. QUALITY GATE: Before marking any task done, you MUST pass: 1. cargo fmt --all 2. cargo clippy --all-targets -- -D warnings 3. cargo test If any step fails, fix and retry (up to 5 attempts). After 5 failures on the same issue, escalate to reviewer. BRANCHES: One branch per bead task. Branch from latest origin/master. Never commit directly to master. COMMITS: Clear, descriptive messages. Include bead ID. Example: \"feat(eu-4af): Add SymbolId and SymbolPool data structures\" UK ENGLISH: All comments, docs, error messages use UK English. PUBLIC-READY: All code will be merged to the public eucalypt repo after 0.3.0. Write everything as if it's public from day one: - No internal jargon, no references to private repos or tools - Clean, helpful error messages that make sense to external users - Comments explain \"why\", not \"what\" \u2014 assume a competent Rust reader - Doc metadata on prelude functions (` \"description\") is user-facing CLIPPY: Fix ALL warnings. Never use #[allow()] to suppress. DECISION AUTHORITY: You CAN decide without escalating: - Implementation details not covered by the spec (choose sensibly, document in PR description) - Internal API additions (helper functions, internal modules) - Performance trade-offs within the spec's stated approach - Test strategy and coverage details beyond what the spec lists - Error message wording (following UK English, clear and helpful) - Minor refactoring needed to land your feature cleanly ESCALATION: Send a message to Seren ONLY when: - You've failed 5 fix attempts on the same issue - You discover a cross-stream conflict affecting another agent - You need to change a user-visible API not covered by the spec - The spec is fundamentally unclear about the desired behaviour (not just missing detail \u2014 missing intent) DESIGN PLANS: Read docs/plans/*.md for your assigned features BEFORE starting implementation. These are the source of truth. OPPORTUNISTIC P3 FIXES: When you are already modifying a file, fix nearby issues from these beads if the fix is trivial and local: - eu-0yz: Replace unwrap() with expect(\"descriptive message\") - eu-2254: (Niamh only) Lazy iteration for stack_trace if touching vm.rs - eu-mrg: Remove or resolve TODO/FIXME/HACK comments you encounter Do NOT go hunting for these. Only fix them if they're in your path. NO BEADS MANAGEMENT: The orchestrator manages beads. You report task completion via SendMessage to the orchestrator. Do not run bd commands. BEAD LIFECYCLE (orchestrator handles all of this): - in_progress: when orchestrator assigns/signals an agent to start - close (sub-bead): after Seren confirms squash-merge to master and master CI passes - close (parent epic): when all child sub-beads are closed - bd sync: orchestrator runs periodically and at session end","title":"Shared Rules (All Agents)"},{"location":"plans/2026-02-06-agent-team-plan/#niamh-vm-specialist","text":"WORKTREE: /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-vm TASK ORDER (strictly serial \u2014 each depends on the previous): 1. eu-4af: Symbol interning (eu-e1p3 \u2192 eu-jo9y \u2192 eu-pq9i \u2192 eu-uaoj \u2192 eu-yvpz \u2192 eu-eysh \u2192 eu-r9bv) 2. eu-brj: Block indexing (eu-z5br \u2192 eu-wnkl \u2192 eu-pb3e \u2192 eu-piho) 3. eu-xg2x: STG optimisations (eu-knx, eu-otxv, eu-354y, eu-drh0 \u2014 any order within) 4. eu-8d8: DCE inside blocks (eu-qmw3 \u2192 eu-eezd \u2192 eu-j642) 5. eu-40jb: String intrinsics phase 1 (eu-cb9k \u2192 eu-nq77) 6. eu-181d: GC Immix \u2014 all 3 phases, hyper-incrementally: Phase 1: Benchmarking infrastructure (statistics, --statistics-file, Criterion benchmarks, gc-bench.sh, workflow docs) Phase 2: Lazy sweeping \u2014 implement behind conditional, validate with benchmarks, then switch on in a separate small commit Phase 3: Evacuation \u2014 land in stages: a. Forwarding pointer support in AllocHeader (inert, tested in isolation) b. Evacuation candidate selection logic (inert, unit tested) c. Evacuation during marking (disabled by default, tested via unit tests) d. Reference updating in GcScannable (disabled, tested per-type) e. Enable opportunistic evacuation (small commit, full validation) Each sub-step is a separate branch/PR, validated independently. Code can land before it's active. The switch-on is always a separate, small, reviewable commit. BENCHMARKING DISCIPLINE: - After eu-4af: benchmark symbol-heavy programs before/after - After eu-brj: benchmark block lookup before/after - After each STG optimisation: check ticks/allocs in -S output - Before/after every GC phase: full gc-bench.sh comparison - Record all results in PR descriptions SPECIAL RULES: - Memory code is safety-critical. Document all unsafe blocks. - When changing GcScannable, verify ALL implementors are updated. - Symbol interning changes Native::Sym everywhere \u2014 grep exhaustively. - After completing eu-4af, notify orchestrator so eu-kfe can start. - GC changes: never enable new behaviour in the same PR that adds the code. Implement \u2192 validate \u2192 enable are separate PRs.","title":"Niamh (VM Specialist)"},{"location":"plans/2026-02-06-agent-team-plan/#callum-prelude-features","text":"WORKTREE: /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-prelude TASK ORDER: Phase 1 (immediate, no dependencies): - eu-u1m: Polymorphic lt/gt for ZDTs + strings + symbols - eu-nbc: Version assertions (eu.requires) - eu-dyx: Base64 encode/decode - eu-dd5: SHA-256 hash Phase 2 (after eu-u1m): - eu-da3: String comparison intrinsics (str.lt etc.) + sort-keys - eu-dlr: Catch {name: name} recursion (depends on eu-twg for error tests) Phase 3 (after eu-da3 + eu-u1m): - eu-ert: Sorting lists (sort-nums, sort-strs, sort-by etc.) Phase 4 (after eu-4af merges to master): - eu-kfe: Set implementation (Primitive::Sym uses SymbolId) Phase 5 (independent, can start any time): - eu-kd1: ZDT literal syntax (eu-yfcw \u2192 eu-7y85 \u2192 eu-qarv, then tests) Phase 6 (independent, lower priority): - eu-z0l: Deep find functions NAMING CONVENTION: - String comparison goes INSIDE str block: str.lt, str.gt, str.lte, str.gte - New intrinsics follow existing pattern in src/eval/stg/ - Prelude functions have doc metadata: ` \"description\" INTRINSIC PATTERN: Read an existing intrinsic (e.g., src/eval/stg/string.rs) and follow the exact pattern: struct, name(), execute(), CallGlobal trait, register in src/eval/stg/mod.rs. HARNESS TESTS: Every new feature gets harness tests in harness/test/. Follow the existing numbered naming: find the highest number, increment by 1.","title":"Callum (Prelude &amp; Features)"},{"location":"plans/2026-02-06-agent-team-plan/#fricka-tooling-infrastructure","text":"WORKTREE: /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-tooling TASK ORDER: Phase 1 (independent, start immediately): - eu-twg: Error message tests (eu-aiux \u2192 eu-icdc \u2192 eu-i1pi) - eu-01v: Benchmarks in tester (eu-s5n5 \u2192 eu-asdm \u2192 eu-2c1x \u2192 eu-1yxc \u2192 eu-f2pk) Phase 2 (after prelude stabilises): - eu-ber: Prelude docs generator (eu-980b \u2192 eu-j2pu \u2192 eu-9ez6) TESTER CHANGES: The tester code is in src/driver/tester.rs and lib/test.eu. Test plan analysis is in testplan.rs (or similar). Read the existing code thoroughly before modifying. ERROR TESTS: - .expect sidecars go alongside .eu files in harness/test/errors/ - Format: YAML with exit: and stderr: fields - Start with 3 smoke tests, then migrate the rest BENCHMARKS: - bench- prefixed targets in test files - Report ticks, allocs, max-stack to stderr - Stats already captured in evidence \u2014 surface them","title":"Fricka (Tooling &amp; Infrastructure)"},{"location":"plans/2026-02-06-agent-team-plan/#ravi-lsp-server","text":"WORKTREE: /Users/greg/dev/curvelogic/eucalypt-worktrees/eucalypt-lsp TASK ORDER (phase 1 only for initial sprint): 1. eu-307.1: LSP scaffold (eu lsp subcommand, lsp-server, stdio loop) 2. eu-307.2: Diagnostics from Rowan parser 3. eu-307.3: Document symbols 4. eu-307.4: Folding and selection ranges 5. eu-307.5: VS Code extension DEPENDENCIES: - Add lsp-server and lsp-types to Cargo.toml - Wire eu lsp subcommand into src/driver/options.rs clap structure - New module: src/driver/lsp.rs (and src/driver/lsp/ submodules) TESTING: - Unit tests for each LSP feature - Manual testing via VS Code extension during development - Automated test: spawn eu lsp, send JSON-RPC messages, check responses SCOPE: Phase 1 only. Do NOT start phase 2 (semantic features) without explicit go-ahead from orchestrator.","title":"Ravi (LSP Server)"},{"location":"plans/2026-02-06-agent-team-plan/#seren-reviewer-architect","text":"DIRECTORY: /Users/greg/dev/curvelogic/eucalypt (main repo) ROLE: Architect, PR reviewer, documentation guardian. PR REVIEW PROCESS: 1. When notified of a PR, fetch and check out the branch 2. Read the diff against master 3. Verify: a. Code matches the design plan in docs/plans/ b. All tests pass (cargo test) c. Clippy clean (cargo clippy --all-targets -- -D warnings) d. Formatting clean (cargo fmt --all -- --check) e. UK English in comments/docs/errors f. No #[allow()] suppressions g. Harness tests added for new features h. No unintended changes to other features 4. If issues found: send specific feedback to the authoring agent 5. If clean: squash-merge to master via gh pr merge --squash 6. After merge: verify master CI passes 7. Notify orchestrator that eu-XXXX has merged (triggers bead close) ARCHITECTURAL OVERSIGHT: - Watch for cross-stream conflicts (e.g., two agents modifying intrinsics.rs) - Ensure symbol interning changes propagate correctly - Flag if an agent deviates from the design plan - Check that new intrinsics follow the established pattern DOCUMENTATION OVERSIGHT: - Flag if harness tests are missing for new features - Check that prelude doc metadata (` \"description\") is present - Note if docs/plans/ need updating based on implementation changes - Track whether docs/ user-facing docs need refresh DOCS REFRESH (eu-ptu): - After the bulk of features have landed, assess and fix the docs build - Refresh user-facing documentation to reflect 0.3.0 features - This feeds into Phase 5 documentation replacement work DECISION AUTHORITY: You CAN decide without escalating: - Architectural details not specified in the plans (choose the simpler approach, document in PR review comments) - Merge conflict resolution for trivial conflicts (adjacent lines, import ordering in mod.rs) - Requesting minor code changes from agents (naming, structure, error handling) - Approving reasonable implementation choices agents made beyond what the spec covers ESCALATION: Escalate to orchestrator ONLY when: - Semantic merge conflict between two agents' work - An agent's PR fundamentally deviates from the design plan - Two rounds of feedback haven't resolved an issue - Cross-stream dependency needs re-ordering MERGE CONFLICT RESOLUTION: - Trivial conflicts (e.g., adjacent lines in mod.rs): resolve yourself - Semantic conflicts (overlapping features): escalate to orchestrator ESCALATION TRIAGE: When a coding agent escalates to you (not a PR): 1. Build/clippy failure after 5 attempts: - Read the agent's code and error output - Suggest a concrete fix or alternative approach - If you can identify the root cause, send it back with instructions - If you cannot: escalate to orchestrator 2. Unclear spec intent: - Read the relevant design plan in docs/plans/ - If the intent is recoverable from context, make a decision and document it in a message back to the agent - If the spec is genuinely ambiguous about user-visible behaviour: escalate to orchestrator 3. Cross-stream conflict: - Escalate to orchestrator immediately (they coordinate agents) 4. User-visible API change not in spec: - Escalate to orchestrator (they decide scope changes)","title":"Seren (Reviewer / Architect)"},{"location":"plans/2026-02-06-agent-team-plan/#orchestrator-decision-authority","text":"The orchestrator (me) has FULL authority to make decisions without consulting the user, provided the decision: - Has only private/internal impact (code structure, implementation approach, task ordering, agent coordination) - Is REVERSIBLE (can be undone in a subsequent PR or commit) This includes: - Resolving spec ambiguities (choosing between reasonable interpretations) - Re-ordering tasks or shifting work between agents - Approving minor scope adjustments (e.g., adding a helper function not in the spec, renaming an internal API) - Deciding on implementation approaches when agents or Seren escalate - Resolving cross-stream conflicts by choosing which agent yields - Adjusting GC phase ordering if benchmarks suggest a different sequence - Changing branch/merge order for dependency reasons Decisions that require user consultation (NOT reversible or public-facing): - Dropping a planned feature from 0.3.0 scope - Changing user-visible syntax or semantics beyond what specs define - Altering the public merge-back content decisions - Any change to the release process or version numbering","title":"Orchestrator Decision Authority"},{"location":"plans/2026-02-06-agent-team-plan/#coordination-protocol","text":"","title":"Coordination Protocol"},{"location":"plans/2026-02-06-agent-team-plan/#cross-stream-dependencies","text":"Dependency Producer Consumer Signal eu-4af (symbol interning) done Niamh Callum (for eu-kfe) orchestrator notifies Callum eu-u1m (polymorphic lt) done Callum Callum (for eu-ert) internal, same agent eu-da3 (string comparison) done Callum Callum (for eu-ert) internal, same agent eu-twg (error tests) done Fricka Callum (for eu-dlr) orchestrator notifies Callum","title":"Cross-Stream Dependencies"},{"location":"plans/2026-02-06-agent-team-plan/#communication-flow","text":"Niamh/Callum/Fricka/Ravi \u2192 Seren (PR ready) Seren \u2192 authoring agent (feedback / merge confirmation) any agent \u2192 orchestrator (task complete / escalation) orchestrator \u2192 any agent (unblock / new assignment) orchestrator \u2192 Seren (priority guidance) Coding agents do NOT message each other directly. All cross-stream coordination goes through the orchestrator.","title":"Communication Flow"},{"location":"plans/2026-02-06-agent-team-plan/#ralph-loop-iteration-pattern","text":"For each task, agents follow this cycle: 1. Read the design plan 2. Implement 3. cargo fmt --all 4. cargo clippy --all-targets -- -D warnings 5. cargo test 6. If 3-5 fail: fix and go to 3 (up to 5 iterations) 7. If pass: commit, push, create PR, notify Seren 8. If 5 iterations exhausted: escalate to Seren Seren has a similar loop for merge issues: 1. Review PR 2. If issues: send specific feedback, agent fixes, re-review (up to 2 rounds) 3. If clean: merge to master, verify CI 4. If 2 rounds don't resolve: escalate to orchestrator","title":"Ralph-Loop Iteration Pattern"},{"location":"plans/2026-02-06-agent-team-plan/#launch-sequence","text":"","title":"Launch Sequence"},{"location":"plans/2026-02-06-agent-team-plan/#step-0-pre-launch-fixes-orchestrator-before-spawning-agents","text":"Fix CI clippy to use --all-targets in .github/workflows/build-rust.yaml (line 63: cargo clippy -- -D warnings \u2192 cargo clippy --all-targets -- -D warnings ) This aligns CI with the CLAUDE.md pre-commit checklist and what agents validate locally. Commit and push this fix to master before creating worktrees.","title":"Step 0: Pre-launch Fixes (orchestrator, before spawning agents)"},{"location":"plans/2026-02-06-agent-team-plan/#step-1-infrastructure-setup-orchestrator","text":"Create worktree directories Create worktrees from master (after the CI fix is on master) Create the team via TeamCreate Set up initial task list from beads","title":"Step 1: Infrastructure Setup (orchestrator)"},{"location":"plans/2026-02-06-agent-team-plan/#step-2-spawn-agents-orchestrator","text":"Spawn all 5 agents with their briefs Each agent starts by reading their assigned design plans Coding agents begin their first tasks immediately","title":"Step 2: Spawn Agents (orchestrator)"},{"location":"plans/2026-02-06-agent-team-plan/#step-3-steady-state","text":"Agents work autonomously, creating PRs Reviewer processes PRs as they arrive Orchestrator monitors progress, manages beads, unblocks dependencies When eu-4af merges, orchestrator signals Callum to start eu-kfe","title":"Step 3: Steady State"},{"location":"plans/2026-02-06-agent-team-plan/#step-4-completion","text":"All PRs merged to master Full test suite passes on master Orchestrator closes beads and syncs Orchestrator shuts down team","title":"Step 4: Completion"},{"location":"plans/2026-02-06-agent-team-plan/#verification","text":"After all work is merged: 1. cargo test \u2014 full suite passes 2. cargo clippy --all-targets -- -D warnings \u2014 clean 3. cargo fmt --all -- --check \u2014 clean 4. cargo build --release \u2014 builds successfully 5. target/release/eu harness/test -T \u2014 release binary passes harness 6. All beads for targeted features are closed 7. bd sync \u2014 beads fully synced","title":"Verification"},{"location":"plans/2026-02-06-agent-team-plan/#context-management","text":"Large tasks (especially eu-181d GC Immix with ~15+ sub-steps) risk exhausting agent context windows. Mitigation: One branch/PR per sub-bead : Each sub-bead is a self-contained unit of work. The agent completes it, creates a PR, and moves on. Re-briefing between sub-beads : When an agent finishes a sub-bead, the orchestrator can send a fresh brief for the next one, including any relevant context from the completed work. Agent resumption : If an agent is suspended or loses context, the orchestrator re-spawns it with a focused prompt referencing the design plan and the current state of the branch. Design plans as ground truth : The docs/plans/ files are the authoritative reference. Agents re-read them at the start of each sub-bead rather than relying on accumulated context. PR descriptions as memory : Each completed PR's description captures what was done and why. This serves as durable context for subsequent work. For Niamh's GC Immix work specifically: each of the 5 evacuation sub-steps (a-e) is a separate PR. If context runs out mid-step, the orchestrator re-briefs Niamh with just that sub-step's scope.","title":"Context Management"},{"location":"plans/2026-02-06-agent-team-plan/#phase-5-public-merge-back-eu-4bkv","text":"After all 0.3.0 work is verified on master, merge the private repo back to the public curvelogic/eucalypt repository.","title":"Phase 5: Public Merge-Back (eu-4bkv)"},{"location":"plans/2026-02-06-agent-team-plan/#pre-merge-content-decisions","text":"Item Decision .beads/ directory Exclude \u2014 start fresh with GitHub issues on public repo openspec/ directory Exclude \u2014 proposals are internal planning artefacts .claude/ , .gemini/ , .opencode/ Exclude \u2014 AI tooling config is developer-specific docs/plans/ design documents Remove \u2014 replace with proper user-facing documentation Commit history Squash/rebase to clean public history (see below)","title":"Pre-Merge Content Decisions"},{"location":"plans/2026-02-06-agent-team-plan/#documentation-replacement","text":"Before the merge, replace docs/plans/ with public-facing documentation: CHANGELOG.md \u2014 Comprehensive 0.3.0 release notes covering all new features, improvements, and breaking changes. Grouped by category (language features, performance, tooling, prelude additions). Updated README.md \u2014 Reflect new capabilities (LSP, ZDT literals, sets, sorting, deep-find, etc.). Update installation and usage sections. Architecture guide ( docs/architecture.md ) \u2014 High-level overview of the codebase for contributors. Cover the compilation pipeline, STG machine, GC, prelude, and test infrastructure. Generated prelude reference \u2014 Output from eu-ber (Fricka's docs gen work). Auto-generated from prelude doc metadata. LSP setup guide ( docs/lsp.md ) \u2014 How to configure Emacs and VS Code with the language server.","title":"Documentation Replacement"},{"location":"plans/2026-02-06-agent-team-plan/#merge-back-steps","text":"Create clean branch : git checkout -b release/0.3.0 master Remove excluded content : rm -rf .beads/ openspec/ .claude/ .gemini/ .opencode/ docs/plans/ Add new documentation : CHANGELOG.md, updated README, architecture guide, LSP guide (as listed above) Verify build : cargo test && cargo clippy --all-targets -- -D warnings Squash to single commit against upstream/master : one commit titled \"Eucalypt 0.3.0\" with a comprehensive message summarising all changes. Push to public : git push upstream release/0.3.0 Create PR on curvelogic/eucalypt for review After merge : Tag as v0.3.0 , create GitHub release","title":"Merge-Back Steps"},{"location":"plans/2026-02-06-agent-team-plan/#cicd-on-public-repo","text":"The public repo's CI/CD workflows are identical to the private repo (zero drift). Three workflow files: build-rust.yaml \u2014 On every push/PR: check, test (ubuntu + macOS), fmt, clippy. On master only: builds release binaries (linux amd64 + macOS), generates changelog via Claude API, creates draft GitHub release. release.yaml \u2014 On published release: updates latest branch, generates homebrew formula. docs.yaml \u2014 Documentation workflow. Changes from 0.3.0 that affect CI : - Clippy flag change: cargo clippy -- -D warnings \u2192 cargo clippy --all-targets -- -D warnings (Step 0 fix, carried in the squashed commit) - New crate dependencies (lsp-server, lsp-types, semver, base64, sha2) \u2014 these are in Cargo.toml and will be picked up automatically - New eu lsp subcommand \u2014 no CI impact - No other workflow changes needed The CI will work on public without modification after the merge.","title":"CI/CD on Public Repo"},{"location":"plans/2026-02-06-agent-team-plan/#changelog-and-release-process","text":"The public repo auto-generates release notes from git log latest..HEAD via the Claude API (using .github/release-notes-prompt.md ). Since we're squashing to a single commit, the auto-generated changelog would only see that one commit. Strategy : Write a comprehensive squash commit message that lists all major changes in a structured format. The Claude API will use this to generate polished release notes. The commit message should include: Eucalypt 0.3.0 Major release with performance improvements, new language features, expanded prelude, developer tooling, and LSP support. ## Performance - Symbol interning: SymbolId(u32) replaces heap-allocated symbols - Block indexing: O(1) lookup for blocks with 16+ keys - STG case optimisations: known-constructor, tag-only, dead-alt, wrapper - DCE inside blocks: static-access dead code elimination - GC Immix completion: lazy sweeping and opportunistic evacuation ## Language Features - ZDT literals: t\"2023-01-15T10:30:00Z\" syntax - Set data type: {| :a, :b, :c |} with full set operations - Polymorphic comparison: lt/gt work for numbers, strings, symbols, ZDTs ## Prelude - Sorting: sort-nums, sort-strs, sort-by, sort-by-num, sort-by-str - String comparison: str.lt, str.gt, str.lte, str.gte, sort-keys - Deep find: deep-find, deep-find-first, deep-find-paths, deep-query - Utilities: eu.requires, str.base64-encode, str.base64-decode, str.sha256 - Catch {name: name} self-reference cycles ## Tooling - LSP server: eu lsp with diagnostics, document symbols, folding - VS Code extension with TextMate grammar - Error message test infrastructure (.expect sidecars) - Benchmark support in test harness - Generated prelude reference documentation ## String Intrinsics - Streaming return for SPLIT, MATCH, MATCHES, LETTERS After the squash commit is pushed to public master, CI will: 1. Run check/test/fmt/clippy (should all pass) 2. Build release binaries 3. Generate changelog from the commit message via Claude API 4. Create a draft GitHub release The orchestrator then: 1. Reviews the draft release notes 2. Edits if needed (the Claude API output is a starting point) 3. Publishes the release 4. release.yaml runs: updates latest branch, generates homebrew formula","title":"Changelog and Release Process"},{"location":"plans/2026-02-06-agent-team-plan/#who-does-this","text":"Seren prepares the documentation (steps 1-4). The orchestrator handles the git operations (steps 5-8) since interactive rebase and force-push require human judgement.","title":"Who Does This"},{"location":"plans/2026-02-06-agent-team-plan/#timing-and-approval-gate","text":"This phase starts ONLY after: 1. Step 4 (Completion) of the main implementation is fully verified 2. The orchestrator presents a summary of all completed work to the user 3. The user explicitly approves proceeding with the public merge-back No agent or the orchestrator may touch the public repository (upstream) until the user has reviewed the final state of the private repo and given the go-ahead. This is a hard gate \u2014 not a formality. During 0.3.0 development (Steps 0-4), the user is not consulted unless they are actively watching and showing interest. The orchestrator operates autonomously within its decision authority.","title":"Timing and Approval Gate"},{"location":"plans/2026-02-06-agent-team-plan/#risk-mitigation","text":"Risk Mitigation Merge conflicts in shared files (intrinsics.rs, mod.rs, prelude.eu) Reviewer merges sequentially; agents always branch from latest master Agent diverges from design plan Reviewer checks every PR against the plan GC/VM changes break correctness Niamh runs full test suite; benchmarks before/after Agents spin on unfixable issues 5-iteration cap, then escalate Beads get out of sync Only orchestrator manages beads Context exhaustion on long tasks Each task is one branch/PR; agents can be resumed Concurrent cargo builds in worktrees Separate worktrees have separate target dirs Agent context exhaustion on large tasks One branch/PR per sub-bead; orchestrator re-briefs between sub-beads Public merge-back loses important context Design decisions captured in docs, not just plans; public-ready code from start Internal references leak to public PUBLIC-READY rule in shared guidelines; Seren checks during review","title":"Risk Mitigation"},{"location":"plans/2026-02-06-gc-evacuation-design/","text":"GC Evacuation Design Date: 2026-02-06 1. Overview and Goals Implement opportunistic evacuation for the Immix garbage collector -- Phase 3 of the GC Immix completion plan. During collection, instead of always marking objects in place, the collector can move objects out of fragmented blocks into fresh blocks, compacting the heap. This is the core Immix innovation: selective object movement during the mark phase to reduce fragmentation and improve allocation throughput. Success Criteria Objects in fragmented blocks are evacuated to fresh blocks during collection All existing tests pass throughout ( cargo test ) 008_fragmentation.eu stress test shows improved blocks_recycled and reduced blocks_used Non-fragmented workloads show no regression (evacuation not triggered) Mark phase overhead from forwarding checks is minimal (<5%) All new unsafe code has safety documentation 2. Current State The codebase already has significant scaffolding for evacuation: AllocHeader ( src/eval/memory/header.rs ): Has forwarded_to: Option<NonNull<()>> , is_forwarded() , set_forwarded() , and clear_forwarded() methods -- all present but unused. CollectionStrategy ( src/eval/memory/heap.rs ): Enum with MarkInPlace , SelectiveEvacuation(Vec<usize>) , and DefragmentationSweep variants -- defined but unused. FragmentationAnalysis ( src/eval/memory/heap.rs ): Struct with block density categorisation and analyze_fragmentation() method -- implemented but never drives decisions. GcScannable trait ( src/eval/memory/collect.rs ): Has scan() method that pushes discovered grey pointers but does not support reference rewriting. Needs a companion scan_and_update() method. collect() function ( src/eval/memory/collect.rs ): Takes roots: &dyn GcScannable (immutable borrow). Needs mutable roots for reference updating. Native::Str and Native::Set : Contain RefPtr<HeapString> and RefPtr<HeapSet> heap pointers that are not traced during GC scan. These must be traced for evacuation to correctly update all references. GcScannable Implementations The following types implement GcScannable and will each need a scan_and_update() method: Type File Heap Pointers LambdaForm src/eval/memory/syntax.rs body: NonNull<HeapSyn> HeapSyn src/eval/memory/syntax.rs Various NonNull<HeapSyn> , arrays SynClosure src/eval/machine/env.rs code: RefPtr<HeapSyn> , env: RefPtr<EnvFrame> EnvFrame src/eval/machine/env.rs bindings: Array<SynClosure> , next: Option<NonNull<EnvFrame>> Continuation src/eval/machine/cont.rs Branch tables, environments, fallbacks MachineState src/eval/machine/vm.rs globals , closure , stack (roots) Vec<NonNull<T>> src/eval/memory/collect.rs Generic pointer vector 3. Sub-bead 1: Trace Native::Str and Native::Set Heap Pointers Problem Native::Str(RefPtr<HeapString>) and Native::Set(RefPtr<HeapSet>) contain heap pointers but these are not traced during GC scanning. The HeapSyn::Atom { evaluand } and HeapSyn::Cons { args } paths mark only the Ref array, not the Native variants within Ref::V(...) . For mark-in-place collection this is not a correctness bug (the objects are still reachable via other paths, and line marking keeps them alive). But for evacuation, every pointer that could reference a moved object must be discoverable so it can be updated. Changes File: src/eval/memory/syntax.rs In the GcScannable impl for HeapSyn , when scanning Ref arrays (in Cons , App , Bif arms and anywhere Ref::V(Native::Str(ptr)) or Ref::V(Native::Set(ptr)) can appear), add marking calls: // When iterating Ref values in arrays: match ref_val { Ref::V(Native::Str(ptr)) => { marker.mark(*ptr); } Ref::V(Native::Set(ptr)) => { marker.mark(*ptr); } _ => {} } This does not change the scan output (strings and sets are leaf objects with no further heap references), but it ensures their line marks are set and, crucially, that evacuation can discover and update these pointers. Validation All existing tests pass Add a unit test that allocates a HeapString , creates a Native::Str referencing it, collects, and verifies the string's line is marked 4. Sub-bead 2: Change collect() to Take Mutable Roots Problem The current collect() signature is: pub fn collect(roots: &dyn GcScannable, heap: &mut Heap, clock: &mut Clock, dump_heap: bool) The roots parameter is an immutable borrow. For evacuation, the collector must be able to update root pointers when a root's referent is evacuated. This requires mutable access to the roots. Changes File: src/eval/memory/collect.rs Change the signature to: pub fn collect(roots: &mut dyn GcScannableMut, heap: &mut Heap, clock: &mut Clock, dump_heap: bool) Where GcScannableMut extends GcScannable with mutable scanning. Alternatively, change GcScannable::scan() to take &mut self : pub trait GcScannable { fn scan<'a>( &'a mut self, scope: &'a dyn CollectorScope, marker: &mut CollectorHeapView<'a>, out: &mut Vec<ScanPtr<'a>>, ); } The simpler approach: just change the roots parameter to &mut dyn GcScannable and update the trait to take &mut self . All current implementations already have mutable access available at the call site (the MachineState is owned by Machine ). File: src/eval/machine/vm.rs Update the collect() call in Machine::step() or wherever GC is triggered to pass &mut self.state instead of &self.state . Validation All existing tests pass The change is purely mechanical -- no behaviour change for mark-in-place collection 5. Sub-bead 3: Evacuation Target Block Management Problem When evacuating, the collector needs fresh blocks to copy live objects into. These \"evacuation target blocks\" must be managed separately from normal allocation blocks to avoid interference with the mutator's bump pointer. Changes File: src/eval/memory/heap.rs Add evacuation target management to HeapState : /// Block currently being used as evacuation target during collection evacuation_target: Option<BumpBlock>, /// Blocks filled during evacuation (become part of rest after collection) filled_evacuation_blocks: Vec<BumpBlock>, Add methods to Heap : acquire_evacuation_target() -> Option<&mut BumpBlock> : Get or allocate an evacuation target block. Returns None if the heap limit would be exceeded (fall back to mark-in-place for remaining objects). bump_allocate_in_target(size: usize) -> Option<NonNull<u8>> : Bump allocate into the evacuation target. If the current target is full, move it to filled_evacuation_blocks and acquire a new one. finalise_evacuation() : After collection, move all evacuation blocks (target + filled) into rest . These blocks are freshly allocated and fully live -- no need to sweep them. File: src/eval/memory/collect.rs Add to CollectorHeapView : evacuate_alloc(size: usize) -> Option<NonNull<u8>> : Delegate to heap.bump_allocate_in_target(size) . Validation Unit test: acquire target, allocate into it, finalise, verify blocks appear in rest Verify heap limit is respected (no unbounded allocation during evacuation) 6. Sub-bead 4: Object Evacuation (memcpy + Forwarding) Problem The core evacuation operation: copy an object from a fragmented block to an evacuation target block, set the forwarding pointer in the old location, and mark the new copy. Changes File: src/eval/memory/collect.rs Add evacuate() method to CollectorHeapView : /// Evacuate an object from a candidate block to the evacuation target. /// /// # Safety /// /// - `obj` must point to a valid, live heap object with an AllocHeader /// prefix /// - The object must be in a candidate (fragmented) block /// - The object must not already be forwarded /// - The evacuation target must have sufficient space /// /// Returns the new location of the object, or None if evacuation /// failed (e.g. target block full and no replacement available). pub fn evacuate<T>(&mut self, obj: NonNull<T>) -> Option<NonNull<T>> { let header = self.heap.get_header_mut(obj); let size = header.length() as usize + size_of::<AllocHeader>(); // Allocate space in evacuation target let new_raw = self.heap.bump_allocate_in_target(size)?; // Copy object (header + payload) to new location // SAFETY: Both source and destination are valid, non-overlapping, // properly aligned heap memory. Size is the full allocation // including header. unsafe { let src = (obj.as_ptr() as *const u8) .sub(size_of::<AllocHeader>()); std::ptr::copy_nonoverlapping(src, new_raw.as_ptr(), size); } // Compute new object pointer (after header) let new_obj = unsafe { NonNull::new_unchecked( new_raw.as_ptr().add(size_of::<AllocHeader>()) as *mut T ) }; // Set forwarding pointer in old header header.set_forwarded(new_obj.cast()); // Mark the new copy self.heap.mark_object(new_obj); self.heap.mark_line(new_obj); Some(new_obj) } File: src/eval/memory/heap.rs Add get_header_mut() method to Heap : /// Get mutable access to an object's AllocHeader. /// /// # Safety /// /// The pointer must be to a valid heap object allocated with an /// AllocHeader prefix. pub fn get_header_mut<T>(&mut self, obj: NonNull<T>) -> &mut AllocHeader { unsafe { let header_ptr = (obj.as_ptr() as *mut AllocHeader).sub(1); &mut *header_ptr } } Add is_in_candidate_block() method to check whether an object resides in a block marked for evacuation. Validation Unit test: allocate object, evacuate it, verify forwarding pointer set and new copy is accessible Verify old object's header shows is_forwarded() == true Verify new object has correct data (byte-for-byte comparison) 7. Sub-bead 5: Implement scan_and_update for All GcScannable Types Problem During an evacuating collection, after objects are copied and forwarding pointers set, all live objects must have their internal pointers updated to reflect the new locations. The existing scan() method only reads pointers -- it cannot rewrite them. Design Add a scan_and_update() method to GcScannable : pub trait GcScannable { fn scan<'a>( &'a self, scope: &'a dyn CollectorScope, marker: &mut CollectorHeapView<'a>, out: &mut Vec<ScanPtr<'a>>, ); /// Scan this object and update any forwarded pointers in place. /// Called during the update phase of an evacuating collection. /// Default implementation does nothing (for types with no /// rewritable pointers). fn scan_and_update<'a>( &'a mut self, _heap: &'a CollectorHeapView<'a>, ) {} } Changes per Type LambdaForm ( src/eval/memory/syntax.rs ): - Check if body pointer is forwarded; if so, update to new location HeapSyn ( src/eval/memory/syntax.rs ): - For each NonNull<HeapSyn> field (in Case , Let , LetRec , Ann , DeMeta ): check forwarding, update pointer - For each Array<Ref> (in Cons , App , Bif ): iterate refs, check Native::Str / Native::Set pointers for forwarding, update - For Array<LambdaForm> (in Let , LetRec ): iterate, check body pointers - For Array<Option<NonNull<HeapSyn>>> (branch tables in Case ): iterate, check forwarding, update SynClosure ( src/eval/machine/env.rs ): - Check code and env pointers for forwarding, update EnvFrame ( src/eval/machine/env.rs ): - Check next pointer for forwarding, update - Iterate bindings array, update each closure's pointers Continuation ( src/eval/machine/cont.rs ): - For each variant: check all NonNull fields for forwarding, update - Branch tables: iterate and update forwarded entries MachineState ( src/eval/machine/vm.rs ): - Check globals , closure.code , closure.env for forwarding, update - Iterate stack , update forwarded continuation pointers Vec<NonNull<T>> ( src/eval/memory/collect.rs ): - Iterate, check each pointer for forwarding, update in place Forwarding Check Helper Add to CollectorHeapView : /// If the object at `ptr` has been forwarded, return the new location. /// Otherwise return None. pub fn forwarded_to<T>(&self, ptr: NonNull<T>) -> Option<NonNull<T>> { let header = self.heap.get_header(ptr); if header.is_forwarded() { header.forwarded_to.map(|p| p.cast()) } else { None } } Validation Unit test per type: create object with pointers to evacuated objects, call scan_and_update() , verify pointers updated Integration test: full collection cycle with evacuation, verify all references resolve correctly 8. Sub-bead 6: Wire Evacuation into collect() Cycle Problem Integrate the evacuation components into the main collect() function. Design The evacuating collection cycle proceeds as: Analyse fragmentation : Call analyze_fragmentation() on the heap to identify candidate blocks (occupancy below threshold, e.g. <50% of lines marked in previous cycle) Decide strategy : If candidate blocks exist and evacuation target space is available, run an evacuating cycle. Otherwise, fall back to mark-in-place. Mark phase (with evacuation) : During the normal mark-and-trace loop, when scanning reaches an object in a candidate block: Instead of marking in place, call evacuate() to copy to target Set forwarding pointer in old location Push the new copy onto the scan queue (to trace its children) Update phase : After marking completes, traverse all live objects and roots calling scan_and_update() to rewrite any pointers to forwarded objects. Finalise : Call finalise_evacuation() to integrate target blocks into the heap. Candidate blocks are now fully dead (all live objects moved out) and can be returned to the free list without sweeping. Changes File: src/eval/memory/collect.rs pub fn collect( roots: &mut dyn GcScannable, heap: &mut Heap, clock: &mut Clock, dump_heap: bool, ) { clock.switch(ThreadOccupation::CollectorMark); // Decide strategy let strategy = heap.choose_collection_strategy(); let mut heap_view = CollectorHeapView { heap }; heap_view.reset(); let mut queue = VecDeque::default(); let mut scan_buffer = Vec::new(); let scope = Scope(); // Mark roots roots.scan(&scope, &mut heap_view, &mut scan_buffer); queue.extend(scan_buffer.drain(..)); // Trace while let Some(scanptr) = queue.pop_front() { // If evacuating and object is in candidate block, // evacuate instead of scanning in place if strategy.is_evacuating() { if let Some(new_ptr) = maybe_evacuate(&mut heap_view, &scanptr) { // Push new location for scanning queue.push_back(new_ptr); continue; } } scanptr.get().scan(&scope, &mut heap_view, &mut scan_buffer); queue.extend(scan_buffer.drain(..)); } // Update phase (only if evacuation occurred) if strategy.is_evacuating() { // Update root pointers roots.scan_and_update(&heap_view); // Update all live objects (traverse marked objects) heap_view.update_all_forwarded_refs(); } clock.switch(ThreadOccupation::CollectorSweep); heap_view.defer_sweep(); // Finalise evacuation blocks if strategy.is_evacuating() { heap.finalise_evacuation(); } heap.record_collection(); heap.flip_mark_state(); } File: src/eval/memory/heap.rs Add choose_collection_strategy() -> CollectionStrategy : pub fn choose_collection_strategy(&self) -> CollectionStrategy { let analysis = self.analyze_fragmentation(); // Only evacuate if fragmentation is significant if analysis.fragmentation_ratio < 0.2 { return CollectionStrategy::MarkInPlace; } // Need at least one free block for evacuation target if !self.can_acquire_evacuation_target() { return CollectionStrategy::MarkInPlace; } // Collect indices of candidate blocks (sparse + fragmented) let candidates = self.candidate_block_indices(&analysis); if candidates.is_empty() { CollectionStrategy::MarkInPlace } else { CollectionStrategy::SelectiveEvacuation(candidates) } } Interaction with Lazy Sweeping Evacuated candidate blocks become fully dead after evacuation (all live objects moved out). They bypass the lazy sweep queue and go directly to the free list. Non-candidate blocks follow the normal lazy sweep path. Validation All existing tests pass 008_fragmentation.eu shows measurably better blocks_recycled Non-fragmented benchmarks show no regression (strategy remains MarkInPlace ) Benchmark comparison confirms mark phase is only slightly slower (forwarding check overhead) 9. Sub-bead 7: Evacuation Testing and Validation Test Plan Unit Tests ( src/eval/memory/collect.rs::tests ): test_evacuate_single_object : Allocate one object in a block, mark block as candidate, collect with evacuation, verify object accessible at new location via forwarding pointer. test_evacuate_with_internal_refs : Allocate a Let expression whose body points to another HeapSyn. Evacuate the body. Verify the Let's body pointer is updated to the new location. test_evacuate_preserves_data : Evacuate various object types (LambdaForm, HeapSyn variants, Continuation). Verify data integrity by comparing field values before and after evacuation. test_no_evacuation_when_not_fragmented : Create a non-fragmented heap, run collection, verify CollectionStrategy::MarkInPlace is chosen and no forwarding pointers are set. test_evacuation_target_exhaustion : Fill the heap near its limit, trigger evacuating collection, verify graceful fallback to mark-in-place when no evacuation target can be acquired. test_native_str_pointer_updated : Allocate a HeapString , reference it via Native::Str , evacuate the string, verify the Native::Str pointer is updated. test_native_set_pointer_updated : Same as above for Native::Set / HeapSet . Stress Tests ( harness/test/bench/ ): Run 008_fragmentation.eu with --statistics-file before and after Compare blocks_recycled , blocks_used , mark phase timing Run non-fragmented benchmarks to verify no regression Integration : Full cargo test suite passes ./scripts/gc-bench.sh compare shows improvement in fragmentation metrics without regression elsewhere 10. Sub-bead 8: Safety Documentation for Evacuation Unsafe Code Scope All unsafe blocks introduced by evacuation must have safety comments documenting: What invariant the unsafe code relies on Why the invariant holds at this call site What could go wrong if the invariant is violated Locations Requiring Documentation evacuate() in CollectorHeapView : ptr::copy_nonoverlapping , pointer arithmetic, NonNull::new_unchecked get_header_mut() in Heap : raw pointer dereference bump_allocate_in_target() : raw pointer arithmetic for bump allocation scan_and_update() implementations: mutable pointer rewriting in each GcScannable type forwarded_to() : reading header through raw pointer Documentation Standard Each unsafe block should follow the existing project pattern (visible in ScanPtr::from_non_null and the heap safety model comment at the top of heap.rs ): // SAFETY: <what invariant holds> // - <first reason it holds> // - <second reason it holds> // - <what the caller guarantees> unsafe { // ... } Existing Unsafe to Audit Review and update safety comments for existing forwarding-related code in header.rs that is currently unused but will become live: AllocHeader::set_forwarded() AllocHeader::clear_forwarded() AllocHeader::is_forwarded() Also address eu-w68 (Memory management unsafe code lacks safety documentation) by auditing all existing unsafe blocks in src/eval/memory/ and adding missing safety comments. Appendix A: Dependency Graph Sub-bead 1: Trace Native::Str/Set pointers Sub-bead 2: Mutable roots Sub-bead 3: Evacuation target blocks Sub-bead 4: Object evacuation (memcpy + forwarding) \u2190 depends on 3 Sub-bead 5: scan_and_update for all types \u2190 depends on 1, 2, 4 Sub-bead 6: Wire into collect() cycle \u2190 depends on 2, 3, 4, 5 Sub-bead 7: Testing and validation \u2190 depends on 6 Sub-bead 8: Safety documentation \u2190 depends on 4, 5, 6 Sub-beads 1, 2, and 3 can proceed in parallel. Sub-bead 4 requires 3. Sub-bead 5 requires 1, 2, and 4. Sub-bead 6 brings everything together. Sub-beads 7 and 8 follow. Appendix B: Files Modified File Changes src/eval/memory/collect.rs GcScannable trait extension, collect() signature, CollectorHeapView methods src/eval/memory/header.rs No structural changes (existing forwarding API used) src/eval/memory/heap.rs Evacuation target management, strategy selection, get_header_mut() src/eval/memory/syntax.rs Native::Str / Set tracing, scan_and_update() for LambdaForm , HeapSyn src/eval/machine/env.rs scan_and_update() for SynClosure , EnvFrame src/eval/machine/cont.rs scan_and_update() for Continuation src/eval/machine/vm.rs scan_and_update() for MachineState , mutable root passing Appendix C: Risk Assessment Risk Severity Mitigation Missed pointer update (use-after-move) Critical Comprehensive scan_and_update() coverage + stress tests Native::Str / Set pointers not updated High Sub-bead 1 ensures these are traced; sub-bead 5 ensures they are updated Evacuation target exhaustion Medium Graceful fallback to mark-in-place Mark phase overhead from forwarding checks Low Check is a single bit test; benchmark validates Interaction with lazy sweep Medium Evacuated blocks bypass sweep queue; tested explicitly Mutable root borrow conflicts Low Stop-the-world GC ensures exclusive access Appendix D: Related Beads eu-2ij -- GC: Implement lazy sweeping optimisation (Phase 2, prerequisite, completed) eu-5si -- GC: Implement full reference updating system (Phase 3, this work) eu-w68 -- Memory management unsafe code lacks safety documentation (addressed by sub-bead 8)","title":"GC Evacuation Design"},{"location":"plans/2026-02-06-gc-evacuation-design/#gc-evacuation-design","text":"Date: 2026-02-06","title":"GC Evacuation Design"},{"location":"plans/2026-02-06-gc-evacuation-design/#1-overview-and-goals","text":"Implement opportunistic evacuation for the Immix garbage collector -- Phase 3 of the GC Immix completion plan. During collection, instead of always marking objects in place, the collector can move objects out of fragmented blocks into fresh blocks, compacting the heap. This is the core Immix innovation: selective object movement during the mark phase to reduce fragmentation and improve allocation throughput.","title":"1. Overview and Goals"},{"location":"plans/2026-02-06-gc-evacuation-design/#success-criteria","text":"Objects in fragmented blocks are evacuated to fresh blocks during collection All existing tests pass throughout ( cargo test ) 008_fragmentation.eu stress test shows improved blocks_recycled and reduced blocks_used Non-fragmented workloads show no regression (evacuation not triggered) Mark phase overhead from forwarding checks is minimal (<5%) All new unsafe code has safety documentation","title":"Success Criteria"},{"location":"plans/2026-02-06-gc-evacuation-design/#2-current-state","text":"The codebase already has significant scaffolding for evacuation: AllocHeader ( src/eval/memory/header.rs ): Has forwarded_to: Option<NonNull<()>> , is_forwarded() , set_forwarded() , and clear_forwarded() methods -- all present but unused. CollectionStrategy ( src/eval/memory/heap.rs ): Enum with MarkInPlace , SelectiveEvacuation(Vec<usize>) , and DefragmentationSweep variants -- defined but unused. FragmentationAnalysis ( src/eval/memory/heap.rs ): Struct with block density categorisation and analyze_fragmentation() method -- implemented but never drives decisions. GcScannable trait ( src/eval/memory/collect.rs ): Has scan() method that pushes discovered grey pointers but does not support reference rewriting. Needs a companion scan_and_update() method. collect() function ( src/eval/memory/collect.rs ): Takes roots: &dyn GcScannable (immutable borrow). Needs mutable roots for reference updating. Native::Str and Native::Set : Contain RefPtr<HeapString> and RefPtr<HeapSet> heap pointers that are not traced during GC scan. These must be traced for evacuation to correctly update all references.","title":"2. Current State"},{"location":"plans/2026-02-06-gc-evacuation-design/#gcscannable-implementations","text":"The following types implement GcScannable and will each need a scan_and_update() method: Type File Heap Pointers LambdaForm src/eval/memory/syntax.rs body: NonNull<HeapSyn> HeapSyn src/eval/memory/syntax.rs Various NonNull<HeapSyn> , arrays SynClosure src/eval/machine/env.rs code: RefPtr<HeapSyn> , env: RefPtr<EnvFrame> EnvFrame src/eval/machine/env.rs bindings: Array<SynClosure> , next: Option<NonNull<EnvFrame>> Continuation src/eval/machine/cont.rs Branch tables, environments, fallbacks MachineState src/eval/machine/vm.rs globals , closure , stack (roots) Vec<NonNull<T>> src/eval/memory/collect.rs Generic pointer vector","title":"GcScannable Implementations"},{"location":"plans/2026-02-06-gc-evacuation-design/#3-sub-bead-1-trace-nativestr-and-nativeset-heap-pointers","text":"","title":"3. Sub-bead 1: Trace Native::Str and Native::Set Heap Pointers"},{"location":"plans/2026-02-06-gc-evacuation-design/#problem","text":"Native::Str(RefPtr<HeapString>) and Native::Set(RefPtr<HeapSet>) contain heap pointers but these are not traced during GC scanning. The HeapSyn::Atom { evaluand } and HeapSyn::Cons { args } paths mark only the Ref array, not the Native variants within Ref::V(...) . For mark-in-place collection this is not a correctness bug (the objects are still reachable via other paths, and line marking keeps them alive). But for evacuation, every pointer that could reference a moved object must be discoverable so it can be updated.","title":"Problem"},{"location":"plans/2026-02-06-gc-evacuation-design/#changes","text":"File: src/eval/memory/syntax.rs In the GcScannable impl for HeapSyn , when scanning Ref arrays (in Cons , App , Bif arms and anywhere Ref::V(Native::Str(ptr)) or Ref::V(Native::Set(ptr)) can appear), add marking calls: // When iterating Ref values in arrays: match ref_val { Ref::V(Native::Str(ptr)) => { marker.mark(*ptr); } Ref::V(Native::Set(ptr)) => { marker.mark(*ptr); } _ => {} } This does not change the scan output (strings and sets are leaf objects with no further heap references), but it ensures their line marks are set and, crucially, that evacuation can discover and update these pointers.","title":"Changes"},{"location":"plans/2026-02-06-gc-evacuation-design/#validation","text":"All existing tests pass Add a unit test that allocates a HeapString , creates a Native::Str referencing it, collects, and verifies the string's line is marked","title":"Validation"},{"location":"plans/2026-02-06-gc-evacuation-design/#4-sub-bead-2-change-collect-to-take-mutable-roots","text":"","title":"4. Sub-bead 2: Change collect() to Take Mutable Roots"},{"location":"plans/2026-02-06-gc-evacuation-design/#problem_1","text":"The current collect() signature is: pub fn collect(roots: &dyn GcScannable, heap: &mut Heap, clock: &mut Clock, dump_heap: bool) The roots parameter is an immutable borrow. For evacuation, the collector must be able to update root pointers when a root's referent is evacuated. This requires mutable access to the roots.","title":"Problem"},{"location":"plans/2026-02-06-gc-evacuation-design/#changes_1","text":"File: src/eval/memory/collect.rs Change the signature to: pub fn collect(roots: &mut dyn GcScannableMut, heap: &mut Heap, clock: &mut Clock, dump_heap: bool) Where GcScannableMut extends GcScannable with mutable scanning. Alternatively, change GcScannable::scan() to take &mut self : pub trait GcScannable { fn scan<'a>( &'a mut self, scope: &'a dyn CollectorScope, marker: &mut CollectorHeapView<'a>, out: &mut Vec<ScanPtr<'a>>, ); } The simpler approach: just change the roots parameter to &mut dyn GcScannable and update the trait to take &mut self . All current implementations already have mutable access available at the call site (the MachineState is owned by Machine ). File: src/eval/machine/vm.rs Update the collect() call in Machine::step() or wherever GC is triggered to pass &mut self.state instead of &self.state .","title":"Changes"},{"location":"plans/2026-02-06-gc-evacuation-design/#validation_1","text":"All existing tests pass The change is purely mechanical -- no behaviour change for mark-in-place collection","title":"Validation"},{"location":"plans/2026-02-06-gc-evacuation-design/#5-sub-bead-3-evacuation-target-block-management","text":"","title":"5. Sub-bead 3: Evacuation Target Block Management"},{"location":"plans/2026-02-06-gc-evacuation-design/#problem_2","text":"When evacuating, the collector needs fresh blocks to copy live objects into. These \"evacuation target blocks\" must be managed separately from normal allocation blocks to avoid interference with the mutator's bump pointer.","title":"Problem"},{"location":"plans/2026-02-06-gc-evacuation-design/#changes_2","text":"File: src/eval/memory/heap.rs Add evacuation target management to HeapState : /// Block currently being used as evacuation target during collection evacuation_target: Option<BumpBlock>, /// Blocks filled during evacuation (become part of rest after collection) filled_evacuation_blocks: Vec<BumpBlock>, Add methods to Heap : acquire_evacuation_target() -> Option<&mut BumpBlock> : Get or allocate an evacuation target block. Returns None if the heap limit would be exceeded (fall back to mark-in-place for remaining objects). bump_allocate_in_target(size: usize) -> Option<NonNull<u8>> : Bump allocate into the evacuation target. If the current target is full, move it to filled_evacuation_blocks and acquire a new one. finalise_evacuation() : After collection, move all evacuation blocks (target + filled) into rest . These blocks are freshly allocated and fully live -- no need to sweep them. File: src/eval/memory/collect.rs Add to CollectorHeapView : evacuate_alloc(size: usize) -> Option<NonNull<u8>> : Delegate to heap.bump_allocate_in_target(size) .","title":"Changes"},{"location":"plans/2026-02-06-gc-evacuation-design/#validation_2","text":"Unit test: acquire target, allocate into it, finalise, verify blocks appear in rest Verify heap limit is respected (no unbounded allocation during evacuation)","title":"Validation"},{"location":"plans/2026-02-06-gc-evacuation-design/#6-sub-bead-4-object-evacuation-memcpy-forwarding","text":"","title":"6. Sub-bead 4: Object Evacuation (memcpy + Forwarding)"},{"location":"plans/2026-02-06-gc-evacuation-design/#problem_3","text":"The core evacuation operation: copy an object from a fragmented block to an evacuation target block, set the forwarding pointer in the old location, and mark the new copy.","title":"Problem"},{"location":"plans/2026-02-06-gc-evacuation-design/#changes_3","text":"File: src/eval/memory/collect.rs Add evacuate() method to CollectorHeapView : /// Evacuate an object from a candidate block to the evacuation target. /// /// # Safety /// /// - `obj` must point to a valid, live heap object with an AllocHeader /// prefix /// - The object must be in a candidate (fragmented) block /// - The object must not already be forwarded /// - The evacuation target must have sufficient space /// /// Returns the new location of the object, or None if evacuation /// failed (e.g. target block full and no replacement available). pub fn evacuate<T>(&mut self, obj: NonNull<T>) -> Option<NonNull<T>> { let header = self.heap.get_header_mut(obj); let size = header.length() as usize + size_of::<AllocHeader>(); // Allocate space in evacuation target let new_raw = self.heap.bump_allocate_in_target(size)?; // Copy object (header + payload) to new location // SAFETY: Both source and destination are valid, non-overlapping, // properly aligned heap memory. Size is the full allocation // including header. unsafe { let src = (obj.as_ptr() as *const u8) .sub(size_of::<AllocHeader>()); std::ptr::copy_nonoverlapping(src, new_raw.as_ptr(), size); } // Compute new object pointer (after header) let new_obj = unsafe { NonNull::new_unchecked( new_raw.as_ptr().add(size_of::<AllocHeader>()) as *mut T ) }; // Set forwarding pointer in old header header.set_forwarded(new_obj.cast()); // Mark the new copy self.heap.mark_object(new_obj); self.heap.mark_line(new_obj); Some(new_obj) } File: src/eval/memory/heap.rs Add get_header_mut() method to Heap : /// Get mutable access to an object's AllocHeader. /// /// # Safety /// /// The pointer must be to a valid heap object allocated with an /// AllocHeader prefix. pub fn get_header_mut<T>(&mut self, obj: NonNull<T>) -> &mut AllocHeader { unsafe { let header_ptr = (obj.as_ptr() as *mut AllocHeader).sub(1); &mut *header_ptr } } Add is_in_candidate_block() method to check whether an object resides in a block marked for evacuation.","title":"Changes"},{"location":"plans/2026-02-06-gc-evacuation-design/#validation_3","text":"Unit test: allocate object, evacuate it, verify forwarding pointer set and new copy is accessible Verify old object's header shows is_forwarded() == true Verify new object has correct data (byte-for-byte comparison)","title":"Validation"},{"location":"plans/2026-02-06-gc-evacuation-design/#7-sub-bead-5-implement-scan_and_update-for-all-gcscannable-types","text":"","title":"7. Sub-bead 5: Implement scan_and_update for All GcScannable Types"},{"location":"plans/2026-02-06-gc-evacuation-design/#problem_4","text":"During an evacuating collection, after objects are copied and forwarding pointers set, all live objects must have their internal pointers updated to reflect the new locations. The existing scan() method only reads pointers -- it cannot rewrite them.","title":"Problem"},{"location":"plans/2026-02-06-gc-evacuation-design/#design","text":"Add a scan_and_update() method to GcScannable : pub trait GcScannable { fn scan<'a>( &'a self, scope: &'a dyn CollectorScope, marker: &mut CollectorHeapView<'a>, out: &mut Vec<ScanPtr<'a>>, ); /// Scan this object and update any forwarded pointers in place. /// Called during the update phase of an evacuating collection. /// Default implementation does nothing (for types with no /// rewritable pointers). fn scan_and_update<'a>( &'a mut self, _heap: &'a CollectorHeapView<'a>, ) {} }","title":"Design"},{"location":"plans/2026-02-06-gc-evacuation-design/#changes-per-type","text":"LambdaForm ( src/eval/memory/syntax.rs ): - Check if body pointer is forwarded; if so, update to new location HeapSyn ( src/eval/memory/syntax.rs ): - For each NonNull<HeapSyn> field (in Case , Let , LetRec , Ann , DeMeta ): check forwarding, update pointer - For each Array<Ref> (in Cons , App , Bif ): iterate refs, check Native::Str / Native::Set pointers for forwarding, update - For Array<LambdaForm> (in Let , LetRec ): iterate, check body pointers - For Array<Option<NonNull<HeapSyn>>> (branch tables in Case ): iterate, check forwarding, update SynClosure ( src/eval/machine/env.rs ): - Check code and env pointers for forwarding, update EnvFrame ( src/eval/machine/env.rs ): - Check next pointer for forwarding, update - Iterate bindings array, update each closure's pointers Continuation ( src/eval/machine/cont.rs ): - For each variant: check all NonNull fields for forwarding, update - Branch tables: iterate and update forwarded entries MachineState ( src/eval/machine/vm.rs ): - Check globals , closure.code , closure.env for forwarding, update - Iterate stack , update forwarded continuation pointers Vec<NonNull<T>> ( src/eval/memory/collect.rs ): - Iterate, check each pointer for forwarding, update in place","title":"Changes per Type"},{"location":"plans/2026-02-06-gc-evacuation-design/#forwarding-check-helper","text":"Add to CollectorHeapView : /// If the object at `ptr` has been forwarded, return the new location. /// Otherwise return None. pub fn forwarded_to<T>(&self, ptr: NonNull<T>) -> Option<NonNull<T>> { let header = self.heap.get_header(ptr); if header.is_forwarded() { header.forwarded_to.map(|p| p.cast()) } else { None } }","title":"Forwarding Check Helper"},{"location":"plans/2026-02-06-gc-evacuation-design/#validation_4","text":"Unit test per type: create object with pointers to evacuated objects, call scan_and_update() , verify pointers updated Integration test: full collection cycle with evacuation, verify all references resolve correctly","title":"Validation"},{"location":"plans/2026-02-06-gc-evacuation-design/#8-sub-bead-6-wire-evacuation-into-collect-cycle","text":"","title":"8. Sub-bead 6: Wire Evacuation into collect() Cycle"},{"location":"plans/2026-02-06-gc-evacuation-design/#problem_5","text":"Integrate the evacuation components into the main collect() function.","title":"Problem"},{"location":"plans/2026-02-06-gc-evacuation-design/#design_1","text":"The evacuating collection cycle proceeds as: Analyse fragmentation : Call analyze_fragmentation() on the heap to identify candidate blocks (occupancy below threshold, e.g. <50% of lines marked in previous cycle) Decide strategy : If candidate blocks exist and evacuation target space is available, run an evacuating cycle. Otherwise, fall back to mark-in-place. Mark phase (with evacuation) : During the normal mark-and-trace loop, when scanning reaches an object in a candidate block: Instead of marking in place, call evacuate() to copy to target Set forwarding pointer in old location Push the new copy onto the scan queue (to trace its children) Update phase : After marking completes, traverse all live objects and roots calling scan_and_update() to rewrite any pointers to forwarded objects. Finalise : Call finalise_evacuation() to integrate target blocks into the heap. Candidate blocks are now fully dead (all live objects moved out) and can be returned to the free list without sweeping.","title":"Design"},{"location":"plans/2026-02-06-gc-evacuation-design/#changes_4","text":"File: src/eval/memory/collect.rs pub fn collect( roots: &mut dyn GcScannable, heap: &mut Heap, clock: &mut Clock, dump_heap: bool, ) { clock.switch(ThreadOccupation::CollectorMark); // Decide strategy let strategy = heap.choose_collection_strategy(); let mut heap_view = CollectorHeapView { heap }; heap_view.reset(); let mut queue = VecDeque::default(); let mut scan_buffer = Vec::new(); let scope = Scope(); // Mark roots roots.scan(&scope, &mut heap_view, &mut scan_buffer); queue.extend(scan_buffer.drain(..)); // Trace while let Some(scanptr) = queue.pop_front() { // If evacuating and object is in candidate block, // evacuate instead of scanning in place if strategy.is_evacuating() { if let Some(new_ptr) = maybe_evacuate(&mut heap_view, &scanptr) { // Push new location for scanning queue.push_back(new_ptr); continue; } } scanptr.get().scan(&scope, &mut heap_view, &mut scan_buffer); queue.extend(scan_buffer.drain(..)); } // Update phase (only if evacuation occurred) if strategy.is_evacuating() { // Update root pointers roots.scan_and_update(&heap_view); // Update all live objects (traverse marked objects) heap_view.update_all_forwarded_refs(); } clock.switch(ThreadOccupation::CollectorSweep); heap_view.defer_sweep(); // Finalise evacuation blocks if strategy.is_evacuating() { heap.finalise_evacuation(); } heap.record_collection(); heap.flip_mark_state(); } File: src/eval/memory/heap.rs Add choose_collection_strategy() -> CollectionStrategy : pub fn choose_collection_strategy(&self) -> CollectionStrategy { let analysis = self.analyze_fragmentation(); // Only evacuate if fragmentation is significant if analysis.fragmentation_ratio < 0.2 { return CollectionStrategy::MarkInPlace; } // Need at least one free block for evacuation target if !self.can_acquire_evacuation_target() { return CollectionStrategy::MarkInPlace; } // Collect indices of candidate blocks (sparse + fragmented) let candidates = self.candidate_block_indices(&analysis); if candidates.is_empty() { CollectionStrategy::MarkInPlace } else { CollectionStrategy::SelectiveEvacuation(candidates) } }","title":"Changes"},{"location":"plans/2026-02-06-gc-evacuation-design/#interaction-with-lazy-sweeping","text":"Evacuated candidate blocks become fully dead after evacuation (all live objects moved out). They bypass the lazy sweep queue and go directly to the free list. Non-candidate blocks follow the normal lazy sweep path.","title":"Interaction with Lazy Sweeping"},{"location":"plans/2026-02-06-gc-evacuation-design/#validation_5","text":"All existing tests pass 008_fragmentation.eu shows measurably better blocks_recycled Non-fragmented benchmarks show no regression (strategy remains MarkInPlace ) Benchmark comparison confirms mark phase is only slightly slower (forwarding check overhead)","title":"Validation"},{"location":"plans/2026-02-06-gc-evacuation-design/#9-sub-bead-7-evacuation-testing-and-validation","text":"","title":"9. Sub-bead 7: Evacuation Testing and Validation"},{"location":"plans/2026-02-06-gc-evacuation-design/#test-plan","text":"Unit Tests ( src/eval/memory/collect.rs::tests ): test_evacuate_single_object : Allocate one object in a block, mark block as candidate, collect with evacuation, verify object accessible at new location via forwarding pointer. test_evacuate_with_internal_refs : Allocate a Let expression whose body points to another HeapSyn. Evacuate the body. Verify the Let's body pointer is updated to the new location. test_evacuate_preserves_data : Evacuate various object types (LambdaForm, HeapSyn variants, Continuation). Verify data integrity by comparing field values before and after evacuation. test_no_evacuation_when_not_fragmented : Create a non-fragmented heap, run collection, verify CollectionStrategy::MarkInPlace is chosen and no forwarding pointers are set. test_evacuation_target_exhaustion : Fill the heap near its limit, trigger evacuating collection, verify graceful fallback to mark-in-place when no evacuation target can be acquired. test_native_str_pointer_updated : Allocate a HeapString , reference it via Native::Str , evacuate the string, verify the Native::Str pointer is updated. test_native_set_pointer_updated : Same as above for Native::Set / HeapSet . Stress Tests ( harness/test/bench/ ): Run 008_fragmentation.eu with --statistics-file before and after Compare blocks_recycled , blocks_used , mark phase timing Run non-fragmented benchmarks to verify no regression Integration : Full cargo test suite passes ./scripts/gc-bench.sh compare shows improvement in fragmentation metrics without regression elsewhere","title":"Test Plan"},{"location":"plans/2026-02-06-gc-evacuation-design/#10-sub-bead-8-safety-documentation-for-evacuation-unsafe-code","text":"","title":"10. Sub-bead 8: Safety Documentation for Evacuation Unsafe Code"},{"location":"plans/2026-02-06-gc-evacuation-design/#scope","text":"All unsafe blocks introduced by evacuation must have safety comments documenting: What invariant the unsafe code relies on Why the invariant holds at this call site What could go wrong if the invariant is violated","title":"Scope"},{"location":"plans/2026-02-06-gc-evacuation-design/#locations-requiring-documentation","text":"evacuate() in CollectorHeapView : ptr::copy_nonoverlapping , pointer arithmetic, NonNull::new_unchecked get_header_mut() in Heap : raw pointer dereference bump_allocate_in_target() : raw pointer arithmetic for bump allocation scan_and_update() implementations: mutable pointer rewriting in each GcScannable type forwarded_to() : reading header through raw pointer","title":"Locations Requiring Documentation"},{"location":"plans/2026-02-06-gc-evacuation-design/#documentation-standard","text":"Each unsafe block should follow the existing project pattern (visible in ScanPtr::from_non_null and the heap safety model comment at the top of heap.rs ): // SAFETY: <what invariant holds> // - <first reason it holds> // - <second reason it holds> // - <what the caller guarantees> unsafe { // ... }","title":"Documentation Standard"},{"location":"plans/2026-02-06-gc-evacuation-design/#existing-unsafe-to-audit","text":"Review and update safety comments for existing forwarding-related code in header.rs that is currently unused but will become live: AllocHeader::set_forwarded() AllocHeader::clear_forwarded() AllocHeader::is_forwarded() Also address eu-w68 (Memory management unsafe code lacks safety documentation) by auditing all existing unsafe blocks in src/eval/memory/ and adding missing safety comments.","title":"Existing Unsafe to Audit"},{"location":"plans/2026-02-06-gc-evacuation-design/#appendix-a-dependency-graph","text":"Sub-bead 1: Trace Native::Str/Set pointers Sub-bead 2: Mutable roots Sub-bead 3: Evacuation target blocks Sub-bead 4: Object evacuation (memcpy + forwarding) \u2190 depends on 3 Sub-bead 5: scan_and_update for all types \u2190 depends on 1, 2, 4 Sub-bead 6: Wire into collect() cycle \u2190 depends on 2, 3, 4, 5 Sub-bead 7: Testing and validation \u2190 depends on 6 Sub-bead 8: Safety documentation \u2190 depends on 4, 5, 6 Sub-beads 1, 2, and 3 can proceed in parallel. Sub-bead 4 requires 3. Sub-bead 5 requires 1, 2, and 4. Sub-bead 6 brings everything together. Sub-beads 7 and 8 follow.","title":"Appendix A: Dependency Graph"},{"location":"plans/2026-02-06-gc-evacuation-design/#appendix-b-files-modified","text":"File Changes src/eval/memory/collect.rs GcScannable trait extension, collect() signature, CollectorHeapView methods src/eval/memory/header.rs No structural changes (existing forwarding API used) src/eval/memory/heap.rs Evacuation target management, strategy selection, get_header_mut() src/eval/memory/syntax.rs Native::Str / Set tracing, scan_and_update() for LambdaForm , HeapSyn src/eval/machine/env.rs scan_and_update() for SynClosure , EnvFrame src/eval/machine/cont.rs scan_and_update() for Continuation src/eval/machine/vm.rs scan_and_update() for MachineState , mutable root passing","title":"Appendix B: Files Modified"},{"location":"plans/2026-02-06-gc-evacuation-design/#appendix-c-risk-assessment","text":"Risk Severity Mitigation Missed pointer update (use-after-move) Critical Comprehensive scan_and_update() coverage + stress tests Native::Str / Set pointers not updated High Sub-bead 1 ensures these are traced; sub-bead 5 ensures they are updated Evacuation target exhaustion Medium Graceful fallback to mark-in-place Mark phase overhead from forwarding checks Low Check is a single bit test; benchmark validates Interaction with lazy sweep Medium Evacuated blocks bypass sweep queue; tested explicitly Mutable root borrow conflicts Low Stop-the-world GC ensures exclusive access","title":"Appendix C: Risk Assessment"},{"location":"plans/2026-02-06-gc-evacuation-design/#appendix-d-related-beads","text":"eu-2ij -- GC: Implement lazy sweeping optimisation (Phase 2, prerequisite, completed) eu-5si -- GC: Implement full reference updating system (Phase 3, this work) eu-w68 -- Memory management unsafe code lacks safety documentation (addressed by sub-bead 8)","title":"Appendix D: Related Beads"}]}